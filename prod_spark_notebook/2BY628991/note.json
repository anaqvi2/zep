{
  "paragraphs": [
    {
      "text": "%pyspark\n\n## epigeocoder.helpers\n\nimport os\nimport re\nimport traceback\n\nnospace_langs \u003d (\u0027zh\u0027,)\n\nCODE_REGEX \u003d r\u0027(?:\\s|^|!|,|\\()([A-Z]{2,3})(?:\\s|$|!|,|\\))\u0027\nDELETE_REGEX \u003d r\u0027\\\\n|\\\\t|\\\u003e|\\\u003c|\\(|\\)|\\\u003e|\\\u003c\u0027\n\n# Words that should not be considered placenames\nIGNORE_WORDS \u003d [\n    \u0027a\u0027,\n    \u0027ai\u0027,\n    \u0027i\u0027,\n    \u0027of\u0027,\n    \u0027the\u0027\n    \u0027many\u0027,\n    \u0027may\u0027,\n    \u0027march\u0027,\n    \u0027center\u0027,\n    \u0027as\u0027,\n    \u0027see\u0027,\n    \u0027valley\u0027,\n    \u0027university\u0027,\n    \u0027about\u0027,\n    \u0027new\u0027,\n    \u0027sars\u0027,\n    \u0027aids\u0027,\n    \u0027hpv\u0027,\n    \u0027adhd\u0027,\n    \u0027newcastle\u0027,\n    \u0027elisa\u0027,\n    \u0027to\u0027,\n    \u0027influenza\u0027,\n    \u0027who\u0027,\n    \u0027pro\u0027,\n    \u0027os\u0027,\n    \u0027and\u0027,\n    \u0027pdt\u0027,\n    \u0027in\u0027,\n    \u0027flu\u0027,\n    \u0027ha\u0027,\n    \u0027ron\u0027,\n    \u0027control\u0027,\n    \u0027mod\u0027,\n    \u0027northern\u0027,\n    \u0027southern\u0027,\n    \u0027eastern\u0027,\n    \u0027western\u0027,\n    \u0027all\u0027,\n    \u0027am\u0027,\n    \u0027pm\u0027,\n    \u0027eua\u0027,\n    \u0027avian\u0027,\n    \u0027ah\u0027,\n    \u0027human\u0027,\n    \u0027date\u0027,\n    \u0027health\u0027,\n    \u0027fry\u0027,\n    \u0027many\u0027,\n    \u0027is\u0027,\n    \u0027the\u0027,\n    \u0027by\u0027,\n    \u0027on\u0027,\n    \u0027ap\u0027,\n    \u0027er\u0027,\n    \u0027do\u0027,\n    \u0027cnn\u0027,\n    \u0027msm\u0027,\n    \u0027wtf\u0027,\n    \u0027edt\u0027,\n    \u0027est\u0027,\n    \u0027pst\u0027,\n    \u0027pdt\u0027,\n    \u0027cst\u0027,\n    \u0027cdt\u0027,\n    \u0027mst\u0027,\n    \u0027mdt\u0027,\n    \u0027dui\u0027,\n    \u0027dwi\u0027,\n    \u0027led\u0027,\n    \u0027along\u0027,\n    \u0027since\u0027,\n    \u0027we\u0027,\n    \u0027xps\u0027,\n    \u0027never\u0027,\n    \u0027rat\u0027,\n    \u0027me\u0027,\n    \u0027white\u0027,\n    \u0027can\u0027,\n    \u0027camo\u0027,\n    \u0027mountain\u0027,\n    \u0027normal\u0027,\n    \u0027light\u0027,\n    \u0027acs\u0027,\n    \u0027early\u0027,\n    \u0027not\u0027,\n    \u0027do\u0027,\n    \u0027some\u0027,\n    \u0027although\u0027,\n    \u0027bucks\u0027,\n    \u0027most\u0027,\n    \u0027ha\u0027,\n    \u0027of\u0027,\n    \u0027it\u0027,\n    \u0027orange\u0027,\n    \u0027hard\u0027,\n    \u0027green\u0027,\n    \u0027never\u0027,\n    \u0027heard\u0027,\n    \u0027down\u0027,\n    \u0027wayne\u0027,\n    \u0027ross\u0027,\n    \u0027swords\u0027,\n    \u0027college\u0027,\n    \u0027vista\u0027,\n    \u0027bear\u0027,\n    \u0027sauce\u0027,\n    \u0027lit\u0027,\n    \u0027pen\u0027,\n    \u0027are\u0027,\n    \u0027bud\u0027,\n    \u0027savage\u0027,\n    \u0027barrage\u0027,\n    \u0027mag\u0027,\n    \u0027ash\u0027,\n    \u0027shields\u0027,\n    \u0027trinity\u0027,\n    \u0027walk\u0027,\n    \u0027grit\u0027,\n    \u0027bane\u0027,\n    \u0027men\u0027,\n    \u0027punch\u0027,\n    \u0027lot\u0027,\n    \u0027tongue\u0027,\n    \u0027hope\u0027,\n    \u0027mtl\u0027,\n    \u0027sparks\u0027,\n    \u0027surprise\u0027,\n    \u0027some\u0027,\n    \u0027normal\u0027,\n    \u0027time\u0027,\n    \u0027halfway\u0027,\n    \u0027drama\u0027,\n    \u0027dmt\u0027,\n    \u0027god\u0027,\n    \u0027murk\u0027,\n    \u0027turn\u0027,\n    \u0027most\u0027,\n    \u0027imperial\u0027,\n    \u0027morris\u0027,\n    \u0027west\u0027,\n    \u0027east\u0027,\n    \u0027north\u0027,\n    \u0027south\u0027,\n    \u0027lad\u0027,\n    \u0027man\u0027,\n    \u0027young\u0027,\n    \u0027tequila\u0027,\n    \u0027bentley\u0027,\n    \u0027name\u0027,\n    \u0027robert\u0027,\n    \u0027downey\u0027,\n    \u0027angered\u0027,\n    \u0027begun\u0027,\n    \u0027sunset\u0027,\n    \u0027sunrise\u0027,\n    \u0027spencer\u0027,\n    \u0027forest\u0027,\n    \u0027torrent\u0027,\n    \u0027run\u0027,\n    \u0027ran\u0027,\n    \u0027get\u0027,\n    \u0027six\u0027,\n    \u0027got\u0027,\n    \u0027crisp\u0027,\n    \u0027our\u0027,\n    \u0027widen\u0027,\n    \u0027down\u0027,\n    \u0027zero\u0027,\n    \u0027out\u0027,\n    \u0027walk\u0027,\n    \u0027grants\u0027,\n    \u0027mark\u0027,\n    \u0027manage\u0027,\n    \u0027gi\u0027,\n    \u0027street\u0027,\n    \u0027best\u0027,\n    \u0027obama\u0027,\n    \u0027mp\u0027,\n    \u0027th\u0027,\n    \u0027td\u0027,\n    \u0027tank\u0027,\n    \u0027dig\u0027,\n    \u0027gbe\u0027,\n    \u0027city\u0027,\n    \u0027tc\u0027,\n    \u0027ale\u0027,\n    \u0027sarah\u0027,\n    \u0027police\u0027,\n    \u0027bah\u0027,\n    \u0027okay\u0027,\n    \u0027edwards\u0027,\n    \u0027ceo\u0027,\n    \u0027solo\u0027,\n    \u0027siri\u0027,\n    \u0027imo\u0027,\n    \u0027cc\u0027,\n    \u0027pmi\u0027,\n    \u0027powell\u0027,\n    \u0027clinton\u0027,\n    \u0027madeleine\u0027,\n    \u0027than\u0027,\n    \u0027kerry\u0027,\n    \u0027federal\u0027,\n    \u0027dj\u0027,\n    \u0027spring\u0027,\n    \u0027shoreline\u0027,\n    \u0027ridge\u0027,\n    \u0027so\u0027,\n    \u0027kings\u0027,\n    \u0027lion\u0027,\n    \u0027number 1\u0027,\n    \u0027best\u0027,\n    \u0027ganja\u0027,\n    \u0027day\u0027,\n    \u0027aroma\u0027,\n    \u0027griffin\u0027,\n    \u0027dea\u0027,\n    \u0027parker\u0027,\n    \u0027batman\u0027,\n    \u0027long\u0027,\n    \u0027mary\u0027,\n    \u0027mcu\u0027,\n    \u0027side\u0027,\n    \u0027say\u0027,\n    \u0027my\u0027,\n    \u0027papa\u0027,\n    \u0027son\u0027,\n    \u0027iv\u0027,\n    \u0027cg\u0027,\n    \u0027split\u0027,\n    \u0027stone\u0027,\n    \u0027hospital\u0027,\n    \u0027monster\u0027,\n    \u0027pbr\u0027,\n    \u0027put\u0027,\n    \u0027outer\u0027,\n    \u0027ao\u0027,\n    \u0027stewart\u0027,\n    \u0027kanye\u0027,\n    \u0027bush\u0027,\n    \u0027superior\u0027,\n    \u0027kissing\u0027,\n    \u0027brass\u0027,\n    \u0027the city\u0027,\n    \u0027van\u0027,\n    \u0027mallet\u0027,\n    \u0027at\u0027,\n    \u0027adv\u0027,\n    \u0027nye\u0027,\n    \u0027nancy\u0027,\n    \u0027commerce\u0027,\n    \u0027taft\u0027,\n    \u0027roosevelt\u0027,\n    \u0027union\u0027,\n    \u0027was\u0027,\n    \u0027oscar\u0027,\n    \u0027off\u0027,\n    \u0027george\u0027,\n    \u0027justice\u0027,\n    \u0027moody\u0027,\n    \u0027katana\u0027,\n    \u0027dpe\u0027,\n    \u0027liberal\u0027,\n    \u0027vladimir\u0027,\n    \u0027pg\u0027,\n    \u0027tell\u0027,\n    \u0027moron\u0027,\n    \u0027ama\u0027,\n    \u0027real\u0027,\n    \u0027republic\u0027,\n    \u0027lee\u0027,\n    \u0027rj\u0027,\n    \u0027jones\u0027,\n    \u0027bro\u0027,\n    \u0027cd\u0027,\n    \u0027no\u0027,\n    \u0027yes\u0027,\n    \u0027tv\u0027,\n    \u0027diy\u0027,\n    \u0027still\u0027,\n    \u0027cast\u0027,\n    \u0027silly\u0027,\n    \u0027front\u0027,\n    \u0027theres\u0027,\n    \u0027jesus\u0027,\n    \u0027change\u0027,\n    \u0027force\u0027,\n    \u0027nana\u0027,\n    \u0027wrist\u0027,\n    \u0027much\u0027,\n    \u0027kinda\u0027,\n    \u0027capri\u0027,\n    \u0027price\u0027,\n    \u0027atlantic\u0027,\n    \u0027pacific\u0027,\n    \u0027x\u0027,\n    \u0027bf\u0027,\n    \u0027grad\u0027,\n    \u0027bong\u0027,\n    \u0027end\u0027,\n    \u0027phone\u0027,\n    \u0027michael\u0027,\n    \u0027hang\u0027,\n    \u0027bender\u0027,\n    \u0027unknown\u0027,\n    \u0027gold\u0027,\n    \u0027julia\u0027,\n    \u0027mmj\u0027,\n    \u0027somali\u0027,\n    \u0027chihuahua\u0027,\n    \u0027store\u0027,\n    \u0027im\u0027,\n    \u0027sour\u0027,\n    \u0027same\u0027,\n    \u0027wed\u0027,\n    \u0027white house\u0027,\n    \u0027stalin\u0027,\n    \u0027ppm\u0027,\n    \u0027tampon\u0027,\n    \u0027mission\u0027,\n    \u0027ocd\u0027,\n    \u0027apv\u0027,\n    \u0027north east\u0027,\n    \u0027central\u0027,\n    \u0027save\u0027,\n    \u0027plateau\u0027,\n    \u0027northland\u0027,\n    \u0027nikki\u0027,\n    \u0027adams\u0027,\n    \u0027patrick\u0027,\n    \u0027kyle\u0027,\n    \u0027david\u0027,\n    \u0027ariana\u0027,\n    \u0027leon\u0027,\n    \u0027davis\u0027,\n    \u0027taylor\u0027,\n    \u0027lewis\u0027,\n    \u0027paul\u0027,\n    \u0027hamm\u0027,\n    \u0027sandy\u0027,\n    \u0027temple\u0027,\n    \u0027sing\u0027,\n    \u0027pain\u0027,\n    \u0027drug\u0027,\n    \u0027sale\u0027,\n    \u0027cars\u0027,\n    \u0027race\u0027,\n    \u0027salt\u0027,\n    \u0027male\u0027,\n    \u0027oral\u0027,\n    \u0027mila\u0027,\n    \u0027luck\u0027,\n]\n\ndef relation_score(places, admins, countries):\n    \"\"\"\n    Adjust scores if the admin/country name or code of a place/admin match\n    equals the name/code of an admin/country match.\n    \"\"\"\n\n    for place in places.keys():\n        for item in places[place]:\n            admin_code \u003d item[\u0027admin1\u0027]\n            country \u003d item[\u0027country\u0027]\n            if admin_code:\n                admin_code \u003d admin_code.lower()\n                if admin_code in admins.keys():\n                    item[\u0027score\u0027] *\u003d 100\n                    for admitem in admins[admin_code]:\n                        if country and admitem[\u0027country\u0027] and admitem[\u0027country\u0027].lower() \u003d\u003d country:\n                            admitem[\u0027score\u0027] *\u003d 3\n                else:\n                    for admin in admins.keys():\n                        for admitem in admins[admin]:\n                            if (country and admitem[\u0027country\u0027] and\n                                        admitem[\u0027country\u0027].lower() \u003d\u003d country.lower() and\n                                        admitem[\u0027admin1\u0027].lower() \u003d\u003d admin_code and\n                                        admitem[\u0027feature_code\u0027] \u003d\u003d \u0027ADM1\u0027):\n                                item[\u0027score\u0027] *\u003d 4\n                                admitem[\u0027score\u0027] *\u003d 3\n            if country:\n                country \u003d country.lower()\n                for cc in countries.keys():\n                    for ccitem in countries[cc]:\n                        if ccitem[\u0027country\u0027].lower() \u003d\u003d country:\n                            ccitem[\u0027score\u0027] *\u003d 2\n                            if ccitem[\u0027matching_word\u0027] !\u003d item[\u0027matching_word\u0027] \\\n                                    and item[\u0027matching_word\u0027] not in (admins.keys() + countries.keys()):\n                                item[\u0027score\u0027] *\u003d 2\n    for adm in admins.keys():\n        for item in admins[adm]:\n            icountry \u003d item[\u0027country\u0027]\n            if icountry:\n                for cc in countries.keys():\n                    for ccitem in countries[cc]:\n                        if ccitem[\u0027country\u0027] \u003d\u003d icountry:\n                            if ccitem[\u0027matching_word\u0027] !\u003d item[\u0027matching_word\u0027] \\\n                                    and item[\u0027matching_word\u0027] in admins.keys():\n                                item[\u0027score\u0027] *\u003d 2\n\n\ndef name_match(name, results, admin1\u003dNone, country\u003dNone):\n    \"\"\"\n    Create a list of objects for each match found for a name\n    0 - geonameid\n    1 - name\n    2 - admin1_code\n    3 - admin_abbr\n    4 - feature_code\n    5 - source\n    6 - country_code\n    7: population\n    8: latitude\n    9: longitude\n    \"\"\"\n\n    name_matches \u003d []\n    for geoname in results:\n        if not admin1 or admin1 \u003d\u003d geoname[3]:\n            if not country or country \u003d\u003d geoname[6]:\n                pop \u003d geoname[7]\n                if not pop:\n                    pop \u003d 0\n                name_matches.append({\n                    \u0027coord\u0027: [\n                        geoname[9],\n                        geoname[8]\n                    ],\n                    \u0027geonameid\u0027: geoname[0],\n                    \u0027name\u0027: geoname[1],\n                    \u0027matching_word\u0027: name,\n                    \u0027admin1\u0027: geoname[3],\n                    \u0027feature_code\u0027: geoname[4],\n                    \u0027country\u0027: geoname[6],\n                    \u0027population\u0027: pop,\n                    \u0027source\u0027: geoname[5],\n                    \u0027score\u0027: -1\n                })\n    return name_matches\n\n\ndef score_matches(matches, name, codes):\n    \"\"\"\n    Make a semi-educated guess about which match is better\n    \"\"\"\n    for nmatch in matches:\n        fc \u003d nmatch[\u0027feature_code\u0027]\n        alternates \u003d []\n        score \u003d 1\n        exact_match \u003d False\n        source \u003d nmatch[\u0027source\u0027]\n        if source \u003d\u003d \u0027name\u0027:\n            # Best type of nmatch\n            score +\u003d 100000\n            exact_match \u003d True\n        if name not in codes:\n            if re.match(name.lower(), nmatch[\u0027name\u0027].lower()):\n                score +\u003d 50000\n                if fc.startswith(\u0027PCL\u0027):\n                    score +\u003d 1000000\n        elif source in (\u0027alternate\u0027, \u0027ascii\u0027):\n            # 2nd best (alternate name - for all foreign languages)\n            score +\u003d 100000\n            exact_match \u003d True\n        if name in codes:\n            if (name.upper() \u003d\u003d nmatch[\n                \u0027country\u0027] or name.upper() in alternates) and fc.startswith(\n                    \u0027PCL\u0027):\n                score +\u003d 50000\n            elif (name.upper() \u003d\u003d nmatch[\n                \u0027admin1\u0027] or name.upper() in alternates) and fc \u003d\u003d \u0027ADM1\u0027:\n                score +\u003d 50000\n            else:\n                score \u003d -1\n        # Give higher scores to political capitals.\n        # \u0027Moscow\u0027 is most likely the one in Russia, not Maine\n        if score \u003e 50000:\n            if fc:\n                if fc.startswith(u\u0027PCL\u0027):\n                    score +\u003d 100000\n                elif fc \u003d\u003d \u0027ADM1\u0027:\n                    score +\u003d 50000\n                elif fc.startswith(\u0027PPLA\u0027) and exact_match:\n                    score +\u003d 40000\n                elif fc \u003d\u003d \u0027PPLC\u0027:\n                    score +\u003d 400000\n        if score \u003e 1:\n            if codes:\n                admin \u003d nmatch[\u0027admin1\u0027]\n                if admin and admin in codes:\n                    score *\u003d 4\n                admin \u003d nmatch[\u0027country\u0027]\n                if admin and admin in codes:\n                    score *\u003d 2\n            if alternates:\n                # Bonus points for actually having alternate names\n                score +\u003d 5000\n            if nmatch[\u0027feature_code\u0027].startswith(\u0027PPL\u0027):\n                score +\u003d ((nmatch[\u0027population\u0027] + 1) / 10)\n            else:\n                score +\u003d ((nmatch[\u0027population\u0027] + 1) / 1000)\n        nmatch[\u0027score\u0027] \u003d score\n\n",
      "dateUpdated": "Oct 23, 2016 3:30:18 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476878442802_-130135715",
      "id": "20161019-120042_149888139",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 19, 2016 12:00:42 PM",
      "dateStarted": "Oct 23, 2016 6:23:52 AM",
      "dateFinished": "Oct 23, 2016 6:23:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/**\n * Create version columns if necessary in Scylla/Cassandra. \n * Written in Scala because pyspark drivers seem to be missing?\n * Make sure to update processor_version as necessary\n **/\n\nimport java.util.Calendar\nimport java.text.SimpleDateFormat\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.SparkContext\n\nval processor_name \u003d \"place_list\"\nvar processor_version \u003d \"v0.0.1\"\n\nval format \u003d new SimpleDateFormat(\"yw\")\nval calendar \u003d Calendar.getInstance()\ncalendar.add(Calendar.DAY_OF_MONTH, -7)\nval yrweek \u003d format.format(calendar.getTime())\n\nprocessor_version \u003d processor_version.replace(\".\", \"_\")\n\nval v \u003d  s\"${processor_name}_v\"\nval pvd \u003d s\"${processor_name}__${processor_version}__${yrweek}\"\nval pvd \u003d \"place_list__v0_0_1a__201642\"\n\ndef writeVersionColumn(sc:SparkContext, column: String, coltype: String, table: String, keyspace: String) \u003d\n{\n\n val cassTable \u003d sc.cassandraTable(keyspace, table)\n val cassSchema \u003d cassTable.selectedColumnNames.toSet\n \n if(!cassSchema.contains(column)) {\n      val cassTable \u003d sc.cassandraTable(keyspace, table)\n      cassTable.connector.withSessionDo {\n            session \u003d\u003e {\n              session.execute(s\"ALTER TABLE $keyspace.$table ADD $column $coltype\")\n    } } }\n }\n\nwriteVersionColumn(sc, v, \"text\", \"agg_table\", \"epione\")\nwriteVersionColumn(sc, pvd, \"frozen\u003cset\u003cfrozen\u003cplace\u003e\u003e\u003e\", \"agg_table\", \"epione\")",
      "dateUpdated": "Oct 23, 2016 3:30:26 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476878927243_-1748500147",
      "id": "20161019-120847_495189361",
      "dateCreated": "Oct 19, 2016 12:08:47 PM",
      "dateStarted": "Oct 19, 2016 11:15:03 PM",
      "dateFinished": "Oct 19, 2016 11:15:12 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n# coding: utf-8\nimport argparse\nimport os\nimport datetime\nimport re\nimport itertools\nfrom timeit import default_timer as timer\nfrom pyspark.sql.functions import lit, udf, col, when, size, lower, length\nfrom pyspark.sql.types import StringType, StructType, FloatType, DoubleType, IntegerType, StructField, ArrayType\nfrom pyspark.storagelevel import StorageLevel\n\n\n\"\"\"\nCreate a cached Spark dataframe of geonames data\n\"\"\"\ngeocode_df \u003d sqlContext.read.format(\n    \"org.apache.spark.sql.cassandra\").options(\n    table\u003d\"geonames\", keyspace\u003d\"geonames\").load()\n\ngeocode_df \u003d geocode_df.filter(\n    (geocode_df[\u0027name_lower\u0027].isin(IGNORE_WORDS) \u003d\u003d False) \u0026 (\n    ((geocode_df[\u0027population\u0027] \u003e 50000) \u0026 (geocode_df[\u0027feature_code\u0027] \u003d\u003d \u0027PPL\u0027)) |\n    ((geocode_df[\u0027feature_code\u0027].isin([\u0027PPLA\u0027, \u0027PPLC\u0027, \u0027ADM1\u0027, \u0027CONT\u0027, \u0027OCN\u0027, \u0027SEA\u0027])\n      \u0026 (length(geocode_df[\u0027name\u0027]) \u003e 3))) |\n    (geocode_df[\u0027feature_code\u0027].isin([\u0027PCL\u0027,\u0027PCLD\u0027,\u0027PCLF\u0027,\u0027PCLI\u0027,\u0027PCLS\u0027,\u0027TERR\u0027]))\n    )\n).cache()\n\ndef get_geonames_map():\n    \"\"\"\n    Generate and return a dictionary with lowercase placename as key, geonames data as value,\n    plus a list of unique placenames (not lowercase).\n    :return:\n    \"\"\"\n    name_map \u003d geocode_df.select(\"geonameid\", \"name\", \"admin1_code\",\n                             \"admin1_abbr\", \"feature_code\", \"source\",\n                             \"country_code\", \"population\", \"latitude\",\n                             \"longitude\", \"name_lower\") \\\n        .map(lambda w: (w[\"name_lower\"], list(w))) \\\n        .groupByKey() \\\n        .map(lambda x: (x[0], list(x[1]))).collectAsMap()\n    names \u003d list(\n        set(geocode_df\n            .select(\"name\")\n            .map(lambda w: w[\"name\"]).collect()))\n    print \"{} NAMES\".format(len(names))\n    return name_map, names\n\nname_mapper, name_list \u003d get_geonames_map()\nname_lower \u003d list(set([i.lower() for i in name_list]))\n\nprint type(name_mapper)\nprint type(name_list)\nprint type(name_lower)\n\ngmap \u003d sc.broadcast(name_mapper)\ngnames \u003d sc.broadcast(name_list)\ngnames_lower \u003d sc.broadcast(name_lower)\n\n\ndef map_query(names):\n    \"\"\"\n    Performs a query against Geonames Panda DataFrame.\n    :param names: list of placenames to search for\n    :param codes: list of potential admin/country codes to search for\n    :return: matching geonames records\n    \"\"\"\n    # start \u003d timer()\n    place_matches \u003d {}\n    admin_matches \u003d {}\n    country_matches \u003d {}\n\n    for name in names:\n        if name in gmap.value:\n            results \u003d gmap.value[name]\n            name_matches \u003d name_match(name, results)\n            score_matches(name_matches, name, [])\n            for nmatch in name_matches:\n                featurecode \u003d nmatch[\u0027feature_code\u0027]\n                if featurecode and featurecode.startswith(\u0027ADM\u0027):\n                    admin_matches.setdefault(name, []).append(nmatch)\n                elif featurecode and featurecode.startswith(\u0027PCL\u0027):\n                    country_matches.setdefault(name, []).append(nmatch)\n                else:\n                    place_matches.setdefault(name, []).append(nmatch)\n    # elapsed \u003d timer() - start\n    # print(\"panda_query Time: {} seconds\".format(elapsed))\n    return place_matches, admin_matches, country_matches\n\n\ndef get_geonames(names):\n    \"\"\"\n    Find matching geonames records given a list of placenames and (possibly blank) state/country codes\n    \"\"\"\n    all_matches \u003d {}\n    place_matches, admin_matches, country_matches \u003d map_query([x.lower() for x in names])\n    relation_score(place_matches, admin_matches, country_matches)\n\n    for group in (place_matches, admin_matches, country_matches):\n        unique_ids \u003d []\n        for k, v in group.iteritems():\n            ranked \u003d sorted(v, key\u003dlambda k: k[\u0027score\u0027], reverse\u003dTrue)\n            if ranked[0][\u0027score\u0027] \u003e 1 and \\\n                            ranked[0][\u0027geonameid\u0027] not in unique_ids and \\\n                    (k not in all_matches or all_matches[k][\u0027score\u0027] \u003c\n                        ranked[0][\u0027score\u0027]):\n                all_matches[k] \u003d ranked[0]\n                unique_ids.append(ranked[0][\u0027geonameid\u0027])\n\n    place_list \u003d []\n    for place in sorted(all_matches.values(), key\u003dlambda k: k[\u0027score\u0027],\n                        reverse\u003dTrue):\n        place_list.append({\n            \u0027place_name\u0027: place[\u0027name\u0027],\n            \u0027place_lat\u0027: place[\u0027coord\u0027][1],\n            \u0027place_lng\u0027: place[\u0027coord\u0027][0],\n            \u0027geo_geonameid\u0027: int(place[\u0027geonameid\u0027])\n        })\n    return place_list\n\n\ndef geocode(name):\n    return get_geonames([name])\n\n\ndef extract(text):\n    \"\"\"\n    Extract possible placenames from text,\n    then query Geonames dataframe for matches.\n    \"\"\"\n    matches \u003d set()\n    if text:\n        txt2match \u003d \u0027 \u0027 + re.sub(r\u0027[\\n\\.\\+\\_\\-\\{\\}\\[\\]\\(\\)\\,\\;\\:\\!]+\u0027, \u0027 \u0027,\n                                 text)\n        case_array \u003d txt2match.split(\u0027 \u0027)\n\n        for i in case_array:\n            if i.isupper() and i in gnames.value:\n                matches.add(i)\n            elif len(i) \u003e 3 and i.lower() in gnames_lower.value:\n                matches.add(i)\n\n        for a, b in itertools.combinations(matches, 2):\n            if a in b:\n                matches.discard(a)\n            elif b in a:\n                matches.discard(b)\n            elif b.lower() \u003d\u003d a.lower():\n                matches.discard(max([a, b]))\n\n        places \u003d [m for m in matches if not m.isdigit()]\n        #print u\u0027PLACES: {}\u0027.format(places)\n        return get_geonames(places)\n    return []\n\n\"\"\"\nUDF Functions for a) extract and geocode, b) geocode only\n\"\"\"\nPlaceType \u003d ArrayType(StructType([\n    StructField(\u0027place_name\u0027, StringType(), True),\n    StructField(\u0027place_lat\u0027, FloatType(), True),\n    StructField(\u0027place_lng\u0027, FloatType(), True),\n    StructField(\u0027place_id\u0027, IntegerType(), True),\n    StructField(\u0027geo_geonameid\u0027, IntegerType(), True)\n]), True)\n\n",
      "dateUpdated": "Oct 23, 2016 3:30:30 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476899569124_211675450",
      "id": "20161019-175249_958167686",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 1 times, most recent failure: Lost task 0.0 in stage 52.0 (TID 11753, localhost): java.io.IOException: Exception during execution of SELECT \"geonameid\", \"name\", \"admin1_abbr\", \"admin1_code\", \"admin2_code\", \"admin3_code\", \"admin4_code\", \"cc2\", \"country_code\", \"feature_class\", \"feature_code\", \"isolanguage\", \"latitude\", \"longitude\", \"modification_date\", \"name_lower\", \"population\", \"source\", \"timezone\" FROM \"geonames\".\"geonames\" WHERE token(\"geonameid\") \u003e ? AND token(\"geonameid\") \u003c\u003d ?   ALLOW FILTERING: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:320)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy31.execute(Unknown Source)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:309)\n\t... 34 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Exception during execution of SELECT \"geonameid\", \"name\", \"admin1_abbr\", \"admin1_code\", \"admin2_code\", \"admin3_code\", \"admin4_code\", \"cc2\", \"country_code\", \"feature_class\", \"feature_code\", \"isolanguage\", \"latitude\", \"longitude\", \"modification_date\", \"name_lower\", \"population\", \"source\", \"timezone\" FROM \"geonames\".\"geonames\" WHERE token(\"geonameid\") \u003e ? AND token(\"geonameid\") \u003c\u003d ?   ALLOW FILTERING: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:320)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy31.execute(Unknown Source)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:309)\n\t... 34 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n\u0027, JavaObject id\u003do487), \u003ctraceback object at 0x7f3d1de5ac68\u003e)"
      },
      "dateCreated": "Oct 19, 2016 5:52:49 PM",
      "dateStarted": "Oct 23, 2016 6:23:53 AM",
      "dateFinished": "Oct 23, 2016 6:24:49 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nextract_udf \u003d udf(lambda txt: extract(txt), PlaceType)\ngeocode_udf \u003d udf(lambda txt: geocode(txt), PlaceType)\n\nprint extract(\"I love Colorado\")\nprint extract(\"\")",
      "dateUpdated": "Oct 23, 2016 3:30:32 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476975207763_-1709250967",
      "id": "20161020-145327_1045987794",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-5792155387511642302.py\", line 239, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027PlaceType\u0027 is not defined\n"
      },
      "dateCreated": "Oct 20, 2016 2:53:27 PM",
      "dateStarted": "Oct 23, 2016 6:23:53 AM",
      "dateFinished": "Oct 23, 2016 6:24:49 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ntable \u003d \u0027agg_table\u0027\n\nPROCESSOR_NAME \u003d \u0027place_list\u0027\nPROCESSOR_VERSION \u003d \u0027v0.0.1a\u0027\n\nepione_all \u003d sqlContext.read.format(\n    \"org.apache.spark.sql.cassandra\").options(\n    table\u003dtable, keyspace\u003d\"epione\").load()\nepione_all\n\n\"\"\"\n Select records which have no loc and different version or never processed\n \"\"\"\nsub_epione \u003d epione_all.select(\"id\", \"source\", \"cr\", \"description\", \"place_list_v\").filter(\n    (epione_all[\u0027description\u0027].isNotNull())\n).persist(StorageLevel.MEMORY_AND_DISK_SER)",
      "dateUpdated": "Oct 20, 2016 4:03:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476879210806_1166740625",
      "id": "20161019-121330_1550673167",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 19, 2016 12:13:30 PM",
      "dateStarted": "Oct 20, 2016 4:03:22 PM",
      "dateFinished": "Oct 20, 2016 4:03:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ntable \u003d \u0027agg_table\u0027\n\nPROCESSOR_NAME \u003d \u0027place_list\u0027\nPROCESSOR_VERSION \u003d \u0027v0.0.1a\u0027\n\n\n\"\"\"\nGeocode rows \n\"\"\"\nepione_placelist \u003d sub_epione.withColumn(\"place_list\", extract_udf(sub_epione[\"description\"]))\n\n\n\"\"\"\nAssign extract/geocode results to place_list\n\"\"\"\nepione_placelist_final \u003d epione_placelist. \\\n    withColumn(\u0027place_list_v\u0027, lit(PROCESSOR_VERSION))\n\n#print epione_placelist_final.show(5)\n\nepione_placelist_final.select(\"id\", \"source\", \"cr\", \"place_list\", \"place_list_v\") \\\n    .write.format(\"org.apache.spark.sql.cassandra\") \\\n    .options(table\u003dtable, keyspace\u003d\"epione\").save(mode\u003d\"append\")\n",
      "dateUpdated": "Oct 20, 2016 4:04:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476899075681_-1319526125",
      "id": "20161019-174435_1183355444",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 19, 2016 5:44:35 PM",
      "dateStarted": "Oct 20, 2016 4:04:29 PM",
      "dateFinished": "Oct 20, 2016 4:20:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476975035373_-832052478",
      "id": "20161020-145035_544933203",
      "dateCreated": "Oct 20, 2016 2:50:35 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "epigeocoder_zep_version",
  "id": "2BY628991",
  "angularObjects": {
    "2BUZX9EWW:shared_process": [],
    "2BRP3ZUWJ:shared_process": [],
    "2BTJ3P41C:shared_process": [],
    "2BSJYK2YE:shared_process": [],
    "2BU87RU3U:shared_process": [],
    "2BTB82RPQ:shared_process": [],
    "2BSSEDUXN:shared_process": [],
    "2BRZ896X6:shared_process": [],
    "2BT3JK3T4:shared_process": [],
    "2BTXGDVEJ:shared_process": [],
    "2BUSJ5B5T:shared_process": [],
    "2BT211CDH:shared_process": [],
    "2BTX1MQS6:shared_process": [],
    "2BUCYP1D2:shared_process": [],
    "2BUTB2HA6:shared_process": [],
    "2BRYFEHJ7:shared_process": [],
    "2BSMJA8VG:shared_process": []
  },
  "config": {},
  "info": {}
}