{
  "paragraphs": [
    {
      "text": "import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkConf, SparkContext}\nimport com.mongodb.casbah.{WriteConcern \u003d\u003e MongodbWriteConcern}\nimport com.stratio.datasource._\nimport com.stratio.datasource.mongodb._\nimport com.stratio.datasource.mongodb.schema._\nimport com.stratio.datasource.mongodb.writer._\nimport com.stratio.datasource.mongodb.config._\nimport com.stratio.datasource.mongodb.config.MongodbConfig._\nimport org.apache.spark.sql.SQLContext\nimport com.stratio.datasource.util.Config._\nimport scala.collection.mutable.WrappedArray\n\nval space \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else text.split(\" \")\n})\n\nvar builder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"final_master\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nvar readConfig \u003d builder.build()\nval master \u003d sqlContext.fromMongoDB(readConfig).select(\"name\", \"synonyms\", \"_id\", \"tag\").where(size($\"synonyms\") \u003e 0).withColumn(\"synonyms\", explode($\"synonyms\")).withColumn(\"synonyms\", regexp_replace(lower($\"synonyms\"), \"\"\"[\\p{Punct}]\"\"\", \" \")).withColumn(\"size\", size(space($\"synonyms\"))).sort(desc(\"size\"))\n\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"pos_words\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval pos \u003d sqlContext.fromMongoDB(readConfig).select(\"x\", \"_id\")\n\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"neg_words\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval neg \u003d sqlContext.fromMongoDB(readConfig).select(\"x\", \"_id\")\n\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"final_id_all\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval id_all \u003d sqlContext.fromMongoDB(readConfig).select(\"name\", \"_id\")\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"pii\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval pii \u003d sqlContext.fromMongoDB(readConfig).select(\"phrase\", \"_id\").withColumnRenamed(\"synonyms\", \"phrase\")\n\nval symp \u003d master.filter($\"tag\"\u003d\u003d\u003d\"symptom\").collect()\nval ue \u003d master.filter($\"tag\"\u003d\u003d\u003d\"ue\").collect()\nval prod \u003d master.filter($\"tag\"\u003d\u003d\u003d\"product\").collect()\nval comp \u003d master.filter($\"tag\"\u003d\u003d\u003d\"organization\").collect()\nval top \u003d master.filter($\"tag\"\u003d\u003d\u003d\"business_category\").collect()\nval des \u003d master.filter($\"tag\"\u003d\u003d\u003d\"disease\").collect()\n//val cat \u003d master.filter($\"tag\"\u003d\u003d\u003d\"place_category\").collect()\n//val spec \u003d master.filter($\"tag\"\u003d\u003d\u003d\"species\").collect()\nval pi \u003d pii.collect()\nval poso \u003d pos.collect()\nval nego \u003d neg.collect()\nval ids \u003d id_all.collect()\nval p_syn \u003d prod.map(x\u003d\u003e x(1).toString).toSet\nval s_syn \u003d symp.map(x\u003d\u003e x(1).toString).toSet\nval u_syn \u003d ue.map(x\u003d\u003e x(1).toString).toSet\nval pi_syn \u003d pi.map(x\u003d\u003e x(1).toString).toSet\nval comp_syn \u003d comp.map(x\u003d\u003e x(1).toString).toSet\nval top_syn \u003d top.map(x\u003d\u003e x(1).toString).toSet\nval des_syn \u003d des.map(x\u003d\u003e x(1).toString).toSet\n//val cat_syn \u003d cat.map(x\u003d\u003e x(1).toString).toSet\n//val spec_syn \u003d spec.map(x\u003d\u003e x(1).toString).toSet\nval pos_syn \u003d poso.map(x\u003d\u003e x(0).toString).toSet\nval neg_syn \u003d nego.map(x\u003d\u003e x(0).toString).toSet\nval id_syn \u003d nego.map(x\u003d\u003e x(0).toString).toSet\nvar pa :Map[String,String] \u003d Map()\nvar sa :Map[String,String] \u003d Map()\nvar ua :Map[String,String] \u003d Map()\nvar pia :Map[String,String] \u003d Map()\nvar ca :Map[String,String] \u003d Map()\nvar ta :Map[String,String] \u003d Map()\nvar das :Map[String,String] \u003d Map()\nvar cas :Map[String,String] \u003d Map()\nvar sas :Map[String,String] \u003d Map()\nvar pas :Map[String,String] \u003d Map()\nvar nas :Map[String,String] \u003d Map()\nvar ias :Map[String,String] \u003d Map()\nfor(i \u003c- prod){\n    pa+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- symp){\n    sa+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- ue){\n    ua+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- pi){\n    pia+\u003d (i(0).toString -\u003e i(1).toString)\n}\nfor(i \u003c- comp){\n    ca+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- top){\n    ta+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- des){\n    das+\u003d (i(1).toString -\u003e i(2).toString)\n}\n//for(i \u003c- cat){\n//    cas+\u003d (i(1).toString -\u003e i(2).toString)\n//}\n//for(i \u003c- spec){\n//    sas+\u003d (i(1).toString -\u003e i(2).toString)\n//}\nfor(i \u003c- poso){\n    pas+\u003d (i(0).toString -\u003e i(1).toString)\n}\nfor(i \u003c- nego){\n    nas+\u003d (i(0).toString -\u003e i(1).toString)\n}\nfor(i \u003c- ids){\n    ias+\u003d (i(0).toString -\u003e i(1).toString)\n}\nval ns \u003d Array(master.select(\"size\").take(1)(0)(0).toString.toInt) \nval nsize \u003d ns.max\n\n",
      "dateUpdated": "Oct 6, 2016 12:42:54 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475096352799_1071642498",
      "id": "20160928-205912_564766606",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkConf, SparkContext}\nimport com.mongodb.casbah.{WriteConcern\u003d\u003eMongodbWriteConcern}\nimport com.stratio.datasource._\nimport com.stratio.datasource.mongodb._\nimport com.stratio.datasource.mongodb.schema._\nimport com.stratio.datasource.mongodb.writer._\nimport com.stratio.datasource.mongodb.config._\nimport com.stratio.datasource.mongodb.config.MongodbConfig._\nimport org.apache.spark.sql.SQLContext\nimport com.stratio.datasource.util.Config._\nimport scala.collection.mutable.WrappedArray\nspace: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,ArrayType(StringType,true),List(StringType))\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e final_master, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@62a30823\nmaster: org.apache.spark.sql.DataFrame \u003d [name: string, synonyms: string, _id: int, tag: string, size: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e pos_words, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@88f4ee04\npos: org.apache.spark.sql.DataFrame \u003d [x: string, _id: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e neg_words, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@caeb0bf\nneg: org.apache.spark.sql.DataFrame \u003d [x: string, _id: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e final_id_all, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@f747401f\nid_all: org.apache.spark.sql.DataFrame \u003d [name: string, _id: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e pii, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@3f4bf2a5\npii: org.apache.spark.sql.DataFrame \u003d [phrase: string, _id: double]\nsymp: Array[org.apache.spark.sql.Row] \u003d Array([unevaluable event,feels like i m writing with my left hand in my right hand,12329,symptom,13], [hallucination,keep thinking i see people out of the corner of my eye,9843,symptom,12], [abnormal sleep,i fall asleep for roughly 3 hours and then stay awake,13031,symptom,11], [abdominal discomfort,makes me feel like i ve been kicked in the stomach,15576,symptom,11], [nail picking,fingernails i m sorry there s no skin left around you,16506,symptom,11], [vaginal bleeding,have a period for just a bout a solid year,7492,symptom,10], [pain in extremity,feet feel like i have been on them all day,8443,symptom,10], [nonspecific reaction,hate the feeling when you want it to be over,11330,symptom,10], [frequent urination,waking up in the middle of the nig...ue: Array[org.apache.spark.sql.Row] \u003d Array([ue: negative social perception,people look just as gross and stupid smoking e cig things as a real cigarette,17777,ue,15], [ue: negative social perception,dumb grls on the bus are tryna hide smokin an e cig,17777,ue,12], [ue: negative social perception,smoking an e cig in the library does not make you cool,17777,ue,12], [ue: negative social perception,if you smoke an e cig inside i assume you re asshole,17777,ue,12], [ue: negative social perception,if you re smoking an e cig i m judging you,17777,ue,11], [ue: health related testimonial,it has been a long time since i felt this good,17839,ue,11], [ue: high efficacy,it has been a long time since i felt this good,18228,ue,11], [ue: cessation related testimonial,i quit smoking cigarettes after 40...prod: Array[org.apache.spark.sql.Row] \u003d Array([drospirenone/ethinyl estradiol/levomefolate calcium tablets and levomefolate calcium,drospirenone ethinyl estradiol levomefolate calcium tablets and levomefolate calcium,2021,product,9], [6f angio-seal vascular closure device vip,6 f angio seal vascular closure device vip,1514,product,8], [centruroides (scorpion) immune f(ab)2(equine),centruroides scorpion immune f ab 2 equine,880,product,7], [6 shooter saeed multi-band ligator,6 shooter saeed multi band ligator,1510,product,6], [coated vicryl plus antibacterial (polyglactin 910),coated vicryl plus antibacterial polyglactin 910,1552,product,6], [octrode lead kit, 60cm length,octrode lead kit 60 cm length,1591,product,6], [hpv vaccine,vaccine against some kind of cancer,1919,product,6], [acc...comp: Array[org.apache.spark.sql.Row] \u003d Array([epidemico,wholly owned subsidiary booz allen hamilton,216,organization,6], [epidemico,spinoff boston childrens hospital,216,organization,4], [booz allen hamilton,booz allen,204,organization,2], [booz allen hamilton,booze allen,204,organization,2], [pwc,price waterhouse,212,organization,2], [epidemico,john brownstein,216,organization,2], [epidemico,nabarun dasgupta,216,organization,2], [epidemico,robin heffernan,216,organization,2], [epidemico,clark freifeld,216,organization,2], [accenture,accenture,207,organization,1], [deloitte,deloitte,208,organization,1], [raytheon,raytheon,209,organization,1], [ibm,ibm,210,organization,1], [ibm,ibmwatson,210,organization,1], [pwc,pricewaterhousecooper,212,organization,1], [pwc,pricewaterhousecoopers,212...top: Array[org.apache.spark.sql.Row] \u003d Array([cyber,crisis task force officer,8,business_category,4], [digital,inspector of the future,84,business_category,4], [digital,examiner of the future,84,business_category,4], [cyber,supplier security management,8,business_category,3], [cyber,continuous diagnostic mitigation,8,business_category,3], [cyber,intelligence driven operations,8,business_category,3], [cyber,anticipatory threat intelligance,8,business_category,3], [cyber,internet of things,8,business_category,3], [cyber,cyber supply chain,8,business_category,3], [data science,cell based security,54,business_category,3], [data science,semi structured data,54,business_category,3], [data science,data charity bowl,54,business_category,3], [digital,amazon web services,84,business_category,3], ...des: Array[org.apache.spark.sql.Row] \u003d Array([rocky mountain spotted fever,rickettsia tick borne spotted fever r africae r conorii r rickettsii and other,21943,disease,13], [rickettsia,rickettsia tick borne spotted fever tick typhus rmsf african tick bite fever,25777,disease,12], [colitis,inflammatory bowel disease new onset post travel crohns or ulcerative colitis,25949,disease,11], [amebiasis,amebas other e hartmani e nana e coli e polecki,18804,disease,10], [rickettsia,rickettsia tick borne spotted fever tick typhus rmsf atbf msf,25777,disease,10], [undiagnosed,cause of the illness had not yet been determined,23258,disease,9], [other human disease,skin soft tissue infection secondary bacterial of existing lesion,26307,disease,9], [diarrhea,diarrhea chronic responsive to anti parasiti...pi: Array[org.apache.spark.sql.Row] \u003d Array([ethan,6.0], [isabella,18.0], [olivia,19.0], [noah,23.0], [abigail,25.0], [sophia,26.0], [alexis,28.0], [hannah,29.0], [samantha,40.0], [jayden,41.0], [zachary,42.0], [elijah,43.0], [ava,44.0], [caleb,50.0], [alyssa,55.0], [aiden,59.0], [chloe,60.0], [natalie,64.0], [evan,66.0], [isaiah,68.0], [brianna,71.0], [gavin,72.0], [riley,73.0], [connor,76.0], [kayla,78.0], [hailey,82.0], [ella,85.0], [landon,87.0], [aidan,90.0], [jasmine,93.0], [liam,97.0], [avery,98.0], [addison,101.0], [lily,104.0], [nathaniel,108.0], [jeremiah,111.0], [hayden,112.0], [brayden,113.0], [katherine,114.0], [allison,116.0], [kaitlyn,119.0], [wyatt,120.0], [kaylee,121.0], [sebastian,124.0], [peyton,125.0], [megan,126.0], [alexandra,128.0], [lillian,130.0], [xavier,132.0]...poso: Array[org.apache.spark.sql.Row] \u003d Array([a plus,1], [abound,11], [abounds,12], [abundance,25], [abundant,26], [accessable,34], [accessible,35], [acclaim,37], [acclaimed,38], [acclamation,39], [accolade,40], [accolades,41], [accommodative,42], [accomodative,43], [accomplish,44], [accomplished,45], [accomplishment,46], [accomplishments,47], [accurate,49], [accurately,50], [achievable,65], [achievement,66], [achievements,67], [achievible,68], [acumen,76], [adaptable,79], [adaptive,80], [adequate,85], [adjustable,86], [admirable,87], [admirably,88], [admiration,89], [admire,90], [admirer,91], [admiring,92], [admiringly,93], [adorable,99], [adore,100], [adored,101], [adorer,102], [adoring,103], [adoringly,104], [adroit,105], [adroitly,106], [adulate,107], [adulation,108], [adulatory,10...nego: Array[org.apache.spark.sql.Row] \u003d Array([abnormal,2], [abolish,3], [abominable,4], [abominably,5], [abominate,6], [abomination,7], [abort,8], [aborted,9], [aborts,10], [abrade,13], [abrasive,14], [abrupt,15], [abruptly,16], [abscond,17], [absence,18], [absent minded,19], [absentee,20], [absurd,21], [absurdity,22], [absurdly,23], [absurdness,24], [abuse,27], [abused,28], [abuses,29], [abusive,30], [abysmal,31], [abysmally,32], [abyss,33], [accidental,36], [accost,48], [accursed,51], [accusation,52], [accusations,53], [accuse,54], [accuses,55], [accusing,56], [accusingly,57], [acerbate,58], [acerbic,59], [acerbically,60], [ache,61], [ached,62], [aches,63], [achey,64], [aching,69], [acrid,70], [acridly,71], [acridness,72], [acrimonious,73], [acrimoniously,74], [acrimony,75], [adamant...ids: Array[org.apache.spark.sql.Row] \u003d Array([cybersecurity,9], [cyber security,10], [cyber-security,11], [cybercrime,12], [cyber crime,13], [cyber espionage,14], [egovernment,15], [e-government,16], [cybercom,17], [cyber command,18], [cyber operations,19], [cyber control,20], [cyberspace,21], [threatbase,22], [threat intelligence,23], [vulnerability management,24], [mobile security,25], [information protection,26], [application security,27], [supplier security management,28], [postmorten analysis,29], [cyber attack,30], [cyberattack,31], [cyber-attack,32], [incidence response,33], [incident response,34], [continuous diagnostic mitigation,35], [continuous diagnostics,36], [intelligence driven operations,37], [predictive intelligence,38], [anticipatory threat intelligance,39], [cyber wor...p_syn: scala.collection.immutable.Set[String] \u003d Set(typhoid jab, stribid, ego electronic cigs, breo account, ego cig, whooping cough tetanus, neutrogena eyeliner, spencer forrest laser, breo watches, larazapam, tapentadole, meningococcal group b vaccine, klonipin, cig liquids, buprenurphine, cloudchaser, tapentidal, dabigatran, centruroides, prednisolene, oramoprh, percocets, rebiff, mumps vaccination, ms cotin, pantoprazol, bozara, typhoid, arcapta, victanyle, bed time meds, nurophen, sublimayze, toodle puffing, immuran, halo e cigs, lynlore, alieve, xana x, ibrofen, omniprazole, claretin, loropom, powersail coronary dilation catheter, u47, evra patch, tylenol widd codeine, clonapin, co codamole, tylenol   4, levulan, markten cigarettes, percets, e vapes, flonaize, e shisha, msl, zafen...s_syn: scala.collection.immutable.Set[String] \u003d Set(red circles around my eyes, ill just, bad reactions, piss my pants, inflammatory, got me seeing shit, losing it s benefits, \" hasnt touched my migraine \", arm hurtz, arm is so swollen, \" still hasnt taken effect \", peripheral edema, peripheral coldness, swollen breast, doze off, room spinning, anti neutrophil cytoplasmic antibody positive vasculitis, rage, loopin me out, nothing is happening, sleeping most of the day, irritated eyelid, leg hurts, my appetite is gone, no effect on you any more, black out, laughing, stevens johnsons, \" still dont take my headaches away \", serum sickness like reaction, blurry, pregnant on birth control, got me feeling some typa way, took me out on disability, yacking, no way to fight off infections, \" won...u_syn: scala.collection.immutable.Set[String] \u003d Set(need sold today, was taking, nic base, from tysabri, headache, liquid, used, cloudchaser, clearance sale, has been pretty good, pharma, immunosuppressive, so far so good, company coverage, chargers, been a blessing, successful infusion, beautiful, mojito, what are, ohm, birthday, dont want to stop using e cigs, to humira, teenager, dangerous, one stop shop, started my entyvio, smells like shit, savings, menthol, not sure if this is a side effect, researcher, can you advise me, no side effects, tastes awesome, topper, insurance not approved, outlaw, remission rate, need to decice about stopping nicotine, swag, in a lot of pain, it has been a long time since i felt this good, cleaning your, scares me, positive reaction to entyvio, drip t...pi_syn: scala.collection.immutable.Set[String] \u003d Set(4854.0, 6090.0, 17105.0, 14139.0, 27107.0, 13562.0, 18721.0, 9709.0, 3782.0, 14630.0, 31624.0, 15717.0, 13774.0, 9720.0, 26609.0, 20042.0, 8876.0, 35208.0, 34662.0, 28772.0, 11189.0, 5573.0, 13444.0, 140.0, 4841.0, 1855.0, 19844.0, 5667.0, 8802.0, 20122.0, 738.0, 7446.0, 7549.0, 7627.0, 19914.0, 27533.0, 13934.0, 1375.0, 31152.0, 27083.0, 26207.0, 15676.0, 7384.0, 27642.0, 14436.0, 35202.0, 10063.0, 799.0, 35193.0, 3340.0, 355.0, 32590.0, 31354.0, 9347.0, 30639.0, 14281.0, 33068.0, 7080.0, 30579.0, 17553.0, 4549.0, 7250.0, 918.0, 17055.0, 2657.0, 5861.0, 27146.0, 15503.0, 7938.0, 2744.0, 12195.0, 1608.0, 18967.0, 17187.0, 12660.0, 15027.0, 32313.0, 31543.0, 8466.0, 15397.0, 2761.0, 20995.0, 18562.0, 27180.0, 19608.0, 33912.0, 31382.0,...comp_syn: scala.collection.immutable.Set[String] \u003d Set(epidemico, clark freifeld, pricewaterhousecooper, nabarun dasgupta, booz allen, pricewaterhousecoopers, price waterhouse, wholly owned subsidiary booz allen hamilton, booze allen, spinoff boston childrens hospital, medwatcher, ibmwatson, healthmap, streetrx, raytheon, deloitte, ibm, accenture, robin heffernan, john brownstein)\ntop_syn: scala.collection.immutable.Set[String] \u003d Set(mobile apps, data scientist, data veracity, digitalgov, service design, brand strategy, health, devops, digital listening, supplier security management, cyber workforce, channel alignment, shared services, aws, design, data scientists, digital strategy, open data, mobile security, tradecraft, data valuation, unstructured data, social media analytics, big data, hacker, data lake, cybercom, cyber activity, hackathon, health informatics, web apps, continuous diagnostic mitigation, web applications, application security, digital transformation, interactive media, threatbase, content management system, iot, cloud applications, cyber challenge, iaas, semi structured data, spatial design, digital publications, citizen services, cyber crimin...des_syn: scala.collection.immutable.Set[String] \u003d Set(pdcov, mycoplasma bovis, peritonsillar abscess, australian encephalitis, cre, groundnut ringspot virus, diarrhea acute unspecified, die off, bite spider, kala azar, squirrel pox, not diagnosed disease, potato wart, lactose intolerance post infectious, pontic fever, brugia malayi, chytrid fungus, club root, mystery deadly illness, adem, mycobacterium abscessus, pneumonia fungal, rhinovirus, agalactia, aster yellows, cryptococcal, cholera infected swine, helminth intestinal not diarrhea unspecified, type specific stroke, dyscentry, shigella s dysenteriae, evh 1, urethritis non gonococcal, shot dead, rickettsia typhi flea borne murine typhus, influenza a confirmed, melamine, pseudotuberculosis, colitis unspecified, \"fmd \", ah1n1, kalaza...pos_syn: scala.collection.immutable.Set[String] \u003d Set(foolproof, youthful, precious, compliment, lover, snazzy, plentiful, exuberance, outshine, easiness, excelent, unequivocally, steadfastness, capably, accolade, unity, sweet, lush, striking, futuristic, admiringly, vivid, beautiful, blissfully, astutely, steadfastly, outstrip, innocuous, astonishment, carefree, panoramic, masters, empathize, adulation, euphoria, wow, clear cut, wise, terrifically, succeed, easygoing, perfection, liberate, savings, enjoying, spirited, interesting, verifiable, shiny, proficiently, fragrant, rapture, spellbound, picturesque, comely, dumbfounded, smitten, luxury, thrilling, winning, authoritative, unfettered, razor sharp, intricate, impressive, supurbly, stellar, sturdy, elan, reassure, faithfulness, trum...neg_syn: scala.collection.immutable.Set[String] \u003d Set(ferociously, sinister, breaks, bowdlerize, leer, inflammatory, cataclysmal, outrageousness, distraughtness, malignant, terrible, inevitable, rage, sugarcoated, lesser known, scathing, headache, desolate, polluter, disinclined, lawbreaker, irrecoverably, darkened, despotic, buckle, discomfit, dejection, blurry, blurs, scare, overdue, exploit, extraneous, incomprehensible, molestation, insufficiency, affront, craven, tiring, break up, backbiting, static, inferiority, frown, garish, unhappiness, sack, gracelessly, unobserved, manipulative, subservient, degradation, straining, slowwww, sully, rhetorical, uninsured, abyss, bitchy, inefficiency, demolish, boil, timidly, crafty, funny, joker, lonesome, devastating, pillage, calamitous, kill...id_syn: scala.collection.immutable.Set[String] \u003d Set(ferociously, sinister, breaks, bowdlerize, leer, inflammatory, cataclysmal, outrageousness, distraughtness, malignant, terrible, inevitable, rage, sugarcoated, lesser known, scathing, headache, desolate, polluter, disinclined, lawbreaker, irrecoverably, darkened, despotic, buckle, discomfit, dejection, blurry, blurs, scare, overdue, exploit, extraneous, incomprehensible, molestation, insufficiency, affront, craven, tiring, break up, backbiting, static, inferiority, frown, garish, unhappiness, sack, gracelessly, unobserved, manipulative, subservient, degradation, straining, slowwww, sully, rhetorical, uninsured, abyss, bitchy, inefficiency, demolish, boil, timidly, crafty, funny, joker, lonesome, devastating, pillage, calamitous, kille...pa: Map[String,String] \u003d Map()\nsa: Map[String,String] \u003d Map()\nua: Map[String,String] \u003d Map()\npia: Map[String,String] \u003d Map()\nca: Map[String,String] \u003d Map()\nta: Map[String,String] \u003d Map()\ndas: Map[String,String] \u003d Map()\ncas: Map[String,String] \u003d Map()\nsas: Map[String,String] \u003d Map()\npas: Map[String,String] \u003d Map()\nnas: Map[String,String] \u003d Map()\nias: Map[String,String] \u003d Map()\nns: Array[Int] \u003d Array(15)\nnsize: Int \u003d 15\n"
      },
      "dateCreated": "Sep 28, 2016 8:59:12 PM",
      "dateStarted": "Oct 5, 2016 9:09:57 PM",
      "dateFinished": "Oct 5, 2016 9:11:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/*\nvar forum \u003d sqlContext.jsonFile(\"/opt/sup/RS_2015/\")\n.withColumn(\"created_utc\", from_unixtime($\"created_utc\".cast(\"String\")))\n.withColumn(\"ymds\", date_format($\"created_utc\", \"yyyyMMddss\").cast(\"Integer\"))\n.withColumn(\"ds\", when($\"ymds\"\u003d\u003d\u003d20160500, \"pushshift\").otherwise(\"pushshift\"))\n.withColumn(\"type\", when($\"ymds\"\u003d\u003d\u003d20160500, \"reddit\").otherwise(\"reddit\"))\n*/\n    val make_flink \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else Array(\"https://www.reddit.com/r/\", text).mkString(\"\")})\n    \nvar forum2 \u003d forum.withColumn(\"t\", concat_ws(\" \", $\"selftext\", $\"url\")).drop(\"url\").withColumn(\"url\", $\"permalink\").withColumnRenamed(\"name\", \"tid\").withColumn(\"tlink\", $\"url\").withColumnRenamed(\"uname\", \"author\").withColumn(\"dom\", when($\"ymds\"\u003d\u003d\u003d\"20150600\", \"reddit.com\").otherwise(\"reddit.com\")).withColumn(\"bname\", when($\"ymds\"\u003d\u003d\u003d\"20150600\", \"Reddit\").otherwise(\"Reddit\")).withColumn(\"cr\", $\"created_utc\".cast(\"timestamp\")).withColumn(\"rdt\",  from_unixtime($\"retrieved_on\").cast(\"timestamp\")).withColumn(\"flink\", make_flink($\"subreddit\")).withColumn(\"fname\", $\"subreddit\").withColumn(\"tstarter\", when($\"ymds\"\u003d\u003d\u003d20150600, \"T\").otherwise(\"T\")).withColumnRenamed(\"author\", \"uname\").filter(\"ymds is not null\").na.fill(0,Seq(\"num_comments\")).withColumn(\"downs\", $\"score\" - $\"ups\").select(\"type\",\"ds\",\"rdt\",\"ymds\",\"id\",\"bname\",\"cr\",\"dom\",\"downs\",\"flink\",\"fname\",\"num_comments\",\"score\",\"t\",\"tid\",\"title\",\"tlink\",\"tstarter\",\"uname\",\"ups\",\"url\")\n\nforum2 \u003d forum2.withColumn(\"t2\", regexp_replace(lower($\"t\"), \"\"\"[\\p{Punct}]\"\"\", \" \"))\n\nimport org.apache.spark.ml.feature.NGram\nimport org.apache.spark.ml.feature.Tokenizer\nval tokenizer \u003d new Tokenizer().setInputCol(\"t2\").setOutputCol(\"words\")\nvar tokenized \u003d tokenizer.transform(forum2)\nfor(i \u003c- 1 to nsize){\n    var num \u003d Array(\"ngram\", i).mkString\n    var ngi \u003d new NGram().setInputCol(\"words\").setOutputCol(num).setN(i)\n    tokenized \u003d ngi.transform(tokenized)\n    tokenized \u003d tokenized.withColumn(\"test\", tokenized(num).cast(\"String\")).withColumn(\"w2\", concat($\"words\".cast(\"String\"), $\"test\")).drop(num).drop(\"test\")\n}\nval matcher \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else {\n    val words \u003d text.split(\",\").toSet\n    val pc \u003d p_syn \u0026 words\n    val cc \u003d s_syn \u0026 words\n    val uc \u003d u_syn \u0026 words\n    val pic \u003d pi_syn \u0026 words\n    val coc \u003d comp_syn \u0026 words\n    val toc \u003d top_syn \u0026 words\n    val doc \u003d des_syn \u0026 words\n//    val joc \u003d cat_syn \u0026 words\n//    val soc \u003d spec_syn \u0026 words\n    val poc \u003d pos_syn \u0026 words\n    val noc \u003d neg_syn \u0026 words\n    Map(\"prod\" -\u003e (pc.toArray collect pa).distinct.map(_.toDouble),\n\"prod_syn\" -\u003e (pc.toArray collect ias).distinct.map(_.toDouble),\n \"con\" -\u003e (cc.toArray collect sa).distinct.map(_.toDouble),\n\"con_syn\" -\u003e (cc.toArray collect ias).distinct.map(_.toDouble),\n \"ue\" -\u003e (uc.toArray collect ua).distinct.map(_.toDouble),\n\"ue_syn\" -\u003e (uc.toArray collect ias).distinct.map(_.toDouble),\n\"pii\" -\u003e (pic.toArray collect pia).distinct.map(_.toDouble),\n\"company\" -\u003e (coc.toArray collect ca).distinct.map(_.toDouble),\n\"company_syn\" -\u003e (coc.toArray collect ias).distinct.map(_.toDouble),\n\"topic\" -\u003e (toc.toArray collect ta).distinct.map(_.toDouble),\n\"keys\" -\u003e (toc.toArray collect ias).distinct.map(_.toDouble),\n\"disease\" -\u003e (doc.toArray collect das).distinct.map(_.toDouble),\n\"disease_syn\" -\u003e (doc.toArray collect ias).distinct.map(_.toDouble), \n//\"category\" -\u003e (joc.toArray collect cas).distinct.map(_.toDouble),\n//\"category_syn\" -\u003e (joc.toArray collect ias).distinct.map(_.toDouble), \n//\"species\" -\u003e (soc.toArray collect sas).distinct.map(_.toDouble),\n//\"species_syn\" -\u003e (soc.toArray collect ias).distinct.map(_.toDouble),\n\"pos\" -\u003e (poc.toArray collect pas).map(_.toDouble), \n\"neg\" -\u003e (noc.toArray collect nas).map(_.toDouble))\n}})\nval to_double \u003d udf((text: WrappedArray[String]) \u003d\u003e {\n    if(text.length\u003d\u003d0) null\n    else text.map(_.toDouble)\n})\nval end \u003d tokenized.withColumn(\"temp\", matcher($\"w2\")).\nwithColumn(\"product\", $\"temp\"(\"prod\")).\nwithColumn(\"product_synonyms\", $\"temp\"(\"prod_syn\")).\nwithColumn(\"symptom\", $\"temp\"(\"con\")).\nwithColumn(\"symptom_synonyms\", $\"temp\"(\"con_syn\")).\nwithColumn(\"ue\", $\"temp\"(\"ue\")).\nwithColumn(\"ue_synonyms\", $\"temp\"(\"ue_syn\")).\nwithColumn(\"pii\", $\"temp\"(\"pii\")).\n//withColumn(\"pii_synonyms\", $\"temp\"(\"pii_syn\"))\nwithColumn(\"organization\", $\"temp\"(\"company\")).\nwithColumn(\"organization_synonyms\", $\"temp\"(\"company_syn\")).\nwithColumn(\"business_category\", $\"temp\"(\"topic\")).\nwithColumn(\"business_category_synonyms\", $\"temp\"(\"keys\")).\nwithColumn(\"disease\", $\"temp\"(\"disease\")).\nwithColumn(\"disease_synonyms\", $\"temp\"(\"disease_syn\")).\n//withColumn(\"category\", $\"temp\"(\"category\")).\n//withColumn(\"category_synonyms\", $\"temp\"(\"category_syn\")).\n//withColumn(\"species\", $\"temp\"(\"species\")).\n//withColumn(\"species_synonyms\", $\"temp\"(\"species_syn\")).\nwithColumn(\"pos\",  $\"temp\"(\"pos\")).\nwithColumn(\"neg\", $\"temp\"(\"neg\")).drop(\"w2\").drop(\"temp\").drop(\"words\")\nimport org.apache.spark.ml.PipelineModel\nval yolo \u003d PipelineModel.load(\"/opt/syed/AeClassifier.model\")\nval class_s \u003d udf((prediction: Double, ue: WrappedArray[Double]) \u003d\u003e {\n    if(ue \u003d\u003d null) 1.0\n  else if (ue.length \u003d\u003d 0) 1.0\n  else if (ue.length !\u003d0 \u0026\u0026 prediction \u003d\u003d 0.0D) 2.0\n  else if (ue.length !\u003d0 \u0026\u0026 prediction \u003d\u003d 1.0D) 7.0\n  else 1.0\n})\nval ind \u003d udf((ue: org.apache.spark.mllib.linalg.Vector) \u003d\u003e {\n    ue.toArray.max\n})\nval senti \u003d udf((sen: Int) \u003d\u003e {\n    if(sen \u003d\u003d0) 0\n    else if (sen\u003e0) 1\n    else -1\n})\nval end20 \u003d yolo.transform(end).drop(\"rawPrediction\").drop(\"words\").drop(\"features\").drop(\"filtered\").withColumn(\"ind\", ind($\"probability\").cast(\"Double\")).drop(\"probability\").withColumn(\"tg\", class_s($\"prediction\", $\"ue\")).drop(\"t2\").withColumn(\"sentiment\", senti((size($\"pos\") - size($\"neg\")))).drop(\"pos\").drop(\"neg\").drop(\"prediction\")\n\nend20.write.format(\"org.apache.spark.sql.cassandra\").mode(\"append\").options(Map( \"table\" -\u003e \"reddit\", \"keyspace\" -\u003e \"processed_forum\")).save()",
      "dateUpdated": "Oct 6, 2016 12:42:48 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475676539015_782477808",
      "id": "20161005-140859_2060180787",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "make_flink: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,List(StringType))\nforum2: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string]\nforum2: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string, t2: string]\nimport org.apache.spark.ml.feature.NGram\nimport org.apache.spark.ml.feature.Tokenizer\ntokenizer: org.apache.spark.ml.feature.Tokenizer \u003d tok_e1f0b3c1d103\ntokenized: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string, t2: string, words: array\u003cstring\u003e]\nmatcher: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,MapType(StringType,ArrayType(DoubleType,false),true),List(StringType))\nto_double: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,ArrayType(DoubleType,false),List(ArrayType(StringType,true)))\nend: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string, t2: string, product: array\u003cdouble\u003e, product_synonyms: array\u003cdouble\u003e, symptom: array\u003cdouble\u003e, symptom_synonyms: array\u003cdouble\u003e, ue: array\u003cdouble\u003e, ue_synonyms: array\u003cdouble\u003e, pii: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, organization_synonyms: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, business_category_synonyms: array\u003cdouble\u003e, disease: array\u003cdouble\u003e, disease_synonyms: array\u003cdouble\u003e, pos: array\u003cdouble\u003e, neg: array\u003cdouble\u003e]\nimport org.apache.spark.ml.PipelineModel\nyolo: org.apache.spark.ml.PipelineModel \u003d pipeline_e6ec2d152454\nclass_s: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction2\u003e,DoubleType,List(DoubleType, ArrayType(DoubleType,false)))\nind: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,DoubleType,List(org.apache.spark.mllib.linalg.VectorUDT@f71b0bce))\nsenti: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,IntegerType,List(IntegerType))\nend20: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string, product: array\u003cdouble\u003e, product_synonyms: array\u003cdouble\u003e, symptom: array\u003cdouble\u003e, symptom_synonyms: array\u003cdouble\u003e, ue: array\u003cdouble\u003e, ue_synonyms: array\u003cdouble\u003e, pii: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, organization_synonyms: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, business_category_synonyms: array\u003cdouble\u003e, disease: array\u003cdouble\u003e, disease_synonyms: array\u003cdouble\u003e, ind: double, tg: double, sentiment: int]\norg.apache.spark.SparkException: Job 83 cancelled part of cancelled job group zeppelin-20161005-140859_2060180787\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:37)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:67)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:85)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:173)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:178)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:180)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:182)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:184)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:186)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:188)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:190)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:192)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:194)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:196)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:198)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:200)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:202)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:204)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:206)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:208)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:210)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:212)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:214)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:216)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:218)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:220)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:222)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:224)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:226)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:228)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:230)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:232)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:234)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:236)\n\tat \u003cinit\u003e(\u003cconsole\u003e:238)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:242)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:810)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:753)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:746)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Oct 5, 2016 2:08:59 PM",
      "dateStarted": "Oct 5, 2016 11:42:56 PM",
      "dateFinished": "Oct 6, 2016 1:13:08 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475710513393_1848538451",
      "id": "20161005-233513_2032488496",
      "dateCreated": "Oct 5, 2016 11:35:13 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "testing zika fentanyl",
  "id": "2BWB98MYD",
  "angularObjects": {
    "2BTJ3P41C:shared_process": [],
    "2BUSJ5B5T:shared_process": [],
    "2BUCYP1D2:shared_process": [],
    "2BRYFEHJ7:shared_process": [],
    "2BT211CDH:shared_process": [],
    "2BTX1MQS6:shared_process": [],
    "2BU87RU3U:shared_process": [],
    "2BTB82RPQ:shared_process": [],
    "2BRP3ZUWJ:shared_process": [],
    "2BT3JK3T4:shared_process": [],
    "2BUTB2HA6:shared_process": [],
    "2BTXGDVEJ:shared_process": [],
    "2BRZ896X6:shared_process": [],
    "2BSSEDUXN:shared_process": [],
    "2BSMJA8VG:shared_process": [],
    "2BSJYK2YE:shared_process": [],
    "2BUZX9EWW:shared_process": []
  },
  "config": {},
  "info": {}
}