{
  "paragraphs": [
    {
      "text": "import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.SparkContext\nimport scala.collection.mutable.WrappedArray",
      "dateUpdated": "Oct 19, 2016 4:33:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475081342751_1104865450",
      "id": "20160928-164902_1988955397",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.SparkContext\nimport scala.collection.mutable.WrappedArray\n"
      },
      "dateCreated": "Sep 28, 2016 4:49:02 PM",
      "dateStarted": "Oct 19, 2016 4:33:53 PM",
      "dateFinished": "Oct 19, 2016 4:34:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val sup \u003d udf((a: WrappedArray[Double], b: WrappedArray[Double], c: WrappedArray[Double]) \u003d\u003e {\n    a ++ b ++ c\n})",
      "dateUpdated": "Oct 19, 2016 4:34:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476839969656_235652467",
      "id": "20161019-011929_1029924341",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "sup: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction3\u003e,ArrayType(DoubleType,false),List(ArrayType(DoubleType,false), ArrayType(DoubleType,false), ArrayType(DoubleType,false)))\n"
      },
      "dateCreated": "Oct 19, 2016 1:19:29 AM",
      "dateStarted": "Oct 19, 2016 4:34:25 PM",
      "dateFinished": "Oct 19, 2016 4:34:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"news\", \"keyspace\" -\u003e \"test_news\"))\n  .load()\n  .select(\"id\", \"disease\", \"product\", \"organization\", \"business_category\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"impact\", \"ds\")\n  .withColumn(\"source\", lit(\"news\"))\n  .withColumn(\"topic\", sup($\"disease\", $\"product\", $\"organization\"))\n  \nvar df2 \u003d df.filter((size($\"disease\")\u003e0 || size($\"organization\")\u003e0 || size($\"product\")\u003e0) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\").drop(\"product\").drop(\"disease\").drop(\"organization\")\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"twitter\", \"keyspace\" -\u003e \"processed_social\"))\n  .load()\n  .select(\"id\", \"disease\", \"product\", \"organization\", \"business_category\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"klout\", \"ds\")\n  .withColumnRenamed(\"klout\", \"impact\")\n  .withColumn(\"source\", lit(\"twitter\"))\n  .withColumn(\"topic\", sup($\"disease\", $\"product\", $\"organization\"))\n  \ndf \u003d df.filter((size($\"disease\")\u003e0 || size($\"organization\")\u003e0 || size($\"product\")\u003e0) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\").drop(\"product\").drop(\"disease\").drop(\"organization\")\n\ndf2 \u003d df2.unionAll(df)\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"reddit\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\",\"disease\", \"product\", \"organization\", \"business_category\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"ups\", \"ds\")\n  .withColumnRenamed(\"ups\", \"impact\")\n  .withColumn(\"source\", lit(\"reddit\"))\n  .withColumn(\"topic\", sup($\"disease\", $\"product\", $\"organization\"))\n  \ndf \u003d df.filter((size($\"disease\")\u003e0 || size($\"organization\")\u003e0 || size($\"product\")\u003e0) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\").drop(\"product\").drop(\"disease\").drop(\"organization\")\n\ndf2 \u003d df2.unionAll(df)\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"forum\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\",\"disease\", \"product\", \"organization\", \"business_category\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"ups\", \"ds\")\n  .withColumnRenamed(\"ups\", \"impact\")\n  .withColumn(\"source\", lit(\"forum\"))\n  .withColumn(\"topic\", sup($\"disease\", $\"product\", $\"organization\"))\n  \ndf \u003d df.filter((size($\"disease\")\u003e0 || size($\"organization\")\u003e0 || size($\"product\")\u003e0) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\").drop(\"product\").drop(\"disease\").drop(\"organization\")\n\ndf2 \u003d df2.unionAll(df)",
      "dateUpdated": "Oct 19, 2016 4:34:30 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476839723104_397679371",
      "id": "20161019-011523_1976542390",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, product: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, product: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, product: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, product: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, business_category: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string, topic: array\u003cdouble\u003e]\n"
      },
      "dateCreated": "Oct 19, 2016 1:15:23 AM",
      "dateStarted": "Oct 19, 2016 4:34:31 PM",
      "dateFinished": "Oct 19, 2016 4:34:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.show()",
      "dateUpdated": "Oct 19, 2016 4:34:44 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476840183988_1963265329",
      "id": "20161019-012303_182846984",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+-----------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+---------+------+--------------------+\n|                  id|business_category|                 url|         description|               title|sentiment|          place_list|                  cr|impact|       ds|source|               topic|\n+--------------------+-----------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+---------+------+--------------------+\n|http://www.mercur...|          [158.0]|http://www.mercur...|She was the last ...|The short lives o...|       -1|                  []|2016-04-02 08:54:...|     5|   scrapy|  news|           [24328.0]|\n|http://www.swcbul...|           [84.0]|http://www.swcbul...|School District 8...|District secures ...|        1|                  []|2016-05-13 23:21:...|     0|   scrapy|  news|           [24328.0]|\n|http://www.swcbul...|           [84.0]|http://www.swcbul...|School District 8...|District secures ...|        1|                  []|2016-05-13 23:21:...|     0|   scrapy|  news|           [24328.0]|\n|http://www.swcbul...|           [84.0]|http://www.swcbul...|School District 8...|District secures ...|        1|                  []|2016-05-13 23:21:...|     0|   scrapy|  news|           [24328.0]|\n|http://www.swcbul...|           [84.0]|http://www.swcbul...|School District 8...|District secures ...|        1|[[5279468.0,null,...|2016-05-13 23:21:...|     0|   scrapy|  news|           [24328.0]|\n|http://www.swcbul...|           [84.0]|http://www.swcbul...|School District 8...|District secures ...|        1|[[5279468.0,null,...|2016-05-13 23:21:...|  null|   scrapy|  news|           [24328.0]|\n|http://www.boulde...|               []|http://www.boulde...|Denver Homeless O...|    Letters: 3/24/16|       -1|                  []|2016-03-24 15:07:...|     0|   scrapy|  news|[21769.0, 24627.0...|\n|http://www.boulde...|               []|http://www.boulde...|Denver Homeless O...|    Letters: 3/24/16|       -1|[[5417618.0,null,...|2016-03-24 15:07:...|     0|   scrapy|  news|[21769.0, 24627.0...|\n|http://www.afr.co...|          [158.0]|http://www.afr.co...|To truly transfor...|Transforming heal...|        1|                  []|2016-07-11 08:25:...|     5|   scrapy|  news|    [24841.0, 210.0]|\n|http://www.afr.co...|          [158.0]|http://www.afr.co...|To truly transfor...|Transforming heal...|        1|                  []|2016-07-11 08:25:...|     5|   scrapy|  news|    [24841.0, 210.0]|\n|http://www.afr.co...|          [158.0]|http://www.afr.co...|To truly transfor...|Transforming heal...|        1|                  []|2016-07-11 08:25:...|  null|   scrapy|  news|    [24841.0, 210.0]|\n|http://www.afr.co...|          [158.0]|http://www.afr.co...|To truly transfor...|Transforming heal...|        1|                  []|2016-07-11 08:25:...|     5|   scrapy|  news|    [24841.0, 210.0]|\n|http://www.sunad....|          [158.0]|http://www.sunad....|•••Job’s Daughter...|  Notices 04-07-2016|        1|                  []|2016-04-07 21:48:...|     0|   scrapy|  news|           [24841.0]|\n|http://www.laraza...|               []|http://www.laraza...|Los diamantes sin...|Los anillos de co...|        1|                  []|2016-03-31 16:09:...|     0|   scrapy|  news|           [26238.0]|\n|             4101663|               []|http://www.ccente...|Police probing gu...|Police probing gu...|       -1|[[null,232,New Je...|2016-03-17 13:26:...|     0|HealthMap|  news|           [25598.0]|\n|             4101873|          [158.0]|http://colombiare...|Colombia testing ...|Colombia testing ...|       -1|[[null,16,Colombi...|2016-03-17 13:32:...|     0|HealthMap|  news|           [26696.0]|\n|             4102442|          [158.0]|http://www.cdc.go...|      No description|Revision to CDC’s...|       -1|[[null,106,United...|2016-03-17 18:03:...|     0|HealthMap|  news|           [26696.0]|\n|             4265238|               []|http://chicago.su...|4 dead, 18 wounde...|4 dead, 18 wounde...|       -1|[[null,216,Illino...|2016-06-04 08:03:...|     0|HealthMap|  news|           [25598.0]|\n|             4265944|               []|http://gdnonline....|Abandon silos to ...|Abandon silos to ...|        1|[[null,null,null,...|2016-06-04 20:06:...|     0|HealthMap|  news|             [207.0]|\n|             4267956|               []|http://www.latrib...|\u003ctable border\u003d\"0\"...|Madre de beb\u0026eacu...|       -1|[[null,232,New Je...|2016-06-04 16:54:...|     0|HealthMap|  news|           [26696.0]|\n+--------------------+-----------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+---------+------+--------------------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Oct 19, 2016 1:23:03 AM",
      "dateStarted": "Oct 19, 2016 4:34:44 PM",
      "dateFinished": "Oct 19, 2016 4:34:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.drop(\"place_list\").createCassandraTable(\"epione\", \"agg_table\", \npartitionKeyColumns \u003d Some(Seq(\"id\", \"source\", \"cr\")))",
      "dateUpdated": "Oct 19, 2016 1:55:55 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476840632102_-747983549",
      "id": "20161019-013032_1807249840",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "com.datastax.driver.core.exceptions.AlreadyExistsException: Table epione.agg_table already exists\n\tat com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:111)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:39)\n\tat sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy21.execute(Unknown Source)\n\tat sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy21.execute(Unknown Source)\n\tat com.datastax.spark.connector.DataFrameFunctions$$anonfun$createCassandraTable$1.apply(DataFrameFunctions.scala:49)\n\tat com.datastax.spark.connector.DataFrameFunctions$$anonfun$createCassandraTable$1.apply(DataFrameFunctions.scala:49)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:110)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:139)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.DataFrameFunctions.createCassandraTable(DataFrameFunctions.scala:49)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:49)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:55)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:57)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:59)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:61)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:63)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:65)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:67)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:69)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:71)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:73)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:75)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:77)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:79)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:81)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:83)\n\tat \u003cinit\u003e(\u003cconsole\u003e:85)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:89)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:810)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:753)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:746)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Table epione.agg_table already exists\n\tat com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:130)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:140)\n\tat com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:179)\n\tat com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:184)\n\tat com.datastax.driver.core.RequestHandler.access$2500(RequestHandler.java:43)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:798)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:617)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Table epione.agg_table already exists\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:85)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\n"
      },
      "dateCreated": "Oct 19, 2016 1:30:32 AM",
      "dateStarted": "Oct 19, 2016 1:55:42 AM",
      "dateFinished": "Oct 19, 2016 1:55:42 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.count()",
      "dateUpdated": "Oct 19, 2016 1:25:29 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476840324294_1584888594",
      "id": "20161019-012524_1718495347",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 6, localhost): java.io.IOException: Exception during execution of SELECT \"disease\", \"organization\", \"product\", \"cr\" FROM \"test_news\".\"news\" WHERE token(\"ds\", \"ymds\") \u003e ? AND token(\"ds\", \"ymds\") \u003c\u003d ?   ALLOW FILTERING: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:320)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:88)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy21.execute(Unknown Source)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:309)\n\t... 24 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:166)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1515)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1514)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)\n\tat org.apache.spark.sql.DataFrame.count(DataFrame.scala:1514)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:49)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:54)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:56)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:60)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:62)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:64)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:66)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:68)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:70)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:72)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:74)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:76)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:78)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:80)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:82)\n\tat \u003cinit\u003e(\u003cconsole\u003e:84)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:88)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:810)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:753)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:746)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Exception during execution of SELECT \"disease\", \"organization\", \"product\", \"cr\" FROM \"test_news\".\"news\" WHERE token(\"ds\", \"ymds\") \u003e ? AND token(\"ds\", \"ymds\") \u003c\u003d ?   ALLOW FILTERING: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:320)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:88)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\t... 3 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy21.execute(Unknown Source)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:309)\n\t... 24 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\n"
      },
      "dateCreated": "Oct 19, 2016 1:25:24 AM",
      "dateStarted": "Oct 19, 2016 1:25:29 AM",
      "dateFinished": "Oct 19, 2016 1:26:25 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"news\", \"keyspace\" -\u003e \"test_news\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"impact\", \"ds\")\n  .withColumn(\"source\", lit(\"news\"))\n  \nvar df2 \u003d df.filter(array_contains(df(\"disease\"), 26696) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-02-01\" \u0026\u0026  $\"cr\" \u003c\u003d \"2016-04-01\")\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"twitter\", \"keyspace\" -\u003e \"processed_social\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"klout\", \"ds\")\n  .withColumnRenamed(\"klout\", \"impact\")\n  .withColumn(\"source\", lit(\"twitter\"))\n  \ndf \u003d df.filter(array_contains(df(\"disease\"), 26696) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-02-01\" \u0026\u0026  $\"cr\" \u003c\u003d \"2016-04-01\").limit(10000)\n\ndf2 \u003d df2.unionAll(df)\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"reddit\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"ups\", \"ds\")\n  .withColumnRenamed(\"ups\", \"impact\")\n  .withColumn(\"source\", lit(\"reddit\"))\n  \ndf \u003d df.filter(array_contains(df(\"disease\"), 26696) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-02-01\" \u0026\u0026  $\"cr\" \u003c\u003d \"2016-04-01\")\n\ndf2 \u003d df2.unionAll(df)\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"forum\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"ups\", \"ds\")\n  .withColumnRenamed(\"ups\", \"impact\")\n  .withColumn(\"source\", lit(\"forum\"))\n  \ndf \u003d df.filter(array_contains(df(\"disease\"), 26696) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-02-01\" \u0026\u0026  $\"cr\" \u003c\u003d \"2016-04-01\")\n\ndf2 \u003d df2.unionAll(df)",
      "dateUpdated": "Oct 7, 2016 8:42:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475081365934_861350045",
      "id": "20160928-164925_149591937",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\n"
      },
      "dateCreated": "Sep 28, 2016 4:49:25 PM",
      "dateStarted": "Oct 7, 2016 8:42:04 PM",
      "dateFinished": "Oct 7, 2016 8:42:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"twitter\", \"keyspace\" -\u003e \"processed_social\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"klout\", \"ds\")\n  .withColumnRenamed(\"klout\", \"impact\")\n  .withColumn(\"source\", lit(\"twitter\"))\n  \ndf \u003d df.filter(array_contains(df(\"disease\"), 26696) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-02-01\" \u0026\u0026  $\"cr\" \u003c\u003d \"2016-04-01\").limit(10000)",
      "dateUpdated": "Oct 7, 2016 8:50:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475547898008_1685318854",
      "id": "20161004-022458_556989548",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, ds: string, source: string]\n"
      },
      "dateCreated": "Oct 4, 2016 2:24:58 AM",
      "dateStarted": "Oct 7, 2016 8:50:02 PM",
      "dateFinished": "Oct 7, 2016 8:50:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.show()",
      "dateUpdated": "Oct 7, 2016 8:50:24 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475873421724_1386962910",
      "id": "20161007-205021_1655072503",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+------------------+---------+--------------------+--------------------+--------------------+---------+----------+--------------------+------+----+-------+\n|                id|  disease|                 url|         description|               title|sentiment|place_list|                  cr|impact|  ds| source|\n+------------------+---------+--------------------+--------------------+--------------------+---------+----------+--------------------+------+----+-------+\n|704428359657328641|[26696.0]|                null|#HealthyLiving: W...|#HealthyLiving: W...|       -1|        []|2016-02-29 22:09:...|  null|gnip|twitter|\n|704431885896658944|[26696.0]|                null|RT @RoseHorowitz3...|RT @RoseHorowitz3...|       -1|        []|2016-02-29 22:23:...|  null|gnip|twitter|\n|704442705800089601|[26696.0]|https://www.twitt...|RT @healthmap: IC...|RT @healthmap: IC...|        0|        []|2016-02-29 23:06:...|    37|gnip|twitter|\n|704454279516401664|[26696.0]|https://www.twitt...|RT @healthmap: IC...|RT @healthmap: IC...|        0|        []|2016-02-29 23:52:...|    54|gnip|twitter|\n|707581630467219457|[26696.0]|                null|WHO lists Zika R\u0026...|WHO lists Zika R\u0026...|        0|        []|2016-03-09 14:59:...|  null|gnip|twitter|\n|707584649321537536|[26696.0]|                null|WHO lists Zika R\u0026...|WHO lists Zika R\u0026...|        0|        []|2016-03-09 15:11:...|  null|gnip|twitter|\n|707585405965697025|[26696.0]|                null|WHO lists Zika R\u0026...|WHO lists Zika R\u0026...|        0|        []|2016-03-09 15:14:...|  null|gnip|twitter|\n|707660901780946944|[26696.0]|                null|No Zika vaccine f...|No Zika vaccine f...|        0|        []|2016-03-09 20:14:...|  null|gnip|twitter|\n|707662917223616512|[26696.0]|                null|RT @SBSNews: Zika...|RT @SBSNews: Zika...|        0|        []|2016-03-09 20:22:...|  null|gnip|twitter|\n|707666186859745280|[26696.0]|                null|#500aDay #1000aDa...|#500aDay #1000aDa...|        0|        []|2016-03-09 20:35:...|  null|gnip|twitter|\n|707666188193542144|[26696.0]|                null|No Zika vaccine f...|No Zika vaccine f...|        0|        []|2016-03-09 20:35:...|  null|gnip|twitter|\n|707677259725807616|[26696.0]|                null|No Zika vaccine f...|No Zika vaccine f...|        0|        []|2016-03-09 21:19:...|  null|gnip|twitter|\n|703480146804678656|[26696.0]|                null|#Research A UN Ex...|#Research A UN Ex...|       -1|        []|2016-02-27 07:21:...|  null|gnip|twitter|\n|703699089947492352|[26696.0]|                null|Meet London\u0027s Zik...|Meet London\u0027s Zik...|       -1|        []|2016-02-27 21:51:...|  null|gnip|twitter|\n|707179113715339265|[26696.0]|                null|Mayo Clinic Begin...|Mayo Clinic Begin...|       -1|        []|2016-03-08 12:20:...|  null|gnip|twitter|\n|707187167475335168|[26696.0]|                null|RT @GaviSeth: Pha...|RT @GaviSeth: Pha...|        1|        []|2016-03-08 12:52:...|  null|gnip|twitter|\n|695825834431639552|[26696.0]|https://www.twitt...|RT @johnbrownstei...|RT @johnbrownstei...|        1|        []|2016-02-06 04:26:...|    24|gnip|twitter|\n|695998221986963456|[26696.0]|https://www.twitt...|RT @galka_max: Ma...|RT @galka_max: Ma...|       -1|        []|2016-02-06 15:51:...|    30|gnip|twitter|\n|696001240501620737|[26696.0]|https://www.twitt...|RT @galka_max: Ma...|RT @galka_max: Ma...|       -1|        []|2016-02-06 16:03:...|    22|gnip|twitter|\n|714983726019698688|[26696.0]|                null|Florida is hardes...|Florida is hardes...|        0|        []|2016-03-30 01:12:...|  null|gnip|twitter|\n+------------------+---------+--------------------+--------------------+--------------------+---------+----------+--------------------+------+----+-------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Oct 7, 2016 8:50:21 PM",
      "dateStarted": "Oct 7, 2016 8:50:24 PM",
      "dateFinished": "Oct 7, 2016 8:51:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.drop(\"place_list\").write\n  .format(\"org.apache.spark.sql.cassandra\")\n  .mode(\"append\")\n  .options(Map( \"table\" -\u003e \"agg_table\", \"keyspace\" -\u003e \"epione\"))\n  .save()",
      "dateUpdated": "Oct 19, 2016 4:35:01 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475873497624_411178403",
      "id": "20161007-205137_157750849",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 7, 2016 8:51:37 PM",
      "dateStarted": "Oct 19, 2016 4:35:01 PM",
      "dateFinished": "Oct 19, 2016 4:41:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"news\", \"keyspace\" -\u003e \"test_news\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\", \"impact\")\n  .withColumn(\"source\", lit(\"news\"))",
      "dateUpdated": "Sep 29, 2016 7:21:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475175798380_-476688209",
      "id": "20160929-190318_292821281",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int, source: string]\n"
      },
      "dateCreated": "Sep 29, 2016 7:03:18 PM",
      "dateStarted": "Sep 29, 2016 7:21:15 PM",
      "dateFinished": "Sep 29, 2016 7:21:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.filter(\"impact is not null\").show()",
      "dateUpdated": "Sep 29, 2016 8:01:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475176877657_-102192377",
      "id": "20160929-192117_1017119892",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-------+---+-----------+-----+---------+----------+---+------+------+\n| id|disease|url|description|title|sentiment|place_list| cr|impact|source|\n+---+-------+---+-----------+-----+---------+----------+---+------+------+\n+---+-------+---+-----------+-----+---------+----------+---+------+------+\n\n"
      },
      "dateCreated": "Sep 29, 2016 7:21:17 PM",
      "dateStarted": "Sep 29, 2016 8:01:46 PM",
      "dateFinished": "Sep 29, 2016 8:01:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df2 \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"forum\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\")",
      "dateUpdated": "Sep 28, 2016 5:23:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475081481677_1497091501",
      "id": "20160928-165121_1339282674",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df2: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp]\n"
      },
      "dateCreated": "Sep 28, 2016 4:51:21 PM",
      "dateStarted": "Sep 28, 2016 5:23:49 PM",
      "dateFinished": "Sep 28, 2016 5:23:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.write\n  .format(\"org.apache.spark.sql.cassandra\")\n  .mode(\"append\")\n  .options(Map( \"table\" -\u003e \"disease_agg_table\", \"keyspace\" -\u003e \"testing\"))\n  .save()",
      "dateUpdated": "Sep 29, 2016 4:39:18 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475083262652_-1049264852",
      "id": "20160928-172102_1505390304",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 28, 2016 5:21:02 PM",
      "dateStarted": "Sep 29, 2016 4:39:18 AM",
      "dateFinished": "Sep 29, 2016 4:41:42 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df2 \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"forum\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"cr\")",
      "dateUpdated": "Sep 28, 2016 6:00:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475083746078_-1297690048",
      "id": "20160928-172906_52190096",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df2: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp]\n"
      },
      "dateCreated": "Sep 28, 2016 5:29:06 PM",
      "dateStarted": "Sep 28, 2016 6:00:45 PM",
      "dateFinished": "Sep 28, 2016 6:00:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.show()",
      "dateUpdated": "Sep 29, 2016 6:22:02 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475085645406_-361132437",
      "id": "20160928-180045_1626396525",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+\n|                  id|             disease|                 url|         description|               title|sentiment|          place_list|                  cr|source|\n+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+\n|             4216033|           [26696.0]|http://atarde.uol...|\u003ctable border\u003d\"0\"...|Pesquisa de brasi...|       -1|[[null,6,Brazil,-...|2016-05-11 16:00:...|  news|\n|             3925463|[25710.0, 26696.0...|http://boletinald...|La responsable de...|Identifican cómo ...|       -1|[[null,496,Estado...|2016-01-11 01:06:...|  news|\n|             4171344|[25710.0, 26696.0...|http://boletinald...|¿Cómo es posible ...|Aclarando la intr...|       -1|[[null,5626,Asia,...|2016-04-20 02:05:...|  news|\n|             4204525|  [26696.0, 25520.0]|http://www.cdc.go...|      No description|Update: Ongoing Z...|       -1|[[null,192,Puerto...|2016-05-05 15:07:...|  news|\n|             4209460|           [26696.0]|http://antiguaobs...|Health minister s...|Health minister s...|        1|[[null,30,Antigua...|2016-05-05 05:00:...|  news|\n|http://www.laraza...|           [26696.0]|http://www.laraza...|Reconozca los sín...|Si viajas a Latin...|       -1|                  []|2016-03-01 14:20:...|  news|\n|http://www.laraza...|           [26696.0]|http://www.laraza...|Reconozca los sín...|Si viajas a Latin...|       -1|                  []|2016-03-01 14:20:...|  news|\n|             4151116|[25710.0, 26696.0...|http://boletinald...|Con 71 muertes en...|La gripe A se ant...|       -1|[[null,6,Brazil,-...|2016-04-11 02:05:...|  news|\n|             4171257|           [26696.0]|http://www.vangua...|\u003ctable border\u003d\"0\"...|Dengue, chikungu\u0026...|       -1|[[null,3054,Bucar...|2016-04-20 01:30:...|  news|\n|             4172233|           [26696.0]|http://www.vangua...|\u003ctable border\u003d\"0\"...|Dengue, chikungu\u0026...|       -1|[[null,3054,Bucar...|2016-04-20 01:30:...|  news|\n|             4174066|           [26696.0]|http://www.latime...|Caring for babies...|Caring for babies...|        0|[[null,6,Brazil,-...|2016-04-20 21:13:...|  news|\n|             4149459|           [26696.0]|http://www.lacron...|\u003ctable border\u003d\"0\"...|Virus de Zika no ...|        0|[[null,486,Estado...|2016-04-09 15:02:...|  news|\n|             4151876|[20035.0, 26696.0...|http://www.indepe...|Zika virus may be...|Zika virus may be...|       -1|[[null,6,Brazil,-...|2016-04-11 09:31:...|  news|\n|             4152669|  [20035.0, 26696.0]|http://www.eluniv...|\u003ctable border\u003d\"0\"...|Ligan nuevo trast...|       -1|[[null,4,Argentin...|2016-04-11 14:56:...|  news|\n|             4153932|  [20035.0, 26696.0]|http://promedmail...|Cientistas brasil...|PRO/PORT\u003e Zika ví...|       -1|[[null,6,Brazil,-...|2016-04-12 21:50:...|  news|\n|             4386881|[20594.0, 26696.0...|http://www.eweek....|IBM\u0026#39;s Lab-on-...|IBM\u0027s Lab-on-a-Ch...|        1|[[null,106,United...|2016-08-02 00:33:...|  news|\n|             3943031|           [26696.0]|http://promedmail...|                null|PRO/ESP\u003e Zika - E...|       -1|[[null,6,Brazil,-...|2016-01-17 00:57:...|  news|\n|             3943866|  [26696.0, 25312.0]|http://www.stirip...|\u003ctable border\u003d\"0\"...|Postarea anului f...|        0|[[null,null,null,...|2016-01-17 11:58:...|  news|\n|             3944604|           [26696.0]|http://www.andina...|\u003ctable border\u003d\"0\"...|Minsa cuenta con ...|       -1|[[null,10,Peru,-9...|2016-01-17 17:31:...|  news|\n|             4148818|           [26696.0]|http://www.pulseh...|7 things about Zi...|7 things about Zi...|        1|[[null,6,Brazil,-...|2016-04-08 21:04:...|  news|\n+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Sep 28, 2016 6:00:45 PM",
      "dateStarted": "Sep 29, 2016 6:22:02 AM",
      "dateFinished": "Sep 29, 2016 6:22:03 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.groupBy(\"source\").count().show()",
      "dateUpdated": "Sep 29, 2016 6:22:09 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475085675510_-1448955495",
      "id": "20160928-180115_292929763",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+-----+\n| source|count|\n+-------+-----+\n|twitter|  249|\n|  forum|    1|\n| reddit|    1|\n|   news| 4328|\n+-------+-----+\n\n"
      },
      "dateCreated": "Sep 28, 2016 6:01:15 PM",
      "dateStarted": "Sep 29, 2016 6:22:09 AM",
      "dateFinished": "Sep 29, 2016 6:24:11 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.filter(array_contains(df2(\"disease\"), 1012)).count()",
      "dateUpdated": "Sep 29, 2016 6:24:17 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475123228458_2080835877",
      "id": "20160929-042708_995173864",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res42: Long \u003d 0\n"
      },
      "dateCreated": "Sep 29, 2016 4:27:08 AM",
      "dateStarted": "Sep 29, 2016 6:24:17 AM",
      "dateFinished": "Sep 29, 2016 6:26:14 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// var df \u003d sqlContext\n//   .read\n//   .format(\"org.apache.spark.sql.cassandra\")\n//   .options(Map( \"table\" -\u003e \"news\", \"keyspace\" -\u003e \"test_news\"))\n//   .load()\n//   .select(\"id\", \"organization\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"impact\", \"cr\")\n//   .filter(size($\"organization\")\u003e0 \u0026\u0026 $\"cr\" \u003e\u003d \"2016-07-01\" \u0026\u0026 $\"cr\" \u003c\u003d \"2016-09-15\").filter(\"impact is not null\")\n//   .withColumn(\"source\", lit(\"news\"))\n  \n// var df2 \u003d df\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"twitter\", \"keyspace\" -\u003e \"processed_social\"))\n  .load()\n  .select(\"id\", \"organization\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\",\"klout\", \"cr\")\n  .withColumnRenamed(\"klout\", \"impact\")\n  .filter(size($\"organization\")\u003e0 \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\" \u0026\u0026 $\"cr\" \u003c\u003d \"2016-09-15\")\n  .withColumn(\"source\", lit(\"twitter\"))\n\ndf2 \u003d df2.unionAll(df)\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"reddit\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"organization\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"score\", \"cr\")\n  .withColumnRenamed(\"score\", \"impact\")\n  .filter(size($\"organization\")\u003e0 \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\" \u0026\u0026 $\"cr\" \u003c\u003d \"2016-09-15\")\n  .withColumn(\"source\", lit(\"reddit\"))\n\ndf2 \u003d df2.unionAll(df)\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"forum\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"organization\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"score\", \"cr\")\n  .withColumnRenamed(\"score\", \"impact\")\n  .filter(size($\"organization\")\u003e0 \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\" \u0026\u0026 $\"cr\" \u003c\u003d \"2016-09-15\")\n  .withColumn(\"source\", lit(\"forum\"))\n\ndf2 \u003d df2.unionAll(df)",
      "dateUpdated": "Oct 7, 2016 10:10:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "tableHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475123718773_-960146098",
      "id": "20160929-043518_1091776033",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\n"
      },
      "dateCreated": "Sep 29, 2016 4:35:18 AM",
      "dateStarted": "Oct 7, 2016 10:08:31 PM",
      "dateFinished": "Oct 7, 2016 10:09:27 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"twitter\", \"keyspace\" -\u003e \"processed_social\"))\n  .load()\n  .select(\"id\", \"organization\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\",\"klout\", \"cr\")\n  .withColumnRenamed(\"klout\", \"impact\")\n  .filter(size($\"organization\")\u003e0 \u0026\u0026 $\"cr\" \u003e\u003d \"2016-07-01\" \u0026\u0026 $\"cr\" \u003c\u003d \"2016-09-15\")\n  .withColumn(\"source\", lit(\"twitter\"))",
      "dateUpdated": "Oct 8, 2016 3:54:12 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475878234270_-483873127",
      "id": "20161007-221034_1080118395",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, source: string]\n"
      },
      "dateCreated": "Oct 7, 2016 10:10:34 PM",
      "dateStarted": "Oct 8, 2016 3:54:12 AM",
      "dateFinished": "Oct 8, 2016 3:54:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.count()",
      "dateUpdated": "Oct 8, 2016 3:55:05 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475553232418_475311446",
      "id": "20161004-035352_371531924",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 4, 2016 3:53:52 AM",
      "dateStarted": "Oct 8, 2016 3:55:05 AM",
      "dateFinished": "Oct 8, 2016 3:54:44 AM",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write\n  .format(\"org.apache.spark.sql.cassandra\")\n  .mode(\"append\")\n  .options(Map( \"table\" -\u003e \"agg_table\", \"keyspace\" -\u003e \"testing\"))\n  .save()",
      "dateUpdated": "Oct 7, 2016 10:13:24 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475553535717_-1123139632",
      "id": "20161004-035855_450847717",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 1 times, most recent failure: Lost task 3.0 in stage 2.0 (TID 5, localhost): com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.ArrayBackedResultSet$MultiPage.prepareNextRow(ArrayBackedResultSet.java:308)\n\tat com.datastax.driver.core.ArrayBackedResultSet$MultiPage.isExhausted(ArrayBackedResultSet.java:265)\n\tat com.datastax.driver.core.ArrayBackedResultSet$1.hasNext(ArrayBackedResultSet.java:136)\n\tat com.datastax.spark.connector.rdd.reader.PrefetchingResultSetIterator.hasNext(PrefetchingResultSetIterator.scala:21)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat com.datastax.spark.connector.writer.GroupingBatchBuilder.hasNext(GroupingBatchBuilder.scala:101)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat com.datastax.spark.connector.writer.GroupingBatchBuilder.foreach(GroupingBatchBuilder.scala:31)\n\tat com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1.apply(TableWriter.scala:157)\n\tat com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1.apply(TableWriter.scala:134)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:110)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:139)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.writer.TableWriter.write(TableWriter.scala:134)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:37)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:67)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:85)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:41)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:46)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:48)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:50)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:52)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:54)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:56)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:60)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:62)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:64)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:66)\n\tat \u003cinit\u003e(\u003cconsole\u003e:68)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:72)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:810)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:753)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:746)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.ArrayBackedResultSet$MultiPage.prepareNextRow(ArrayBackedResultSet.java:308)\n\tat com.datastax.driver.core.ArrayBackedResultSet$MultiPage.isExhausted(ArrayBackedResultSet.java:265)\n\tat com.datastax.driver.core.ArrayBackedResultSet$1.hasNext(ArrayBackedResultSet.java:136)\n\tat com.datastax.spark.connector.rdd.reader.PrefetchingResultSetIterator.hasNext(PrefetchingResultSetIterator.scala:21)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat com.datastax.spark.connector.writer.GroupingBatchBuilder.hasNext(GroupingBatchBuilder.scala:101)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat com.datastax.spark.connector.writer.GroupingBatchBuilder.foreach(GroupingBatchBuilder.scala:31)\n\tat com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1.apply(TableWriter.scala:157)\n\tat com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1.apply(TableWriter.scala:134)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:110)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:139)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.writer.TableWriter.write(TableWriter.scala:134)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\t... 3 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\n"
      },
      "dateCreated": "Oct 4, 2016 3:58:55 AM",
      "dateStarted": "Oct 7, 2016 10:13:24 PM",
      "dateFinished": "Oct 7, 2016 10:17:31 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.groupBy(\"source\").count().show()",
      "dateUpdated": "Sep 29, 2016 4:55:06 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475124651465_-1970543803",
      "id": "20160929-045051_799255669",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+------+\n| source| count|\n+-------+------+\n|twitter|644853|\n|  forum|     1|\n| reddit|     1|\n|   news|  8873|\n+-------+------+\n\n"
      },
      "dateCreated": "Sep 29, 2016 4:50:51 AM",
      "dateStarted": "Sep 29, 2016 4:55:06 AM",
      "dateFinished": "Sep 29, 2016 4:57:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"news\", \"keyspace\" -\u003e \"test_news\"))\n  .load()\n  .select(\"id\", \"organization\", \"url\", \"description\", \"title\", \"sentiment\", \"place_lat\", \"place_lng\", \"cr\")\n  .filter(size($\"organization\")\u003e0 \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\")\n  .withColumn(\"source\", lit(\"news\"))\n  .withColumn(\"id\", rand())",
      "dateUpdated": "Sep 29, 2016 6:17:51 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475124749639_1034466056",
      "id": "20160929-045229_576285689",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: double, organization: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_lat: float, place_lng: float, cr: timestamp, source: string]\n"
      },
      "dateCreated": "Sep 29, 2016 4:52:29 AM",
      "dateStarted": "Sep 29, 2016 6:17:51 AM",
      "dateFinished": "Sep 29, 2016 6:17:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.show()",
      "dateUpdated": "Sep 29, 2016 6:17:59 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475129764209_1134268142",
      "id": "20160929-061604_1865779402",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------------------+------------+--------------------+--------------------+--------------------+---------+---------+---------+--------------------+------+\n|                 id|organization|                 url|         description|               title|sentiment|place_lat|place_lng|                  cr|source|\n+-------------------+------------+--------------------+--------------------+--------------------+---------+---------+---------+--------------------+------+\n| 0.5977292067910491|     [209.0]|http://finfeed.co...|Tech vision becom...|Tech vision becom...|        1|     null|     null|2016-04-01 01:00:...|  news|\n| 0.5185962438417865|     [210.0]|http://www.wsnews...|What Price Target...|What Price Target...|        1|     null|     null|2016-04-01 08:41:...|  news|\n| 0.8005940537982832|     [209.0]|https://www.sheph...|Jordan to acquire...|Jordan to acquire...|        0|     null|     null|2016-05-11 11:33:...|  news|\n| 0.6914037074881477|     [207.0]|http://www.thecou...|Accenture Plc (NY...|Accenture Plc (NY...|        1|     null|     null|2016-07-12 12:06:...|  news|\n| 0.6158404605072235|     [208.0]|http://www.pymnts...|CPOs\u0026#39; Support...|CPOs\u0027 Support For...|        1|     null|     null|2016-03-24 08:12:...|  news|\n| 0.9543545499647463|     [210.0]|http://www.bizjou...|From Whirlpool to...|From Whirlpool to...|        1|     null|     null|2016-04-20 15:30:...|  news|\n|0.28150328064100294|     [208.0]|http://www.freshb...|Better support fo...|Better support fo...|        1|     null|     null|2016-04-20 19:02:...|  news|\n| 0.3121778322589044|     [216.0]|http://promedmail...|Since 2012 (to da...|PRO/MENA\u003e MERS-Co...|       -1|     null|     null|2016-01-13 10:47:...|  news|\n| 0.2718139539016795|     [207.0]|https://www.finex...|Digital banking t...|Digital banking t...|        1|     null|     null|2016-05-05 06:19:...|  news|\n| 0.9068086209513624|     [210.0]|http://www.dl-onl...|While they might ...|A computer that c...|        1|     null|     null|2016-05-20 15:41:...|  news|\n| 0.7796435342302221|     [207.0]|http://www.invest...|Smart beta ETFs (...|Low Volatility ET...|        1|     null|     null|2016-05-21 03:21:...|  news|\n| 0.4767155758820273|     [208.0]|http://www.iottec...|High prices and s...|High prices and s...|        1|     null|     null|2016-07-21 07:19:...|  news|\n|0.36840862241068106|     [208.0]|https://www.globa...|Deloitte Advisory...|Deloitte Advisory...|        0|     null|     null|2016-07-21 03:55:...|  news|\n| 0.6110793316784224|     [210.0]|http://www.toptec...|Cisco Teams With ...|Cisco Teams With ...|        1|     null|     null|2016-07-01 12:01:...|  news|\n| 0.5482480896030992|     [210.0]|http://www.toptec...|CLOUD COMPUTING C...|CLOUD COMPUTING C...|        1|     null|     null|2016-07-01 12:01:...|  news|\n| 0.8179222969324251|     [210.0]|http://www.enterp...|IBM Joins Blockch...|IBM Joins Blockch...|        0|     null|     null|2016-07-01 13:03:...|  news|\n| 0.9611631505018478|     [216.0]|http://boletinald...|Con 71 muertes en...|La gripe A se ant...|       -1|     null|     null|2016-04-11 02:05:...|  news|\n|0.03645339308155038|     [210.0]|http://www.mysana...|Better Buy: Apple...|Better Buy: Apple...|       -1|     null|     null|2016-04-11 19:29:...|  news|\n|0.03831686944980561|     [210.0]|http://3blmedia.c...|Teams Showcase Br...|Teams Showcase Br...|        1|     null|     null|2016-04-20 13:03:...|  news|\n| 0.9805258496752571|     [207.0]|http://www.virtua...|Accenture Teams w...|Accenture Teams w...|        1|     null|     null|2016-04-26 08:19:...|  news|\n+-------------------+------------+--------------------+--------------------+--------------------+---------+---------+---------+--------------------+------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Sep 29, 2016 6:16:04 AM",
      "dateStarted": "Sep 29, 2016 6:17:59 AM",
      "dateFinished": "Sep 29, 2016 6:18:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.write\n  .format(\"org.apache.spark.sql.cassandra\")\n  .mode(\"append\")\n  .options(Map( \"table\" -\u003e \"agg_table\", \"keyspace\" -\u003e \"testing\"))\n  .save()",
      "dateUpdated": "Sep 29, 2016 6:18:04 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475129784896_-697305188",
      "id": "20160929-061624_21391453",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 29, 2016 6:16:24 AM",
      "dateStarted": "Sep 29, 2016 6:18:04 AM",
      "dateFinished": "Sep 29, 2016 6:20:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.show()",
      "dateUpdated": "Oct 7, 2016 5:48:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475859937013_1019713393",
      "id": "20161007-170537_1536266105",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+---------+------+\n|                  id|             disease|                 url|         description|               title|sentiment|          place_list|                  cr|impact|       ds|source|\n+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+---------+------+\n|             4133654|  [26696.0, 25710.0]|http://www.bbc.co...|\u003ctable border\u003d\"0\"...|El brote de H1N1:...|       -1|[[null,6,Brazil,-...|2016-04-01 14:17:...|     0|HealthMap|  news|\n|             4133778|           [26696.0]|http://www.plengl...|Bolivia Reports I...|Bolivia Reports I...|       -1|[[null,5,Bolivia,...|2016-04-01 16:43:...|     0|HealthMap|  news|\n|             4136989|  [26696.0, 25710.0]|http://www.bbc.co...|\u003ctable border\u003d\"0\"...|El brote de H1N1:...|       -1|[[null,6,Brazil,-...|2016-04-01 14:17:...|     0|HealthMap|  news|\n|             4216033|           [26696.0]|http://atarde.uol...|\u003ctable border\u003d\"0\"...|Pesquisa de brasi...|       -1|[[null,6,Brazil,-...|2016-05-11 16:00:...|     0|HealthMap|  news|\n|http://fortune.co...|           [26696.0]|http://fortune.co...|The new warnings ...|Zika Spreads to M...|       -1|[[2971316.0,null,...|2016-08-20 14:38:...|     0|   scrapy|  news|\n|http://fortune.co...|           [26696.0]|http://fortune.co...|The new warnings ...|Zika Spreads to M...|       -1|[[2971316.0,null,...|2016-08-20 14:38:...|     0|   scrapy|  news|\n|             3925463|[25710.0, 26696.0...|http://boletinald...|La responsable de...|Identifican cómo ...|       -1|[[3177337.0,null,...|2016-01-11 01:06:...|  null|HealthMap|  news|\n|             4115666|           [26696.0]|http://www.eurasi...|Zika Virus And Mi...|Zika Virus And Mi...|       -1|[[null,6,Brazil,-...|2016-03-24 02:10:...|     0|HealthMap|  news|\n|             4115747|           [26696.0]|http://www.lanaci...|\u003ctable border\u003d\"0\"...|Dengue: estiman q...|       -1|[[null,4,Argentin...|2016-03-24 02:03:...|     0|HealthMap|  news|\n|             4171344|[25710.0, 26696.0...|http://boletinald...|¿Cómo es posible ...|Aclarando la intr...|       -1|[[3686120.0,null,...|2016-04-20 02:05:...|  null|HealthMap|  news|\n|http://www.review...|           [26696.0]|http://www.review...|Thirty have been ...|Cuba fighting Zik...|       -1|[[5509151.0,null,...|2016-09-02 16:55:...|     5|   scrapy|  news|\n|http://www.review...|           [26696.0]|http://www.review...|Thirty have been ...|Cuba fighting Zik...|       -1|[[5509151.0,null,...|2016-09-02 16:55:...|     5|   scrapy|  news|\n|             4316891|           [26696.0]|http://news-russi...|\u003ctable border\u003d\"0\"...|Новая вакцина изл...|        1|[[2755476.0,null,...|2016-06-29 11:54:...|  null|HealthMap|  news|\n|             4204525|  [26696.0, 25520.0]|http://www.cdc.go...|      No description|Update: Ongoing Z...|       -1|[[null,192,Puerto...|2016-05-05 15:07:...|     0|HealthMap|  news|\n|             4205395|           [26696.0]|http://www.abc.co...|\u003ctable border\u003d\"0\"...|Suman 16 muertes ...|       -1|[[null,9,Paraguay...|2016-05-05 23:19:...|     0|HealthMap|  news|\n|             4209460|           [26696.0]|http://antiguaobs...|Health minister s...|Health minister s...|        1|[[null,30,Antigua...|2016-05-05 05:00:...|     0|HealthMap|  news|\n|             4361557|           [26696.0]|http://www.infoba...|\u003ctable border\u003d\"0\"...|Identificaron un ...|        0|[[3390760.0,null,...|2016-07-21 20:09:...|  null|HealthMap|  news|\n|             4361687|           [26696.0]|http://newsera.ru...|\u003ctable border\u003d\"0\"...|В Мексике резко в...|        1|                  []|2016-07-21 20:25:...|  null|HealthMap|  news|\n|http://www.laraza...|           [26696.0]|http://www.laraza...|Reconozca los sín...|Si viajas a Latin...|       -1|                  []|2016-03-01 14:20:...|     0|   scrapy|  news|\n|http://www.laraza...|           [26696.0]|http://www.laraza...|Reconozca los sín...|Si viajas a Latin...|       -1|                  []|2016-03-01 14:20:...|  null|   scrapy|  news|\n+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+------+---------+------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Oct 7, 2016 5:05:37 PM",
      "dateStarted": "Oct 7, 2016 5:48:11 PM",
      "dateFinished": "Oct 7, 2016 5:48:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// NEW KEYSPACE\n\n// df.createCassandraTable(\"testing2\", \"agg_table\", \n// partitionKeyColumns \u003d Some(Seq(\"type\", \"ds\")), \n// clusteringKeyColumns \u003d Some(Seq(\"cr\", \"id\")))\n\n// df.write\n//   .format(\"org.apache.spark.sql.cassandra\")\n//   .mode(\"append\")\n//   .options(Map( \"table\" -\u003e \"agg_table2\", \"keyspace\" -\u003e \"testing\"))\n//   .save()",
      "dateUpdated": "Oct 11, 2016 7:18:58 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475859622194_1520796023",
      "id": "20161007-170022_1658313036",
      "dateCreated": "Oct 7, 2016 5:00:22 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.cassandraTable(\"testing\", \"agg_table\").count()",
      "dateUpdated": "Sep 29, 2016 6:20:57 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475129823573_235002693",
      "id": "20160929-061703_39149205",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res35: Long \u003d 357894\n"
      },
      "dateCreated": "Sep 29, 2016 6:17:03 AM",
      "dateStarted": "Sep 29, 2016 6:20:57 AM",
      "dateFinished": "Sep 29, 2016 6:21:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"news\", \"keyspace\" -\u003e \"test_news\"))\n  .load()\n  .select(\"id\", \"product\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"impact\", \"cr\", \"symptom\")\n  .withColumn(\"source\", lit(\"news\"))\n  \nvar df2 \u003d df.filter(array_contains(df(\"product\"), 1012))\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"twitter\", \"keyspace\" -\u003e \"processed_social\"))\n  .load()\n  .select(\"id\", \"product\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\",\"klout\", \"cr\", \"symptom\")\n  .withColumnRenamed(\"klout\", \"impact\")\n  .withColumn(\"source\", lit(\"twitter\"))\n  \ndf \u003d df.filter(array_contains(df(\"product\"), 1012))\n\ndf2 \u003d df2.unionAll(df)\n\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"reddit\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"product\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\",\"ups\", \"cr\", \"symptom\")\n   .withColumnRenamed(\"ups\", \"impact\")\n  .withColumn(\"source\", lit(\"reddit\"))\n  \ndf \u003d df.filter(array_contains(df(\"product\"), 1012))\n\ndf2 \u003d df2.unionAll(df)\nvar df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"reddit\", \"keyspace\" -\u003e \"processed_forum\"))\n  .load()\n  .select(\"id\", \"product\", \"url\", \"description\", \"title\", \"sentiment\", \"place_list\", \"ups\", \"cr\", \"symptom\")\n  .withColumnRenamed(\"ups\", \"impact\")\n  .withColumn(\"source\", lit(\"forum\"))\n  \ndf \u003ddf.filter(array_contains(df(\"product\"), 1012))\n\ndf2 \u003d df2.unionAll(df)",
      "dateUpdated": "Oct 12, 2016 2:18:21 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "tableHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475130057761_1259541397",
      "id": "20160929-062057_1049292680",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, product: array\u003cdouble\u003e, url: string, description: string, title: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, impact: int, cr: timestamp, symptom: array\u003cdouble\u003e, source: string]\n"
      },
      "dateCreated": "Sep 29, 2016 6:20:57 AM",
      "dateStarted": "Oct 12, 2016 2:18:21 AM",
      "dateFinished": "Oct 12, 2016 2:18:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.show()",
      "dateUpdated": "Oct 12, 2016 3:28:59 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475130511146_2080041114",
      "id": "20160929-062831_1012694340",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+------+--------------------+--------------------+-------+\n|                  id|             product|                 url|         description|               title|sentiment|          place_list|impact|                  cr|             symptom| source|\n+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+------+--------------------+--------------------+-------+\n|http://www.mlive....|[3372.0, 1012.0, ...|http://www.mlive....|Rick Snyder creat...|Michigan fights d...|       -1|                  []|     2|2016-07-05 06:35:...|[15958.0, 11330.0...|   news|\n|http://www.lehigh...|            [1012.0]|http://www.lehigh...|John Cramsey, 51,...|Holland Tunnel gu...|       -1|                  []|     4|2016-09-08 18:01:...|[10945.0, 8750.0,...|   news|\n|http://www.nj.com...|            [1012.0]|http://www.nj.com...|John Cramsey, 51,...|Holland Tunnel gu...|       -1|[[5101760.0,null,...|     1|2016-09-08 19:20:...|[10945.0, 8750.0,...|   news|\n|http://www.azcent...|            [1012.0]|http://www.azcent...|Certainly nothing...|Guess who will li...|       -1|                  []|     5|                null|   [10886.0, 7937.0]|   news|\n|http://www.mornin...|            [1012.0]|http://www.mornin...|EAST LIVERPOOL - ...|Overdose cases co...|       -1|                  []|     0|                null|[8844.0, 10945.0,...|   news|\n|http://www.azcent...|            [1012.0]|http://www.azcent...|Certainly nothing...|Guess who will li...|       -1|[[5551752.0,null,...|  null|                null|   [10886.0, 7937.0]|   news|\n|             4177082|            [1012.0]|http://www.eldiar...|\u003ctable border\u003d\"0\"...|Adicci\u0026oacute;n a...|       -1|[[null,234,New Yo...|     0|2016-04-22 10:53:...|  [12443.0, 10389.0]|   news|\n|             4307189|            [1012.0]|http://oglobo.glo...|\u003ctable border\u003d\"0\"...|ONU diz que Estad...|        0|[[null,106,United...|     0|2016-06-24 08:38:...|                  []|   news|\n|  683042294623174656|            [1012.0]|http://twitter.co...|@LynnRWebsterMD T...|@LynnRWebsterMD T...|        0|                  []|  null|2016-01-01 21:49:...|           [15797.0]|twitter|\n|  715227883514499075|            [1012.0]|                null|they have these s...|they have these s...|        1|                  []|  null|2016-03-30 17:23:...|                  []|twitter|\n|  576585531850502145|            [1012.0]|                null|@B_Lentz10 it\u0027s a...|@B_Lentz10 it\u0027s a...|        0|                  []|  null|2015-03-14 03:27:...|                  []|twitter|\n|  776352475704008704|    [1012.0, 1249.0]|                null|RT @levibeers: In...|RT @levibeers: In...|        0|                  []|  null|2016-09-15 09:30:...|                  []|twitter|\n|  505597873434943488|     [1012.0, 669.0]|                null|#MedsChat opana 4...|#MedsChat opana 4...|        0|                  []|  null|2014-08-30 06:08:...|                  []|twitter|\n|  554935017990664193|            [1012.0]|                null|god, what I’d giv...|god, what I’d giv...|        1|                  []|  null|2015-01-13 09:36:...|                  []|twitter|\n|  658983835036733440|            [1012.0]|                null|RT @Sil_Lai: The ...|RT @Sil_Lai: The ...|        0|                  []|  null|2015-10-27 12:29:...|                  []|twitter|\n|  683356395316621312|            [1012.0]|http://twitter.co...|Sparla McCann los...|Sparla McCann los...|       -1|                  []|  null|2016-01-02 18:37:...|            [7615.0]|twitter|\n|  585294107771346946|[3012.0, 3562.0, ...|                null|finally home from...|finally home from...|        0|                  []|  null|2015-04-07 04:12:...|                  []|twitter|\n|  421710670632325120|            [1012.0]|                null|Good morning  y\u0027a...|Good morning  y\u0027a...|        1|                  []|  null|2014-01-10 18:30:...|                  []|twitter|\n|  712787782935179264|            [1012.0]|                null|There\u0027s a batch o...|There\u0027s a batch o...|       -1|                  []|  null|2016-03-23 23:47:...|                  []|twitter|\n|  464161012833214464|            [1012.0]|                null|Liquid morphine \u0026...|Liquid morphine \u0026...|        0|                  []|  null|2014-05-07 21:53:...|                  []|twitter|\n+--------------------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+------+--------------------+--------------------+-------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Sep 29, 2016 6:28:31 AM",
      "dateStarted": "Oct 12, 2016 3:28:59 AM",
      "dateFinished": "Oct 12, 2016 7:38:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.write\n  .format(\"org.apache.spark.sql.cassandra\")\n  .mode(\"append\")\n  .options(Map( \"table\" -\u003e \"product_agg_table\", \"keyspace\" -\u003e \"testing\"))\n  .save()",
      "dateUpdated": "Oct 12, 2016 12:37:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476275693980_-56090594",
      "id": "20161012-123453_30213381",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 12, 2016 12:34:53 PM",
      "dateStarted": "Oct 12, 2016 12:37:42 PM",
      "dateFinished": "Oct 12, 2016 4:44:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"news\", \"keyspace\" -\u003e \"test_news\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"dom\", \"sentiment\", \"place_list\", \"cr\")\n  .withColumn(\"dom2\", regexp_replace(lower($\"dom\"), \"\"\"https://|www.|http://|/\"\"\", \"\"))\n  .withColumn(\"source\", lit(\"news\"))\n  \nvar df2 \u003d df.filter(array_contains(df(\"disease\"), 26696) \u0026\u0026 $\"cr\" \u003e\u003d \"2016-01-01\").join(alexa, df(\"dom2\")\u003d\u003d\u003dalexa(\"url2\"), \"left\").na.fill(0,Seq(\"impact\"))",
      "dateUpdated": "Sep 29, 2016 8:07:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475130529060_1057009113",
      "id": "20160929-062849_890682541",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, dom: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, dom2: string, source: string]\ndf2: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, dom: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, dom2: string, source: string, url2: string, impact: int]\n"
      },
      "dateCreated": "Sep 29, 2016 6:28:49 AM",
      "dateStarted": "Sep 29, 2016 8:07:33 PM",
      "dateFinished": "Sep 29, 2016 8:07:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df2.select( \"dom2\", \"dom\", \"url2\", \"impact\").filter($\"impact\"\u003e0).show()",
      "dateUpdated": "Sep 29, 2016 8:08:01 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475179348658_-1029865779",
      "id": "20160929-200228_57613108",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+------------------+--------------------+------------------+------+\n|              dom2|                 dom|              url2|impact|\n+------------------+--------------------+------------------+------+\n|       latimes.com|         latimes.com|       latimes.com|     5|\n|    denverpost.com|http://www.denver...|    denverpost.com|     5|\n|      dnaindia.com|        dnaindia.com|      dnaindia.com|     5|\n|    nhregister.com|      nhregister.com|    nhregister.com|     3|\n|    nhregister.com|      nhregister.com|    nhregister.com|     3|\n|    nhregister.com|      nhregister.com|    nhregister.com|     3|\n|   sfchronicle.com|     sfchronicle.com|   sfchronicle.com|     5|\n|        nypost.com|  http://nypost.com/|        nypost.com|     5|\n|      dnaindia.com|        dnaindia.com|      dnaindia.com|     5|\n|       thehill.com|         thehill.com|       thehill.com|     5|\n|      examiner.com|        examiner.com|      examiner.com|     5|\n|  villagevoice.com|http://www.villag...|  villagevoice.com|     4|\n|dallasobserver.com|http://www.dallas...|dallasobserver.com|     4|\n|dallasobserver.com|http://www.dallas...|dallasobserver.com|     4|\n|    dallasnews.com|http://www.dallas...|    dallasnews.com|     5|\n|      dnaindia.com|        dnaindia.com|      dnaindia.com|     5|\n| indianexpress.com|   indianexpress.com| indianexpress.com|     5|\n|      metronews.ca|        metronews.ca|      metronews.ca|     4|\n|        rep-am.com|http://www.rep-am...|        rep-am.com|     2|\n|      triblive.com|        triblive.com|      triblive.com|     5|\n+------------------+--------------------+------------------+------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Sep 29, 2016 8:02:28 PM",
      "dateStarted": "Sep 29, 2016 8:08:01 PM",
      "dateFinished": "Sep 29, 2016 8:08:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "alexa.show()",
      "dateUpdated": "Sep 29, 2016 8:06:59 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475179363327_-1748961473",
      "id": "20160929-200243_314867536",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+------+\n|                url2|impact|\n+--------------------+------+\n|         nytimes.com|     5|\n|timesofindia.indi...|     5|\n|        usatoday.com|     5|\n|             wsj.com|     5|\n|   indianexpress.com|     5|\n|economictimes.ind...|     5|\n|         latimes.com|     5|\n|          nypost.com|     5|\n|           chron.com|     5|\n|        thehindu.com|     5|\n|          sfgate.com|     5|\n|navbharattimes.in...|     5|\n|          eenadu.net|     5|\n|  chicagotribune.com|     5|\n|hollywoodreporter...|     5|\n|  hindustantimes.com|     5|\n|          smh.com.au|     5|\n|  manoramaonline.com|     5|\n|         thehill.com|     5|\n|    andhrajyothy.com|     5|\n+--------------------+------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Sep 29, 2016 8:02:43 PM",
      "dateStarted": "Sep 29, 2016 8:06:59 PM",
      "dateFinished": "Sep 29, 2016 8:07:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"news2\", \"keyspace\" -\u003e \"test_news\"))\n  .load()\n  .select(\"id\", \"disease\", \"url\", \"description\", \"title\", \"dom\", \"sentiment\", \"place_list\", \"cr\", \"impact\")",
      "dateUpdated": "Sep 29, 2016 8:40:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475179619658_-2131418849",
      "id": "20160929-200659_1178704637",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, disease: array\u003cdouble\u003e, url: string, description: string, title: string, dom: string, sentiment: int, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, cr: timestamp, impact: int]\n"
      },
      "dateCreated": "Sep 29, 2016 8:06:59 PM",
      "dateStarted": "Sep 29, 2016 8:40:48 PM",
      "dateFinished": "Sep 29, 2016 8:41:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.select(\"impact\").show()",
      "dateUpdated": "Sep 29, 2016 8:41:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475180883596_-513173198",
      "id": "20160929-202803_823094075",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+------+\n|impact|\n+------+\n+------+\n\n"
      },
      "dateCreated": "Sep 29, 2016 8:28:03 PM",
      "dateStarted": "Sep 29, 2016 8:41:06 PM",
      "dateFinished": "Sep 29, 2016 8:41:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var df \u003d sqlContext\n  .read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map( \"table\" -\u003e \"agg_table\", \"keyspace\" -\u003e \"testing\"))\n  .load()\n  .cache()",
      "dateUpdated": "Oct 11, 2016 2:19:50 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475180897878_-188598368",
      "id": "20160929-202817_2058785156",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [id: string, cr: timestamp, description: string, impact: int, organization: array\u003cfloat\u003e, place_lat: float, place_list: array\u003cstruct\u003cgeo_geonameid:double,place_id:int,place_name:string,place_lat:float,place_lng:float\u003e\u003e, place_list_v: string, place_lng: float, sentiment: int, source: string, title: string, url: string]\n"
      },
      "dateCreated": "Sep 29, 2016 8:28:17 PM",
      "dateStarted": "Oct 11, 2016 3:21:27 PM",
      "dateFinished": "Oct 11, 2016 7:02:43 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": ".withColumn(\"t\", regexp_replace($\"t\", \"\"\"\u003c(?!\\/?a(?\u003d\u003e|\\s.*\u003e))\\/?.*?\u003e\"\"\", \"\"))",
      "dateUpdated": "Oct 11, 2016 2:20:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476195593909_-881509756",
      "id": "20161011-141953_919726175",
      "dateCreated": "Oct 11, 2016 2:19:53 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.count()",
      "dateUpdated": "Oct 4, 2016 5:39:41 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475602578050_-663309667",
      "id": "20161004-173618_539472836",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res6: Long \u003d 1662149\n"
      },
      "dateCreated": "Oct 4, 2016 5:36:18 PM",
      "dateStarted": "Oct 4, 2016 5:39:41 PM",
      "dateFinished": "Oct 4, 2016 5:39:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.groupBy(to_date($\"cr\")).count().show(1000)",
      "dateUpdated": "Oct 4, 2016 5:40:16 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475602597781_110081595",
      "id": "20161004-173637_558147716",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----------+-----+\n|todate(cr)|count|\n+----------+-----+\n|2016-01-31| 1172|\n|2016-08-18|   27|\n|2016-02-01| 2341|\n|2016-08-19| 1744|\n|2016-08-20| 6041|\n|2016-02-02| 2282|\n|2016-02-03| 2284|\n|2016-08-21| 6475|\n|2016-08-22|11433|\n|2016-02-04| 2174|\n|2016-08-23|13168|\n|2016-02-05| 2322|\n|2016-08-24|12549|\n|2016-02-06| 1265|\n|2016-08-25|14082|\n|2016-02-07|  859|\n|2016-08-26|12569|\n|2016-02-08| 2183|\n|2016-08-27| 7266|\n|2016-02-09| 2684|\n|2016-08-28| 6702|\n|2016-02-10| 2500|\n|2016-08-29|13627|\n|2016-02-11| 1988|\n|2016-08-30|15390|\n|2016-02-12| 2052|\n|2016-08-31|15723|\n|2016-02-13| 1049|\n|2016-09-01|20886|\n|2016-02-14|  839|\n|2016-09-02|15568|\n|2016-02-15| 1725|\n|2016-09-03| 8274|\n|2016-02-16| 3182|\n|2016-02-17| 2818|\n|2016-09-04| 8421|\n|2016-02-18| 2381|\n|2016-09-05|13371|\n|2016-09-06|20695|\n|2016-02-19| 2654|\n|2016-09-07|19946|\n|2016-02-20| 1698|\n|2016-09-08|20961|\n|2016-02-21| 1038|\n|2016-02-22| 2097|\n|2016-09-09|13292|\n|2016-09-10|10679|\n|2016-02-23| 2695|\n|2016-09-11| 9125|\n|2016-02-24| 2619|\n|2016-09-12|17155|\n|2016-02-25| 2552|\n|2016-02-26| 2285|\n|2016-09-13|14730|\n|2016-09-14|16122|\n|2016-02-27| 1560|\n|2016-09-15|15426|\n|2016-02-28| 1056|\n|2016-09-16|14715|\n|2016-02-29| 2375|\n|2016-09-17| 8994|\n|2016-03-01| 3201|\n|2016-03-02| 2975|\n|2016-09-18| 9043|\n|2016-09-19|17670|\n|2016-03-03| 3903|\n|2016-09-20|20697|\n|2016-03-04| 3659|\n|2016-09-21|17184|\n|2016-03-05| 1316|\n|2016-09-22|15946|\n|2016-03-06| 1283|\n|2016-09-23|16859|\n|2016-03-07| 2952|\n|2016-03-08| 4782|\n|2016-09-24| 9833|\n|2016-03-09| 2974|\n|2016-09-25| 8237|\n|2016-09-26|16581|\n|2016-03-10| 3097|\n|2016-03-11| 2018|\n|2016-09-27|19748|\n|2016-09-28|28778|\n|2016-03-12| 1292|\n|2016-09-29|30364|\n|2016-03-13| 1220|\n|2016-09-30|18336|\n|2016-03-14| 2135|\n|2016-10-01| 5570|\n|2016-03-15| 2516|\n|2016-03-16| 2944|\n|2016-10-02|   70|\n|2016-03-17| 2275|\n|2016-10-03|  150|\n|2016-03-18| 2235|\n|2016-03-19| 1487|\n|2016-03-20|  743|\n|2016-03-21| 1705|\n|2016-03-22| 1878|\n|2016-03-23| 2124|\n|2016-03-24| 2975|\n|2016-03-25| 1603|\n|2016-03-26| 1297|\n|2016-03-27|  819|\n|2016-03-28| 1899|\n|2016-03-29| 2072|\n|2016-03-30| 2393|\n|2016-03-31| 2199|\n|2016-04-01|16076|\n|2016-04-02| 9653|\n|2016-04-03| 8255|\n|2016-04-04|17599|\n|2016-04-05|20207|\n|2016-04-06|19321|\n|2016-04-07|24896|\n|2016-04-08|23277|\n|2016-04-09|10946|\n|2016-04-10| 8999|\n|2016-04-11|16607|\n|2016-04-12|24626|\n|2016-04-13|21399|\n|2016-04-14|20604|\n|2016-04-15|17055|\n|2016-04-16| 9850|\n|2016-04-17| 9242|\n|2016-04-18|22446|\n|2016-04-19|28713|\n|2016-04-20|17694|\n|2016-04-21|16741|\n|2016-04-22|15737|\n|2016-04-23|10696|\n|2016-04-24| 8940|\n|2016-04-25|17868|\n|2016-04-26|20387|\n|2016-04-27|21459|\n|2016-04-28|20457|\n|2016-04-29|21033|\n|2016-04-30|11745|\n|2016-05-01| 9867|\n|2016-05-02|20629|\n|2016-05-03|28321|\n|2016-05-04|29214|\n|2016-05-05|22641|\n|2016-05-06|18884|\n|2016-05-07| 9452|\n|2016-05-08|  791|\n|2016-05-09| 2477|\n|2016-05-10| 3001|\n|2016-05-11| 2868|\n|2016-05-12| 2357|\n|2016-05-13| 1938|\n|2016-05-14| 1053|\n|2016-05-15|  977|\n|2016-05-16| 2493|\n|2016-05-17| 3264|\n|2016-05-18| 2877|\n|2016-05-19| 2840|\n|2016-05-20| 2764|\n|2016-05-21| 1403|\n|2016-05-22|  942|\n|2016-05-23| 2251|\n|2016-05-24| 2359|\n|2016-05-25| 2847|\n|2016-05-26| 2381|\n|2016-05-27| 1991|\n|2016-05-28| 1248|\n|2016-05-29|  837|\n|2016-05-30| 1268|\n|2016-05-31| 2346|\n|2016-06-01| 2410|\n|2016-06-02| 2791|\n|2016-06-03| 2800|\n|2016-06-04| 6752|\n|2016-06-05| 6359|\n|2016-06-06|12484|\n|2016-06-07| 6701|\n|2016-06-08| 3101|\n|2016-06-09| 2542|\n|2016-06-10| 2904|\n|2016-06-11| 1384|\n|2016-06-12|  846|\n|2016-06-13| 1971|\n|2016-06-14| 2625|\n|2016-06-15| 3055|\n|2016-06-16| 2262|\n|2016-06-17| 8743|\n|2016-06-18| 8840|\n|2016-06-19| 7291|\n|2016-06-20|16278|\n|2016-06-21| 6666|\n|2016-06-22| 2422|\n|2016-06-23| 3130|\n|2016-06-24| 2383|\n|2016-06-25| 1272|\n|2016-06-26| 1003|\n|2016-06-27| 2359|\n|2016-06-28| 2732|\n|2016-06-29| 2708|\n|2016-06-30| 3218|\n|2016-07-01|   86|\n|2016-07-02|   20|\n|2016-07-03|   17|\n|2016-07-04|   58|\n|2016-07-05|  101|\n|2016-07-06|   89|\n|2016-07-07|   96|\n|2016-07-08|   88|\n|2016-07-09|   15|\n|2016-07-10|   33|\n|2016-07-11|  161|\n|2016-07-12|  179|\n|2016-07-13|  121|\n|2016-07-14|  130|\n|2016-07-15|  122|\n|2016-07-16|   27|\n|2016-07-17|   50|\n|2016-07-18|  163|\n|2016-07-19|  111|\n|2016-01-01|  726|\n|2016-01-02|  862|\n|2016-07-20|   94|\n|2016-01-03|  964|\n|2016-07-21|   98|\n|2016-01-04| 1826|\n|2016-07-22|   90|\n|2016-01-05| 2542|\n|2016-07-23|   29|\n|2016-01-06| 2549|\n|2016-07-24|   28|\n|2016-01-07| 2287|\n|2016-07-25|  136|\n|2016-07-26|  156|\n|2016-01-08| 2105|\n|2016-01-09| 1047|\n|2016-07-27|  141|\n|2016-01-10| 1561|\n|2016-07-28|  147|\n|2016-01-11| 2279|\n|2016-07-29|  127|\n|2016-01-12| 2372|\n|2016-07-30|   29|\n|2016-01-13| 2923|\n|2016-07-31|   46|\n|2016-01-14| 3765|\n|2016-08-01|  157|\n|2016-01-15| 2839|\n|2016-08-02|  194|\n|2016-01-16| 1508|\n|2016-08-03|  137|\n|2016-01-17| 1699|\n|2016-08-04|  125|\n|2016-01-18| 3172|\n|2016-08-05|   18|\n|2016-01-19| 3694|\n|2016-08-06|   25|\n|2016-01-20| 3994|\n|2016-08-07|    3|\n|2016-01-21| 7994|\n|2016-08-08|   12|\n|2016-01-22| 4244|\n|2016-08-09|   11|\n|2016-01-23| 1701|\n|2016-08-10|   18|\n|2016-01-24| 1292|\n|2016-08-11|   27|\n|2016-01-25| 2273|\n|2016-08-12|   26|\n|2016-01-26| 2698|\n|2016-08-13|    3|\n|2016-01-27| 2607|\n|2016-08-14|    3|\n|2016-01-28| 5253|\n|2016-08-15|    4|\n|2016-01-29| 2356|\n|2016-08-16|   23|\n|2016-01-30| 1369|\n|2016-08-17|   11|\n+----------+-----+\n\n"
      },
      "dateCreated": "Oct 4, 2016 5:36:37 PM",
      "dateStarted": "Oct 4, 2016 5:40:16 PM",
      "dateFinished": "Oct 4, 2016 5:41:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475602816029_-2118877697",
      "id": "20161004-174016_1088744443",
      "dateCreated": "Oct 4, 2016 5:40:16 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "data for essam",
  "id": "2BY4E3JDA",
  "angularObjects": {
    "2BTJ3P41C:shared_process": [],
    "2BUSJ5B5T:shared_process": [],
    "2BUCYP1D2:shared_process": [],
    "2BRYFEHJ7:shared_process": [],
    "2BT211CDH:shared_process": [],
    "2BTX1MQS6:shared_process": [],
    "2BU87RU3U:shared_process": [],
    "2BTB82RPQ:shared_process": [],
    "2BRP3ZUWJ:shared_process": [],
    "2BT3JK3T4:shared_process": [],
    "2BUTB2HA6:shared_process": [],
    "2BTXGDVEJ:shared_process": [],
    "2BRZ896X6:shared_process": [],
    "2BSSEDUXN:shared_process": [],
    "2BSMJA8VG:shared_process": [],
    "2BSJYK2YE:shared_process": [],
    "2BUZX9EWW:shared_process": []
  },
  "config": {},
  "info": {}
}