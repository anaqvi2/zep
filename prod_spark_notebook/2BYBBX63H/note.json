{
  "paragraphs": [
    {
      "text": "%pyspark\n\n# coding: utf-8\nimport ConfigParser\nimport argparse\nimport os\nimport nltk\nimport re\nimport datetime\nfrom collections import OrderedDict\n\nimport shutil\nfrom nltk.downloader import download\nfrom pyspark.sql.functions import lit, udf, col, when, size\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import StringType, StructType, FloatType, DoubleType, IntegerType, StructField, ArrayType\nfrom pyspark.storagelevel import StorageLevel\n\n\"\"\"\nGeocode records in processed_news.news table.\n\nUsage:\nspark-submit  --packages com.datastax.spark:spark-cassandra-connector_2.10:1.6.0 --conf spark.cassandra.connection.host\u003d\u003cdatabase_url\u003e  geocode_news.py\n\"\"\"\n\n\"\"\"\nGlobal variables\n\"\"\"\n# Processor info\nPROCESSOR_NAME \u003d \u0027place_list\u0027\nPROCESSOR_VERSION \u003d \u0027v0.0.1\u0027\n\nSCRIPT_DIR \u003d os.path.dirname(os.path.realpath(__file__))\nCODE_REGEX \u003d r\u0027(?:\\s|^|!|,|\\()([A-Z]{2,3})(?:\\s|$|!|,|\\))\u0027\nDELETE_REGEX \u003d r\u0027\\\\n|\\\\t|\\\u003e|\\\u003c|\\(|\\)|\\\u003e|\\\u003c\u0027\n\n# Words that should not be considered placenames\nIGNORE_WORDS \u003d [\u0027a\u0027,\u0027ai\u0027,\u0027I\u0027,\u0027of\u0027,\u0027the\u0027\u0027many\u0027,\u0027may\u0027,\u0027march\u0027,\u0027center\u0027,\u0027as\u0027,\u0027see\u0027,\u0027valley\u0027,\u0027university\u0027,\u0027about\u0027,\u0027new\u0027,\u0027sars\u0027,\u0027aids\u0027,\u0027hpv\u0027,\u0027adhd\u0027,\u0027newcastle\u0027,\u0027elisa\u0027,\u0027to\u0027,\u0027influenza\u0027,\u0027who\u0027,\u0027pro\u0027,\u0027os\u0027,\u0027and\u0027,\u0027pdt\u0027,\u0027in\u0027,\u0027flu\u0027,\u0027ha\u0027,\u0027ron\u0027,\u0027control\u0027,\u0027mod\u0027,\u0027northern\u0027,\u0027southern\u0027,\u0027eastern\u0027,\u0027western\u0027,\u0027all\u0027,\u0027am\u0027,\u0027pm\u0027,\u0027eua\u0027,\u0027avian\u0027,\u0027ah\u0027,\u0027human\u0027,\u0027date\u0027,\u0027health\u0027,\u0027fry\u0027,\u0027many\u0027,\u0027is\u0027,\u0027the\u0027,\u0027by\u0027,\u0027on\u0027,\u0027ap\u0027,\u0027er\u0027,\u0027church\u0027,\n]\n\n\ndef getConfig():\n    \"\"\"\n    Retrieve app configuration parameters\n    such as database connections\n    :param name: text - config file name\n    :return:\n    \"\"\"\n    config_file \u003d os.path.join(SCRIPT_DIR, \u0027conf\u0027, \u0027config.ini\u0027)\n    config \u003d ConfigParser.ConfigParser()\n    config.read(config_file)\n    return config\n\n\ndef versiondate_column(column, version, dt):\n    \"\"\"\n    Calculate version column for this week\n    \"\"\"\n    return \u0027{}__{}__{}\u0027.format(column,\n                               version.replace(\u0027.\u0027, \u0027_\u0027),\n                               datetime.datetime.strftime(dt, \u0027%Y%W\u0027))\n\n\n\"\"\"\nStanford Tagger Functions\nTODO: Need jar loaded in pyspark, training data jar available in path\nNot currently being used\n\"\"\"\n\ndef get_stanford_tagger():\n    \"\"\"\n    Use the superior Stanford tagger\n    \"\"\"\n    training_db \u003d os.path.join(\n        SCRIPT_DIR,\n        \u0027resources/classifiers/english.conll.4class.distsim.crf.ser.gz\u0027)\n    jar_file \u003d os.path.join(SCRIPT_DIR, \u0027resources/stanford-ner.jar\u0027)\n    tagger \u003d nltk.StanfordNERTagger(training_db, path_to_jar\u003djar_file)\n    return tagger\n\n\ndef get_stanford_gpes(text):\n    \"\"\"\n    Return a list of possible names based on Stanford NLTK chunking\n    \"\"\"\n    tokens \u003d [x.strip(\u0027.\u0027) for x in nltk.word_tokenize(text)]\n    tagger \u003d get_stanford_tagger()\n    stagged_text \u003d tagger.tag(tokens)\n    ne_tree \u003d stanford2tree(stagged_text)\n    ne_in_sent \u003d []\n    for subtree in ne_tree:\n        if type(\n                subtree) \u003d\u003d nltk.Tree:  # If subtree is a noun chunk, i.e. NE !\u003d \"O\"\n            ne_label \u003d subtree.label()\n            ne_string \u003d \" \".join([token for token, pos in subtree.leaves()])\n            if ne_label in [\u0027ORGANIZATION\u0027, \u0027LOCATION\u0027]:\n                ne_in_sent.append(ne_string)\n    return ne_in_sent\n\n\ndef stanfordNE2BIO(tagged_sent):\n    bio_tagged_sent \u003d []\n    prev_tag \u003d \"O\"\n    for token, tag in tagged_sent:\n        if tag \u003d\u003d \"O\":  # O\n            prev_tag \u003d tag\n            continue\n        if tag !\u003d \"O\" and prev_tag \u003d\u003d \"O\":  # Begin NE\n            bio_tagged_sent.append((token, \"B-\" + tag))\n            prev_tag \u003d tag\n        elif prev_tag !\u003d \"O\" and prev_tag \u003d\u003d tag:  # Inside NE\n            bio_tagged_sent.append((token, \"I-\" + tag))\n            prev_tag \u003d tag\n        elif prev_tag !\u003d \"O\" and prev_tag !\u003d tag:  # Adjacent NE\n            bio_tagged_sent.append((token, \"B-\" + tag))\n            prev_tag \u003d tag\n\n    return bio_tagged_sent\n\n\ndef stanford2tree(ne_tagged_sent):\n    bio_tagged_sent \u003d stanfordNE2BIO(ne_tagged_sent)\n    bio_tagged_filtered \u003d [x for x in bio_tagged_sent if x[0]]\n    if bio_tagged_filtered:\n        sent_tokens, sent_ne_tags \u003d zip(*bio_tagged_filtered)\n        pos_tokens \u003d nltk.pos_tag(sent_tokens)\n        sent_pos_tags \u003d [pos for token, pos in pos_tokens]\n        sent_conlltags \u003d [(token, pos, ne) for token, pos, ne in\n                          zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n        ne_tree \u003d nltk.conlltags2tree(sent_conlltags)\n        return ne_tree\n    else:\n        return []\n\n\"\"\"\nAlternate NLTK tag methods\nCurrently being used but not as good as Stanford tagger\n\"\"\"\n\nnospace_langs \u003d (\u0027zh\u0027,)\n\ndef get_nltk_gpes(text):\n    \"\"\"\n    Use standard NLTK tagger (sucks)\n    \"\"\"\n    try:\n        sents \u003d nltk.sent_tokenize(re.sub(\u0027[\\:\\\\n]\u0027,\u0027.\u0027,text))\n    except LookupError:\n        print(\u0027Download nltk data\u0027)\n        download([\u0027punkt\u0027,\u0027maxent_treebank_pos_tagger\u0027,\u0027averaged_perceptron_tagger\u0027,\u0027maxent_ne_chunker\u0027,\u0027words\u0027])\n        sents \u003d nltk.sent_tokenize(re.sub(\u0027[\\:\\\\n]\u0027,\u0027.\u0027,text))\n    gpes \u003d []\n    for sent in sents:\n        tokenizer \u003d nltk.RegexpTokenizer(\u0027\\w(?:[-\\w]*[\\\u0027\\w]*\\w)?\u0027)\n        tokens \u003d tokenizer.tokenize(sent.replace(\u0027,\u0027, \u0027.\u0027))\n        tagged_text \u003d nltk.pos_tag(tokens)\n        for subtree in nltk.ne_chunk(tagged_text).subtrees():\n            # State/country codes often end up labelled as \u0027ORGANIZATION\u0027\n            if subtree.label() in [\u0027GPE\u0027, \u0027ORGANIZATION\u0027, \u0027PERSON\u0027]:\n                gpe \u003d u\u0027 \u0027.join([t[0] for t in subtree])\n                if len(gpe) \u003e 2 or (len(gpe) \u003e 1 and gpe.isupper()):\n                    gpes.append(gpe)\n            elif subtree.label() \u003d\u003d \u0027S\u0027:\n                for t in subtree:\n                    if type(t) \u003d\u003d tuple and t[0] and t[1].startswith(\u0027NNP\u0027):\n                        gpe \u003d u\u0027 \u0027.join([t[0]])\n                        if len(gpe) \u003e 2 or (len(gpe) \u003e 1 and gpe.isupper()):\n                            gpes.append(gpe)\n    return gpes\n\n\ndef dumb_gpes(text, max\u003d10):\n    \"\"\"\n    Split text by spaces, stripping non-alphanumeric characters,\n    and return the resulting list\n    \"\"\"\n    words \u003d re.sub(r\u0027,|\\\u0027|\"|:\u0027, \u0027 \u0027, text)\n    words \u003d filter(unicode.isalnum, words.split())\n    words \u003d [word for word in words if len(word) \u003e 2]\n    return words[0:max]\n\n\ndef splitchars(text, maxwords\u003d20, maxlen\u003d5, minlen\u003d2):\n    \"\"\"\n    Intended for Asian languages lacking spaces between words,\n    this function splits text at intervals producing a list of\n    words each containing minlen to maxlen characters.\n    \"\"\"\n    text \u003d filter(unicode.isalnum, text)\n    words \u003d set()\n    texts \u003d text.split()\n    for t in texts:\n        for x in range(len(t)):\n            if len(words) \u003e\u003d maxwords:\n                break\n            for y in range(maxlen, 0, -1):\n                if len(words) \u003e\u003d maxwords:\n                    break\n                if x + y \u003c\u003d len(t) and ((x + y) - x) \u003e\u003d minlen:\n                    words.add((t[x:x + y]))\n    return list(words)\n\n\n\"\"\"\ngeocode match Scoring methods\n\"\"\"\n\ndef relation_score(places, admins, countries):\n    \"\"\"\n    Adjust scores if the admin/country name or code of a place/admin match\n    equals the name/code of an admin/country match.\n    \"\"\"\n    for place in places.keys():\n        for item in places[place]:\n            admin_code \u003d item[\u0027admin1\u0027]\n            country \u003d item[\u0027country\u0027]\n            if admin_code:\n                if admin_code in admins.keys():\n                    item[\u0027score\u0027] *\u003d 3\n                    for admitem in admins[admin_code]:\n                        if admitem[\u0027country\u0027] \u003d\u003d country:\n                            admitem[\u0027score\u0027] *\u003d 3\n                else:\n                    for admin in admins.keys():\n                        for admitem in admins[admin]:\n                            if (admitem[\u0027country\u0027] \u003d\u003d country and\n                                        admitem[\u0027admin1\u0027] \u003d\u003d admin_code and\n                                        admitem[\u0027feature_code\u0027] \u003d\u003d \u0027ADM1\u0027):\n                                item[\u0027score\u0027] *\u003d 4\n                                admitem[\u0027score\u0027] *\u003d 3\n            if country:\n                for cc in countries.keys():\n                    for ccitem in countries[cc]:\n                        if ccitem[\u0027country\u0027] \u003d\u003d country:\n                            ccitem[\u0027score\u0027] *\u003d 2\n                            if ccitem[\u0027matching_word\u0027] !\u003d item[\u0027matching_word\u0027] \\\n                                    and item[\u0027matching_word\u0027] not in (admins.keys() + countries.keys()):\n                                item[\u0027score\u0027] *\u003d 2\n    for adm in admins.keys():\n        for item in admins[adm]:\n            country \u003d item[\u0027country\u0027]\n            if country:\n                for cc in countries.keys():\n                    for ccitem in countries[cc]:\n                        if ccitem[\u0027country\u0027] \u003d\u003d country:\n                            if ccitem[\u0027matching_word\u0027] !\u003d item[\u0027matching_word\u0027] \\\n                                    and item[\u0027matching_word\u0027] in admins.keys():\n                                item[\u0027score\u0027] *\u003d 2\n\n\n\ndef score_matches(matches, name, codes):\n    \"\"\"\n    Make a semi-educated guess about which match is better\n    \"\"\"\n    for nmatch in matches:\n        fc \u003d nmatch[\u0027feature_code\u0027]\n        alternates \u003d []\n        score \u003d 1\n        exact_match \u003d False\n        source \u003d nmatch[\u0027source\u0027]\n        if source in (\u0027name\u0027,\u0027ascii\u0027):\n            # Best type of nmatch\n            score +\u003d 100000\n            exact_match \u003d True\n        elif source \u003d\u003d \u0027alternate\u0027:\n            # 2nd best (alternate name - for all foreign languages)\n            score +\u003d 100000\n            exact_match \u003d True\n        if name not in codes and (nmatch[\u0027name\u0027].lower().startswith(name.lower()) or nmatch[\u0027name\u0027].lower().endswith(name.lower())):\n            score +\u003d 50000\n            if fc.startswith(\u0027PCL\u0027):\n                score +\u003d 1000000\n        if name in codes:\n            if (name.upper() \u003d\u003d nmatch[\u0027country\u0027] or name.upper() in alternates) and fc.startswith(\u0027PCL\u0027):\n                score +\u003d 50000\n            elif (name.upper() \u003d\u003d nmatch[\u0027admin1\u0027] or name.upper() in alternates) and fc \u003d\u003d \u0027ADM1\u0027:\n                score +\u003d 50000\n            else:\n                score \u003d -1\n        # Give higher scores to political capitals.\n        # \u0027Moscow\u0027 is most likely the one in Russia, not Maine\n        if score \u003e 50000:\n            if fc:\n                if fc.startswith(u\u0027PCL\u0027):\n                    score +\u003d 100000\n                elif fc \u003d\u003d \u0027ADM1\u0027:\n                    score +\u003d 50000\n                elif fc.startswith(\u0027PPLA\u0027) and exact_match:\n                    score +\u003d 40000\n                elif fc \u003d\u003d \u0027PPLC\u0027:\n                    score +\u003d 400000\n        if score \u003e 1:\n            if codes:\n                admin \u003d nmatch[\u0027admin1\u0027]\n                if admin and admin in codes:\n                    score *\u003d 4\n                admin \u003d nmatch[\u0027country\u0027]\n                if admin and admin in codes:\n                    score *\u003d 2\n            if alternates:\n                # Bonus points for actually having alternate names\n                score +\u003d 5000\n            if nmatch[\u0027feature_code\u0027].startswith(\u0027PPL\u0027):\n                score +\u003d ((nmatch[\u0027population\u0027] + 1) / 10)\n            else:\n                score +\u003d ((nmatch[\u0027population\u0027] + 1) / 1000)\n        nmatch[\u0027score\u0027] \u003d score\n\n\ndef name_match(name, results, admin1\u003dNone, country\u003dNone):\n    \"\"\"\n    Create a list of objects for each match found for a name\n    \"\"\"\n    name_matches \u003d []\n    for geoname in results:\n        if not admin1 or admin1 \u003d\u003d geoname[\u0027admin1_code\u0027]:\n            if not country or country \u003d\u003d geoname[\"country_code\"]:\n                name_matches.append({\n                    \u0027coord\u0027: [[geoname[\u0027longitude\u0027], geoname[\u0027latitude\u0027]]],\n                    \u0027geonameid\u0027: geoname[\u0027geonameid\u0027],\n                    \u0027name\u0027: geoname[\u0027name\u0027],\n                    \u0027matching_word\u0027: name,\n                    \u0027admin1\u0027: geoname[\u0027admin1_code\u0027],\n                    \u0027admin2\u0027: geoname[\u0027admin2_code\u0027],\n                    \u0027admin3\u0027: geoname[\u0027admin3_code\u0027],\n                    \u0027admin4\u0027: geoname[\u0027admin4_code\u0027],\n                    \u0027feature_code\u0027: geoname[\u0027feature_code\u0027],\n                    \u0027country\u0027: geoname[\u0027country_code\u0027],\n                    \u0027population\u0027: geoname.get(\u0027population\u0027) or 0,\n                    \u0027source\u0027: geoname.get(\u0027source\u0027),\n                    \u0027score\u0027: -1\n                })\n    return name_matches\n\n\ndef create_version_columns(columns, keyspace, table):\n    return\n    # config \u003d getConfig()\n    # cluster \u003d Cluster(config.get(\u0027cassandra\u0027, \u0027host\u0027).split(\u0027,\u0027))\n    # session \u003d cluster.connect()\n    # existing_cols \u003d cluster.metadata.keyspaces[keyspace].tables[table].columns\n    # #print(existing_cols)\n    # #print(columns)\n    # not_in_schema \u003d [i for i in columns.items() if i[0] not in existing_cols.keys()]\n    # if not_in_schema:\n    #     for c in not_in_schema:\n    #         # Add column for processor run date to contain version number\n    #         cql \u003d \u0027ALTER TABLE {}.{} add {} {};\u0027.format(keyspace,\n    #                                                     table,\n    #                                                     c[0],\n    #                                                     c[1])\n    #         #print(cql)\n    #         session.execute(cql)\n\n\ndef process_geonames(table_map):\n\n    \"\"\"\n    Create new version columns if necessary\n    \"\"\"\n    today \u003d datetime.datetime.now()\n    vdc \u003d versiondate_column(PROCESSOR_NAME, PROCESSOR_VERSION, today)\n    v \u003d \u0027{}_v\u0027.format(PROCESSOR_NAME)\n    version_cols \u003d {v: \u0027text\u0027, vdc: \u0027frozen\u003cset\u003cfrozen\u003cplace\u003e\u003e\u003e\u0027}\n    for (keyspace, tables) in table_map.items():\n        for table in tables:\n            print keyspace, table\n            create_version_columns(version_cols, keyspace, table)\n\n\n    \"\"\"\n    Set up SparkContext and sqlContext\n    \"\"\"\n    # conf \u003d SparkConf().setAppName(\"Geocoder\")\n    # conf.set(\"spark.driver.allowMultipleContexts\", True)\n    # conf.set(\"redis.host\", \"localhost\")\n    # conf.set(\"redis.port\", \"6379\")\n    # conf.set(\"redis.db\", \"2\")\n\n    # sc \u003d SparkContext(conf\u003dconf)\n    # sqlContext \u003d SQLContext(sc)\n\n\n\n    \"\"\"\n    Load geonames from ScyllaDB\n    \"\"\"\n    geocode_rdd \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"geonames\", keyspace\u003d\"geonames\").load()\n    geocode_pdd \u003d geocode_rdd.toPandas()\n    geocode_pdd[\u0027name\u0027] \u003d geocode_pdd[\u0027name\u0027].str.lower()\n    sqlContext.registerDataFrameAsTable(geocode_rdd, \"geonames\")\n\n    def panda_query(names, codes):\n        \"\"\"\n        Performs a query against Geonames Panda DataFrame.\n        :param names: list of placenames to search for\n        :param codes: list of potential admin/country codes to search for\n        :return: matching geonames records\n        \"\"\"\n\n        place_matches \u003d {}\n        admin_matches \u003d {}\n        country_matches \u003d {}\n\n        for name in names:\n            result_pdd \u003d geocode_pdd[geocode_pdd[\"name\"] \u003d\u003d name.lower()]\n\n            results \u003d result_pdd.T.to_dict().values()\n            name_matches \u003d name_match(name, results)\n\n            score_matches(name_matches, name, codes)\n            for nmatch in name_matches:\n                featurecode \u003d nmatch[\u0027feature_code\u0027]\n                if featurecode and featurecode.startswith(\u0027ADM\u0027):\n                    admin_matches.setdefault(name, []).append(nmatch)\n                elif featurecode and featurecode.startswith(\u0027PCL\u0027):\n                    country_matches.setdefault(name, []).append(nmatch)\n                else:\n                    place_matches.setdefault(name, []).append(nmatch)\n        return place_matches, admin_matches, country_matches\n\n    def get_geonames(names, codes):\n        \"\"\"\n        Find matching geonames records given a list of placenames and (possibly blank) state/country codes\n        \"\"\"\n        all_matches \u003d {}\n        place_matches, admin_matches, country_matches \u003d panda_query(names,\n                                                                    codes)\n        relation_score(place_matches, admin_matches, country_matches)\n\n        for group in (place_matches, admin_matches, country_matches):\n            unique_ids \u003d []\n            for k, v in group.iteritems():\n                ranked \u003d sorted(v, key\u003dlambda k: k[\u0027score\u0027], reverse\u003dTrue)\n                if ranked[0][\u0027score\u0027] \u003e 1 and \\\n                                ranked[0][\u0027geonameid\u0027] not in unique_ids and \\\n                        (k not in all_matches or all_matches[k][\u0027score\u0027] \u003c\n                            ranked[0][\u0027score\u0027]):\n                    all_matches[k] \u003d ranked[0]\n                    unique_ids.append(ranked[0][\u0027geonameid\u0027])\n\n        place_list \u003d []\n        for place in sorted(all_matches.values(), key\u003dlambda k: k[\u0027score\u0027],\n                            reverse\u003dTrue):\n            place_list.append({\n                \u0027place_name\u0027: place[\u0027name\u0027],\n                \u0027place_lat\u0027: place[\u0027coord\u0027][0][1],\n                \u0027place_lng\u0027: place[\u0027coord\u0027][0][0],\n                \u0027geo_geonameid\u0027: int(place[\u0027geonameid\u0027])\n            })\n        return place_list\n\n    def geocode(name):\n        return get_geonames([name], [])\n\n    def extract(text, lang\u003d\u0027en\u0027, dumb_tokens\u003dFalse, stanford\u003dFalse):\n        \"\"\"\n        Extract possible placenames from text, then query Geonames dataframe for matches.\n        \"\"\"\n        name_counts \u003d OrderedDict([])\n        code_counts \u003d {}\n\n        if not text:\n            # Nothing to do\n            return\n\n        if type(text) \u003d\u003d str:\n            text \u003d unicode(text)\n\n        text \u003d re.sub(DELETE_REGEX, u\u0027, \u0027, text)\n        text \u003d re.sub(r\u0027\\\u0027\u0027, u\u0027\\\u0027\\\u0027\u0027, text)\n\n        # Look for state/country codes (2-3 capital letters)\n        possible_codes \u003d list(\n            set(re.findall(CODE_REGEX, text)))\n        for code in possible_codes:\n            if code.lower() not in IGNORE_WORDS:\n                code_counts[code.lower()] \u003d code_counts.get(\n                    code, 0) + 1\n\n        possible_names \u003d []\n        if lang in nospace_langs:\n            # No spaces in language, so make spaces\n            possible_names \u003d splitchars(text)[0:255]\n        elif dumb_tokens:\n            # Split words on spaces\n            possible_names \u003d dumb_gpes(text)\n        else:\n            # Use NLTK tokenizer and Stanford tagger to find placenames.\n            if stanford:\n                try:\n                    possible_names.extend(get_stanford_gpes(text))\n                except OSError:\n                    possible_names.extend(get_nltk_gpes(text))\n            else:\n                possible_names.extend(get_nltk_gpes(text))\n                name_prefixes \u003d [\u0027san\u0027, \u0027las\u0027, \u0027los\u0027]\n                for prefix in name_prefixes:\n                    san_matches \u003d re.findall(\u0027{} \\w+\u0027.format(prefix), text,\n                                             flags\u003dre.IGNORECASE)\n                    for san_match in san_matches:\n                        if san_match not in possible_names:\n                            possible_names.append(san_match)\n                        for san_word in san_match.split(\u0027 \u0027):\n                            if san_word in possible_names:\n                                possible_names.remove(san_word)\n\n        for i, possible_geoname in enumerate(possible_names):\n            if possible_geoname.lower() in IGNORE_WORDS: continue\n\n            # If it looks like a state/country code, treat it that way\n            if (2 \u003c\u003d len(\n                    possible_geoname) \u003c\u003d 3) and possible_geoname.isupper() and \\\n                            lang not in nospace_langs:\n                code_counts[possible_geoname.lower()] \u003d code_counts.get(\n                    possible_geoname, 0) + 1\n            else:\n                name_counts[possible_geoname.lower()] \u003d name_counts.get(\n                    possible_geoname.lower(), 0) + 1\n\n        if not name_counts and not code_counts:\n            # Nothing to do\n            return []\n\n        codes \u003d code_counts.keys()\n\n        names \u003d name_counts.keys()\n        names.extend(code_counts.keys())\n\n        return get_geonames(names, codes)\n\n\n    \"\"\"\n    UDF Functions for a) extract and geocode, b) geocode only\n    \"\"\"\n    PlaceType \u003d  ArrayType(StructType([\n        StructField(\u0027place_name\u0027, StringType(), True),\n        StructField(\u0027place_lat\u0027, FloatType(), True),\n        StructField(\u0027place_lng\u0027, FloatType(), True),\n        StructField(\u0027place_id\u0027, IntegerType(), True),\n        StructField(\u0027geo_geonameid\u0027, IntegerType(), True)\n        ]),True)\n\n    extract_udf \u003d udf(lambda txt: extract(txt), PlaceType)\n    geocode_udf \u003d udf(lambda txt: geocode(txt), PlaceType)\n\n\n    \"\"\"\n    Load processed_news.news table into SparkRDD\n    \"\"\"\n    if \u0027processed_news\u0027 in table_map.keys() or \u0027test_news\u0027 in table_map.keys():\n        # Get news table\n        keyspace \u003d \u0027test_news\u0027\n        all_news \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"news\", keyspace\u003dkeyspace).load()\n        all_news.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n        # Select news records which have a place_name and different version\n        sub_hm_news \u003d all_news.filter(\n            (all_news[\"place_name\"].isNotNull()) \u0026\n            (\n                (all_news[\u0027place_list_v\u0027].isNull()) |\n                (all_news[\u0027place_list_v\u0027] !\u003d PROCESSOR_VERSION)\n            )\n        )\n        sub_hm_news.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n        # Geocode rows with existing place_name column\n        news_placelist \u003d sub_hm_news\\\n            .withColumn(\"place_list\", geocode_udf(sub_hm_news[\"place_name\"]))\\\n            .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n\n        # Create versioning columns and populate them\n        print(\"SAVING PLACENAME GEOCODES\")\n        news_placelist\\\n            .filter(news_placelist[\"place_list\"].isNotNull())\\\n            .select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_list\")\\\n            .withColumn(v, lit(PROCESSOR_VERSION))\\\n            .withColumn(vdc, news_placelist[\u0027place_list\u0027])\\\n            .write.format(\"org.apache.spark.sql.cassandra\")\\\n            .options(table\u003d\"news\", keyspace\u003dkeyspace)\\\n            .save(mode\u003d\"append\")\n\n        news_placelist.unpersist()\n        sub_hm_news.unpersist()\n\n        # Select rows with null/empty place_name values and non-null t values\n        sub_news_noname \u003d all_news.filter(\n            (all_news[\"place_name\"].isNull()) \u0026\n            (all_news[\"t\"].isNotNull()) \u0026\n            (\n                (all_news[\u0027place_list_v\u0027].isNull()) |\n                (all_news[\u0027place_list_v\u0027] !\u003d PROCESSOR_VERSION)\n            )\n        ).persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n        # Extract and geocode rows without place_name values based on \u0027t\u0027 column\n        sub_news_noname2 \u003d sub_news_noname\\\n            .withColumn(\"place_list\", extract_udf(sub_news_noname[\"t\"]))\\\n            .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n        \"\"\"\n        Assign extract/geocode results to  \"place_lat\", \"place_lng\" columns, based on 1st element of place_list\n        \"\"\"\n        print(\"SAVING T GEOCODES\")\n        sub_news_noname2.filter(sub_news_noname2[\"place_list\"].isNotNull()) \\\n            .select(sub_news_noname.select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_list\"))\\\n            .withColumn(v, lit(PROCESSOR_VERSION))\\\n            .withColumn(vdc, sub_news_noname2[\u0027place_list\u0027])\\\n            .write.format(\"org.apache.spark.sql.cassandra\")\\\n            .options(table\u003d\"news\", keyspace\u003dkeyspace).save(mode\u003d\"append\")\n\n        sub_news_noname2.unpersist()\n        sub_news_noname.unpersist()\n        all_news.unpersist()\n\n    if \u0027testing\u0027 in table_map.keys():\n        print(\"GET ALLAGG\")\n        all_agg \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(\n            table\u003d\"agg_table\", keyspace\u003d\"testing\").load()\n        all_agg.persist(StorageLevel.MEMORY_AND_DISK_SER)\n        print(\"GOT ALL AGG\")\n        agg_geo \u003d all_agg.filter(all_agg[\u0027place_lat\u0027].isNull())\\\n                         .withColumn(\"place_list\", extract_udf(all_agg[\"description\"]))\n        agg_geo.persist(StorageLevel.MEMORY_AND_DISK_SER)\n        print(\"GEOCODED\")\n        print agg_geo.count()\n        print(\"COUNTED\")\n\n        agg_geo2 \u003d agg_geo.filter(agg_geo[\"place_list\"].isNotNull())\\\n            .withColumn(\"place_lat\", agg_geo[\"place_list\"][0].place_lat)\\\n            .withColumn(\"place_lng\", agg_geo[\"place_list\"][0].place_lng)\n        agg_geo2.persist(StorageLevel.MEMORY_AND_DISK_SER)\n        print(\"UPDATED COORDS\")\n        agg_geo2.select(\"id\", \"place_lat\", \"place_lng\")\\\n            .write.format(\"org.apache.spark.sql.cassandra\")\\\n            .options(table\u003d\"agg_table\", keyspace\u003d\"testing\").save(mode\u003d\"append\")\n        print(\"SAVED\")\n        agg_geo2.unpersist()\n        agg_geo.unpersist()\n        all_agg.unpersists()\n\n    if \u0027processed_forum\u0027 in table_map.keys():\n        for table in table_map[\u0027processed_forum\u0027]:\n            forum_all \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\")\\\n                .options(table\u003dtable, keyspace\u003d\"processed_forum\").load()\\\n                .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Select records which have a loc and different version or never processed\n            sub_forum \u003d forum_all.filter(\n                (forum_all[\u0027loc\u0027].isNotNull()) #\u0026 (\n                #    (forum_all[\u0027place_list_v\u0027].isNull()) |\n                #    (forum_all[\u0027place_list_v\u0027] !\u003d PROCESSOR_VERSION)\n                #)\n            )\n\n            # Geocode\n            subforum_geocoded \u003d sub_forum.select(\"id\", \"ds\", \"ymds\", \"rdt\", \"loc\") \\\n                .withColumn(\"place_list\", geocode_udf(sub_forum[\"loc\"])) \\\n                .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Version and save\n            print(\"SAVING FORUMTABLE {} TO SCYLLA - GEOCODE BY LOC\".format(table))\n            subforum_geocoded.withColumn(v, lit(PROCESSOR_VERSION)) \\\n                .withColumn(vdc, subforum_geocoded[\u0027place_list\u0027])\\\n                .write.format(\"org.apache.spark.sql.cassandra\")\\\n                .options(table\u003dtable, keyspace\u003d\"processed_forum\").save(mode\u003d\"append\")\n            print(\"DONE SAVING FORUMTABLE {} TO SCYLLA\".format(table))\n\n\n            # Select records which have no loc and different version or never processed\n            sub_forum \u003d forum_all.filter(\n                (forum_all[\u0027loc\u0027].isNull()) #\u0026 (\n                #    (forum_all[\u0027place_list_v\u0027].isNull()) |\n                #    (forum_all[\u0027place_list_v\u0027] !\u003d PROCESSOR_VERSION)\n                #)\n            ).persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Geocode rows without loc column\n            forum_placelist \u003d sub_forum\\\n                .withColumn(\"place_list\", extract_udf(sub_forum[\"t\"]))\\\n                .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            print(\"SAVING FORUMTABLE {} TO SCYLLA - EXTRACT BY T\".format(table))\n            # Assign extract/geocode results to place_list\n            forum_placelist \\\n                .select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_list\") \\\n                .withColumn(v, lit(PROCESSOR_VERSION)) \\\n                .withColumn(vdc, forum_placelist[\u0027place_list\u0027])\\\n                .write.format(\"org.apache.spark.sql.cassandra\") \\\n                .options(table\u003dtable, keyspace\u003d\"processed_forum\").save(mode\u003d\"append\")\n            print(\"DOE SAVING FORUMTABLE {} TO SCYLLA - EXTRACT BY T\".format(table))\n            \n            forum_all.unpersist()\n            forum_placelist.unpersist()\n\n    if \u0027processed_social\u0027 in table_map.keys():\n        for table in table_map[\u0027processed_social\u0027]:\n\n            social_all \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\")\\\n                .options(table\u003dtable, keyspace\u003d\"processed_social\").load()\\\n                .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Select records which have a loc and different version or never processed\n            sub_social \u003d social_all.filter(\n                    (social_all[\u0027pln\u0027].isNull()) \u0026\n                    (social_all[\u0027tln\u0027].isNull()) \u0026\n                    (social_all[\u0027loc\u0027].isNotNull()) \u0026 (\n                        (social_all[\u0027place_list_v\u0027].isNull()) |\n                        (social_all[\u0027place_list_v\u0027] !\u003d PROCESSOR_VERSION)\n                    )\n            ).persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Geocode rows with loc column\n            social_placelist \u003d sub_social\\\n                .withColumn(\"place_list\", geocode_udf(sub_social[\"loc\"]))\\\n                .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Version and save results\n            print(\"SAVING {} TO SCYLLA - GEOCODE BY LOC\".format(table))\n            social_placelist \\\n                .select(\"id\", \"ds\", \"ymds\", \"cr\", \"place_list\")\\\n                .withColumn(v, lit(PROCESSOR_VERSION))\\\n                .withColumn(vdc, social_placelist[\u0027place_list\u0027])\\\n                .write.format(\"org.apache.spark.sql.cassandra\") \\\n                .options(table\u003dtable, keyspace\u003d\"processed_social\")\\\n                .save(mode\u003d\"append\")\n            print(\"DONE SAVING {} TO SCYLLA - GEOCODE BY LOC\".format(table))\n\n            # Select records which have no loc and different version or never processed\n            sub_social \u003d social_all.filter(\n                (social_all[\u0027pln\u0027].isNull()) \u0026\n                (social_all[\u0027tln\u0027].isNull()) \u0026\n                (social_all[\u0027loc\u0027].isNull()) \u0026 (\n                    (social_all[\u0027place_list_v\u0027].isNull()) |\n                    (social_all[\u0027place_list_v\u0027] !\u003d PROCESSOR_VERSION)\n                )\n            ).persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Geocode rows with loc column\n            social_placelist \u003d sub_social.\\\n                withColumn(\"place_list\",extract_udf(sub_forum[\"t\"]))\\\n                .persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n            # Assign version and save results to place_list\n            print(\"SAVING {} TO SCYLLA - EXTRACT BY T\".format(table))\n            social_placelist\\\n                .select(\"id\", \"ds\", \"ymds\", \"cr\", \"place_list\")\\\n                .withColumn(v, lit(PROCESSOR_VERSION))\\\n                .withColumn(vdc, social_placelist[\u0027place_list\u0027])\\\n                .write.format(\"org.apache.spark.sql.cassandra\") \\\n                .options(table\u003dtable, keyspace\u003d\"processed_social\")\\\n                .save( mode\u003d\"append\")\n            print(\"DONE SAVING {} TO SCYLLA - EXTRACT BY T\".format(table))\n\n            social_placelist.unpersist()\n            social_all.unpersist()\n\n# if __name__ \u003d\u003d \u0027__main__\u0027:\n#     \"\"\"\n#     Proper way to run if loading to ScyllaDB:\n#     spark-submit\n#         --packages com.datastax.spark:spark-cassandra-connector_2.10:1.6.0,\n#                     com.databricks:spark-csv_2.10:1.5.0\n#         --conf spark.cassandra.connection.host\u003d\u003cScylla Host IP\u003e\n#         --driver-memory 4G\n#         \u003cpath_to\u003e/geocode_news.py [options]\n#     \"\"\"\n#     parser \u003d argparse.ArgumentParser(description\u003d\u0027ETL for geonames data\u0027)\n#     parser.add_argument(\u0027--news\u0027, dest\u003d\u0027news\u0027, action\u003d\u0027store_true\u0027,\n#                         help\u003d\u0027Geocode news table\u0027)\n#     parser.add_argument(\u0027--forum\u0027, dest\u003d\u0027forum\u0027, action\u003d\u0027store_true\u0027,\n#                         help\u003d\u0027Geocode forum tables\u0027)\n#     parser.add_argument(\u0027--social\u0027, dest\u003d\u0027social\u0027, action\u003d\u0027store_true\u0027,\n#                         help\u003d\u0027Geocode social tables\u0027)\n#     parser.add_argument(\u0027--test\u0027, dest\u003d\u0027test\u0027, action\u003d\u0027store_true\u0027,\n#                         help\u003d\u0027Geocode test tables\u0027)\n#     args \u003d parser.parse_args()\n#     tables \u003d {}\n#     if args.news:\n#         #tables[\u0027processed_news\u0027] \u003d [\u0027news\u0027]\n#         tables[\u0027test_news\u0027] \u003d [\u0027news\u0027]\n#     if args.forum:\n#         tables[\u0027processed_forum\u0027] \u003d [\u0027forum\u0027, \u0027reddit\u0027]\n#     if args.social:\n#         tables[\u0027processed_social\u0027] \u003d [\u0027twitter\u0027, \u0027facebook\u0027]\n#     if args.test:\n#         tables[\u0027testing\u0027] \u003d [\u0027agg_table\u0027]\n#     print tables\n\n#     process_geonames(tables)\n",
      "dateUpdated": "Oct 5, 2016 1:30:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475610246967_1450004657",
      "id": "20161004-194406_1693644784",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 4, 2016 7:44:06 PM",
      "dateStarted": "Oct 5, 2016 1:30:26 PM",
      "dateFinished": "Oct 5, 2016 1:30:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475613259376_-1347009886",
      "id": "20161004-203419_583655553",
      "dateCreated": "Oct 4, 2016 8:34:19 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ntables\u003d {\u0027processed_forum\u0027: [\u0027forum\u0027]}\n",
      "dateUpdated": "Oct 5, 2016 1:30:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475610395387_1724192705",
      "id": "20161004-194635_1276000344",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 4, 2016 7:46:35 PM",
      "dateStarted": "Oct 5, 2016 1:30:35 PM",
      "dateFinished": "Oct 5, 2016 1:30:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nprocess_geonames(tables)",
      "dateUpdated": "Oct 5, 2016 1:30:37 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475610526576_810565664",
      "id": "20161004-194846_489341777",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "processed_forum forum\nSAVING FORUMTABLE forum TO SCYLLA - GEOCODE BY LOC\nPy4JJavaError: An error occurred while calling o247.save.\n: org.apache.spark.SparkException: Job 109 cancelled part of cancelled job group zeppelin-20161004-194846_489341777\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:37)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:67)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:85)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o247.save.\\n\u0027, JavaObject id\u003do248), \u003ctraceback object at 0x7f8d1a371c68\u003e)"
      },
      "dateCreated": "Oct 4, 2016 7:48:46 PM",
      "dateStarted": "Oct 5, 2016 1:30:37 PM",
      "dateFinished": "Oct 5, 2016 2:41:38 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475610562170_271394538",
      "id": "20161004-194922_296659152",
      "dateCreated": "Oct 4, 2016 7:49:22 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "geocoder_module",
  "id": "2BYBBX63H",
  "angularObjects": {
    "2BTJ3P41C:shared_process": [],
    "2BUSJ5B5T:shared_process": [],
    "2BUCYP1D2:shared_process": [],
    "2BRYFEHJ7:shared_process": [],
    "2BT211CDH:shared_process": [],
    "2BTX1MQS6:shared_process": [],
    "2BU87RU3U:shared_process": [],
    "2BTB82RPQ:shared_process": [],
    "2BRP3ZUWJ:shared_process": [],
    "2BT3JK3T4:shared_process": [],
    "2BUTB2HA6:shared_process": [],
    "2BTXGDVEJ:shared_process": [],
    "2BRZ896X6:shared_process": [],
    "2BSSEDUXN:shared_process": [],
    "2BSMJA8VG:shared_process": [],
    "2BSJYK2YE:shared_process": [],
    "2BUZX9EWW:shared_process": []
  },
  "config": {},
  "info": {}
}