{
  "paragraphs": [
    {
      "text": "import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkConf, SparkContext}\nimport com.mongodb.casbah.{WriteConcern \u003d\u003e MongodbWriteConcern}\nimport com.stratio.datasource._\nimport com.stratio.datasource.mongodb._\nimport com.stratio.datasource.mongodb.schema._\nimport com.stratio.datasource.mongodb.writer._\nimport com.stratio.datasource.mongodb.config._\nimport com.stratio.datasource.mongodb.config.MongodbConfig._\nimport org.apache.spark.sql.SQLContext\nimport com.stratio.datasource.util.Config._\nimport scala.collection.mutable.WrappedArray\n\n\nval space \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else text.split(\" \")\n})\n\nvar builder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"final_master\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nvar readConfig \u003d builder.build()\nval master \u003d sqlContext.fromMongoDB(readConfig).select(\"name\", \"synonyms\", \"_id\", \"tag\").where(size($\"synonyms\") \u003e 0).withColumn(\"synonyms\", explode($\"synonyms\")).withColumn(\"synonyms\", regexp_replace(lower($\"synonyms\"), \"\"\"[\\p{Punct}]\"\"\", \" \")).withColumn(\"size\", size(space($\"synonyms\"))).sort(desc(\"size\"))\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"pos_words\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval pos \u003d sqlContext.fromMongoDB(readConfig).select(\"x\", \"_id\")\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"neg_words\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval neg \u003d sqlContext.fromMongoDB(readConfig).select(\"x\", \"_id\")\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"final_id_all\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval id_all \u003d sqlContext.fromMongoDB(readConfig).select(\"name\", \"_id\")\n\nbuilder \u003d MongodbConfigBuilder(Map(Host -\u003e List(\"one-mongo.epidemi.co\"), Database -\u003e \"EpiOne\", Collection -\u003e\"pii\", SamplingRatio -\u003e 1.0, WriteConcern -\u003e \"normal\"))\nreadConfig \u003d builder.build()\nval pii \u003d sqlContext.fromMongoDB(readConfig).select(\"phrase\", \"_id\").withColumnRenamed(\"synonyms\", \"phrase\")\n\n\n\nval symp \u003d master.filter($\"tag\"\u003d\u003d\u003d\"symptom\").collect()\nval ue \u003d master.filter($\"tag\"\u003d\u003d\u003d\"ue\").collect()\nval prod \u003d master.filter($\"tag\"\u003d\u003d\u003d\"product\").collect()\nval comp \u003d master.filter($\"tag\"\u003d\u003d\u003d\"organization\").collect()\nval top \u003d master.filter($\"tag\"\u003d\u003d\u003d\"business_category\").collect()\nval des \u003d master.filter($\"tag\"\u003d\u003d\u003d\"disease\").collect()\n//val cat \u003d master.filter($\"tag\"\u003d\u003d\u003d\"place_category\").collect()\n//val spec \u003d master.filter($\"tag\"\u003d\u003d\u003d\"species\").collect()\nval pi \u003d pii.collect()\nval poso \u003d pos.collect()\nval nego \u003d neg.collect()\nval ids \u003d id_all.collect()\nval p_syn \u003d sc.broadcast(prod.map(x\u003d\u003e x(1).toString).toSet)\nval s_syn \u003d sc.broadcast(symp.map(x\u003d\u003e x(1).toString).toSet)\nval u_syn \u003d sc.broadcast(ue.map(x\u003d\u003e x(1).toString).toSet)\nval pi_syn \u003d sc.broadcast(pi.map(x\u003d\u003e x(0).toString).toSet)\nval comp_syn \u003d sc.broadcast(comp.map(x\u003d\u003e x(1).toString).toSet)\nval top_syn \u003d sc.broadcast(top.map(x\u003d\u003e x(1).toString).toSet)\nval des_syn \u003d sc.broadcast(des.map(x\u003d\u003e x(1).toString).toSet)\n//val cat_syn \u003d cat.map(x\u003d\u003e x(1).toString).toSet\n//val spec_syn \u003d spec.map(x\u003d\u003e x(1).toString).toSet\nval pos_syn \u003d sc.broadcast(poso.map(x\u003d\u003e x(0).toString).toSet)\nval neg_syn \u003d sc.broadcast(nego.map(x\u003d\u003e x(0).toString).toSet)\nval id_syn \u003d sc.broadcast(nego.map(x\u003d\u003e x(0).toString).toSet)\n\nvar pa0 : Map[String,String] \u003d Map()\nvar sa0 :Map[String,String] \u003d Map()\nvar ua0 :Map[String,String] \u003d Map()\nvar pia0 :Map[String,String] \u003d Map()\nvar ca0 :Map[String,String] \u003d Map()\nvar ta0 :Map[String,String] \u003d Map()\nvar das0 :Map[String,String] \u003d Map()\nvar cas0 :Map[String,String] \u003d Map()\nvar sas0 :Map[String,String] \u003d Map()\nvar pas0 :Map[String,String] \u003d Map()\nvar nas0 :Map[String,String] \u003d Map()\nvar ias0 :Map[String,String] \u003d Map()\n\nfor(i \u003c- prod){\n    pa0+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- symp){\n    sa0+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- ue){\n    ua0+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- pi){\n    pia0+\u003d (i(0).toString -\u003e i(1).toString)\n}\nfor(i \u003c- comp){\n    ca0+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- top){\n    ta0+\u003d (i(1).toString -\u003e i(2).toString)\n}\nfor(i \u003c- des){\n    das0+\u003d (i(1).toString -\u003e i(2).toString)\n}\n//for(i \u003c- cat){\n//    cas+\u003d (i(1).toString -\u003e i(2).toString)\n//}\n//for(i \u003c- spec){\n//    sas+\u003d (i(1).toString -\u003e i(2).toString)\n//}\nfor(i \u003c- poso){\n    pas0+\u003d (i(0).toString -\u003e i(1).toString)\n}\nfor(i \u003c- nego){\n    nas0+\u003d (i(0).toString -\u003e i(1).toString)\n}\nfor(i \u003c- ids){\n    ias0+\u003d (i(0).toString -\u003e i(1).toString)\n}\n\nvar pa \u003d sc.broadcast(pa0)\nvar sa \u003d sc.broadcast(sa0)\nvar ua \u003d sc.broadcast(ua0)\nvar pia \u003d sc.broadcast(pia0)\nvar ca \u003d sc.broadcast(ca0)\nvar ta \u003d sc.broadcast(ta0)\nvar das \u003d sc.broadcast(das0)\nvar cas \u003d sc.broadcast(cas0)\nvar sas \u003d sc.broadcast(sas0)\nvar pas \u003d sc.broadcast(pas0)\nvar nas \u003d sc.broadcast(nas0)\nvar ias \u003d sc.broadcast(ias0)\n\nval ns \u003d Array(master.select(\"size\").take(1)(0)(0).toString.toInt) \nval nsize \u003d ns.max\n",
      "dateUpdated": "Oct 23, 2016 4:30:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476916391444_-532483494",
      "id": "20161019-223311_827118306",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkConf, SparkContext}\nimport com.mongodb.casbah.{WriteConcern\u003d\u003eMongodbWriteConcern}\nimport com.stratio.datasource._\nimport com.stratio.datasource.mongodb._\nimport com.stratio.datasource.mongodb.schema._\nimport com.stratio.datasource.mongodb.writer._\nimport com.stratio.datasource.mongodb.config._\nimport com.stratio.datasource.mongodb.config.MongodbConfig._\nimport org.apache.spark.sql.SQLContext\nimport com.stratio.datasource.util.Config._\nimport scala.collection.mutable.WrappedArray\nspace: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,ArrayType(StringType,true),List(StringType))\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e final_master, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@62a30823\nmaster: org.apache.spark.sql.DataFrame \u003d [name: string, synonyms: string, _id: int, tag: string, size: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e pos_words, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@88f4ee04\npos: org.apache.spark.sql.DataFrame \u003d [x: string, _id: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e neg_words, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@caeb0bf\nneg: org.apache.spark.sql.DataFrame \u003d [x: string, _id: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e final_id_all, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@f747401f\nid_all: org.apache.spark.sql.DataFrame \u003d [name: string, _id: int]\nbuilder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder \u003d MongodbConfigBuilder(Map(database -\u003e EpiOne, writeConcern -\u003e normal, schema_samplingRatio -\u003e 1.0, collection -\u003e pii, host -\u003e List(one-mongo.epidemi.co)))\nreadConfig: com.stratio.datasource.util.Config \u003d com.stratio.datasource.util.ConfigBuilder$$anon$1@3f4bf2a5\npii: org.apache.spark.sql.DataFrame \u003d [phrase: string, _id: double]\nsymp: Array[org.apache.spark.sql.Row] \u003d Array([unevaluable event,feels like i m writing with my left hand in my right hand,12329,symptom,13], [hallucination,keep thinking i see people out of the corner of my eye,9843,symptom,12], [abnormal sleep,i fall asleep for roughly 3 hours and then stay awake,13031,symptom,11], [abdominal discomfort,makes me feel like i ve been kicked in the stomach,15576,symptom,11], [nail picking,fingernails i m sorry there s no skin left around you,16506,symptom,11], [vaginal bleeding,have a period for just a bout a solid year,7492,symptom,10], [pain in extremity,feet feel like i have been on them all day,8443,symptom,10], [nonspecific reaction,hate the feeling when you want it to be over,11330,symptom,10], [frequent urination,waking up in the middle of the nig...ue: Array[org.apache.spark.sql.Row] \u003d Array([ue: negative social perception,people look just as gross and stupid smoking e cig things as a real cigarette,17777,ue,15], [ue: negative social perception,dumb grls on the bus are tryna hide smokin an e cig,17777,ue,12], [ue: negative social perception,smoking an e cig in the library does not make you cool,17777,ue,12], [ue: negative social perception,if you smoke an e cig inside i assume you re asshole,17777,ue,12], [ue: negative social perception,if you re smoking an e cig i m judging you,17777,ue,11], [ue: health related testimonial,it has been a long time since i felt this good,17839,ue,11], [ue: high efficacy,it has been a long time since i felt this good,18228,ue,11], [ue: cessation related testimonial,i quit smoking cigarettes after 40...prod: Array[org.apache.spark.sql.Row] \u003d Array([drospirenone/ethinyl estradiol/levomefolate calcium tablets and levomefolate calcium,drospirenone ethinyl estradiol levomefolate calcium tablets and levomefolate calcium,2021,product,9], [6f angio-seal vascular closure device vip,6 f angio seal vascular closure device vip,1514,product,8], [centruroides (scorpion) immune f(ab)2(equine),centruroides scorpion immune f ab 2 equine,880,product,7], [6 shooter saeed multi-band ligator,6 shooter saeed multi band ligator,1510,product,6], [coated vicryl plus antibacterial (polyglactin 910),coated vicryl plus antibacterial polyglactin 910,1552,product,6], [octrode lead kit, 60cm length,octrode lead kit 60 cm length,1591,product,6], [hpv vaccine,vaccine against some kind of cancer,1919,product,6], [acc...comp: Array[org.apache.spark.sql.Row] \u003d Array([epidemico,wholly owned subsidiary booz allen hamilton,216,organization,6], [epidemico,spinoff boston childrens hospital,216,organization,4], [booz allen hamilton,booz allen,204,organization,2], [booz allen hamilton,booze allen,204,organization,2], [pwc,price waterhouse,212,organization,2], [epidemico,john brownstein,216,organization,2], [epidemico,nabarun dasgupta,216,organization,2], [epidemico,robin heffernan,216,organization,2], [epidemico,clark freifeld,216,organization,2], [accenture,accenture,207,organization,1], [deloitte,deloitte,208,organization,1], [raytheon,raytheon,209,organization,1], [ibm,ibm,210,organization,1], [ibm,ibmwatson,210,organization,1], [pwc,pricewaterhousecooper,212,organization,1], [pwc,pricewaterhousecoopers,212...top: Array[org.apache.spark.sql.Row] \u003d Array([cyber,crisis task force officer,8,business_category,4], [digital,inspector of the future,84,business_category,4], [digital,examiner of the future,84,business_category,4], [cyber,supplier security management,8,business_category,3], [cyber,continuous diagnostic mitigation,8,business_category,3], [cyber,intelligence driven operations,8,business_category,3], [cyber,anticipatory threat intelligance,8,business_category,3], [cyber,internet of things,8,business_category,3], [cyber,cyber supply chain,8,business_category,3], [data science,cell based security,54,business_category,3], [data science,semi structured data,54,business_category,3], [data science,data charity bowl,54,business_category,3], [digital,amazon web services,84,business_category,3], ...des: Array[org.apache.spark.sql.Row] \u003d Array([rocky mountain spotted fever,rickettsia tick borne spotted fever r africae r conorii r rickettsii and other,21943,disease,13], [rickettsia,rickettsia tick borne spotted fever tick typhus rmsf african tick bite fever,25777,disease,12], [colitis,inflammatory bowel disease new onset post travel crohns or ulcerative colitis,25949,disease,11], [amebiasis,amebas other e hartmani e nana e coli e polecki,18804,disease,10], [rickettsia,rickettsia tick borne spotted fever tick typhus rmsf atbf msf,25777,disease,10], [undiagnosed,cause of the illness had not yet been determined,23258,disease,9], [other human disease,skin soft tissue infection secondary bacterial of existing lesion,26307,disease,9], [diarrhea,diarrhea chronic responsive to anti parasiti...pi: Array[org.apache.spark.sql.Row] \u003d Array([ethan,6.0], [isabella,18.0], [olivia,19.0], [noah,23.0], [abigail,25.0], [sophia,26.0], [alexis,28.0], [hannah,29.0], [samantha,40.0], [jayden,41.0], [zachary,42.0], [elijah,43.0], [ava,44.0], [caleb,50.0], [alyssa,55.0], [aiden,59.0], [chloe,60.0], [natalie,64.0], [evan,66.0], [isaiah,68.0], [brianna,71.0], [gavin,72.0], [riley,73.0], [connor,76.0], [kayla,78.0], [hailey,82.0], [ella,85.0], [landon,87.0], [aidan,90.0], [jasmine,93.0], [liam,97.0], [avery,98.0], [addison,101.0], [lily,104.0], [nathaniel,108.0], [jeremiah,111.0], [hayden,112.0], [brayden,113.0], [katherine,114.0], [allison,116.0], [kaitlyn,119.0], [wyatt,120.0], [kaylee,121.0], [sebastian,124.0], [peyton,125.0], [megan,126.0], [alexandra,128.0], [lillian,130.0], [xavier,132.0]...poso: Array[org.apache.spark.sql.Row] \u003d Array([a plus,1], [abound,11], [abounds,12], [abundance,25], [abundant,26], [accessable,34], [accessible,35], [acclaim,37], [acclaimed,38], [acclamation,39], [accolade,40], [accolades,41], [accommodative,42], [accomodative,43], [accomplish,44], [accomplished,45], [accomplishment,46], [accomplishments,47], [accurate,49], [accurately,50], [achievable,65], [achievement,66], [achievements,67], [achievible,68], [acumen,76], [adaptable,79], [adaptive,80], [adequate,85], [adjustable,86], [admirable,87], [admirably,88], [admiration,89], [admire,90], [admirer,91], [admiring,92], [admiringly,93], [adorable,99], [adore,100], [adored,101], [adorer,102], [adoring,103], [adoringly,104], [adroit,105], [adroitly,106], [adulate,107], [adulation,108], [adulatory,10...nego: Array[org.apache.spark.sql.Row] \u003d Array([abnormal,2], [abolish,3], [abominable,4], [abominably,5], [abominate,6], [abomination,7], [abort,8], [aborted,9], [aborts,10], [abrade,13], [abrasive,14], [abrupt,15], [abruptly,16], [abscond,17], [absence,18], [absent minded,19], [absentee,20], [absurd,21], [absurdity,22], [absurdly,23], [absurdness,24], [abuse,27], [abused,28], [abuses,29], [abusive,30], [abysmal,31], [abysmally,32], [abyss,33], [accidental,36], [accost,48], [accursed,51], [accusation,52], [accusations,53], [accuse,54], [accuses,55], [accusing,56], [accusingly,57], [acerbate,58], [acerbic,59], [acerbically,60], [ache,61], [ached,62], [aches,63], [achey,64], [aching,69], [acrid,70], [acridly,71], [acridness,72], [acrimonious,73], [acrimoniously,74], [acrimony,75], [adamant...ids: Array[org.apache.spark.sql.Row] \u003d Array([cybersecurity,9], [cyber security,10], [cyber-security,11], [cybercrime,12], [cyber crime,13], [cyber espionage,14], [egovernment,15], [e-government,16], [cybercom,17], [cyber command,18], [cyber operations,19], [cyber control,20], [cyberspace,21], [threatbase,22], [threat intelligence,23], [vulnerability management,24], [mobile security,25], [information protection,26], [application security,27], [supplier security management,28], [postmorten analysis,29], [cyber attack,30], [cyberattack,31], [cyber-attack,32], [incidence response,33], [incident response,34], [continuous diagnostic mitigation,35], [continuous diagnostics,36], [intelligence driven operations,37], [predictive intelligence,38], [anticipatory threat intelligance,39], [cyber wor...p_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(32)\ns_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(33)\nu_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(34)\npi_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(35)\ncomp_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(36)\ntop_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(37)\ndes_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(38)\npos_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(39)\nneg_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(40)\nid_syn: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] \u003d Broadcast(41)\npa0: Map[String,String] \u003d Map()\nsa0: Map[String,String] \u003d Map()\nua0: Map[String,String] \u003d Map()\npia0: Map[String,String] \u003d Map()\nca0: Map[String,String] \u003d Map()\nta0: Map[String,String] \u003d Map()\ndas0: Map[String,String] \u003d Map()\ncas0: Map[String,String] \u003d Map()\nsas0: Map[String,String] \u003d Map()\npas0: Map[String,String] \u003d Map()\nnas0: Map[String,String] \u003d Map()\nias0: Map[String,String] \u003d Map()\npa: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(42)\nsa: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(43)\nua: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(44)\npia: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(45)\nca: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(46)\nta: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(47)\ndas: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(48)\ncas: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(49)\nsas: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(50)\npas: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(51)\nnas: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(52)\nias: org.apache.spark.broadcast.Broadcast[Map[String,String]] \u003d Broadcast(53)\nns: Array[Int] \u003d Array(15)\nnsize: Int \u003d 15\n"
      },
      "dateCreated": "Oct 19, 2016 10:33:11 PM",
      "dateStarted": "Oct 23, 2016 4:30:52 PM",
      "dateFinished": "Oct 23, 2016 4:31:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var forum \u003d sqlContext.jsonFile(\"/opt/sup/submissions\")\n.withColumn(\"created_utc\", from_unixtime($\"created_utc\".cast(\"String\")))\n.withColumn(\"ymds\", date_format($\"created_utc\", \"yyyyMMddss\").cast(\"Integer\"))\n.withColumn(\"ds\", when($\"ymds\"\u003d\u003d\u003d20160500, \"pushshift\").otherwise(\"pushshift\"))\n.withColumn(\"type\", when($\"ymds\"\u003d\u003d\u003d20160500, \"reddit\").otherwise(\"reddit\"))\n\nvar forum2  \u003d forum\nval n \u003d forum.columns\n\nif(n.contains(\"selftext\")){\n    \n    val make_flink \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else Array(\"https://www.reddit.com/r/\", text).mkString(\"\")})\n    \n    forum2 \u003d forum.withColumn(\"t2\", concat_ws(\" \", $\"selftext\", $\"url\")).withColumnRenamed(\"selftext\", \"t\").drop(\"url\").withColumn(\"url\", $\"permalink\").withColumnRenamed(\"name\", \"tid\").withColumn(\"tlink\", $\"url\").withColumnRenamed(\"uname\", \"author\").withColumn(\"dom\", when($\"ymds\"\u003d\u003d\u003d\"20150600\", \"reddit.com\").otherwise(\"reddit.com\")).withColumn(\"bname\", when($\"ymds\"\u003d\u003d\u003d\"20150600\", \"Reddit\").otherwise(\"Reddit\")).withColumn(\"cr\", $\"created_utc\".cast(\"timestamp\")).withColumn(\"rdt\",  from_unixtime($\"retrieved_on\").cast(\"timestamp\")).withColumn(\"flink\", make_flink($\"subreddit\")).withColumn(\"fname\", $\"subreddit\").withColumn(\"tstarter\", when($\"ymds\"\u003d\u003d\u003d20150600, \"T\").otherwise(\"T\")).withColumnRenamed(\"author\", \"uname\").filter(\"ymds is not null\").na.fill(0,Seq(\"num_comments\")).withColumn(\"downs\", $\"score\" - $\"ups\").select(\"type\",\"ds\",\"rdt\",\"ymds\",\"id\",\"bname\",\"cr\",\"dom\",\"downs\",\"flink\",\"fname\",\"num_comments\",\"score\",\"t\", \"t2\", \"tid\",\"title\",\"tlink\",\"tstarter\",\"uname\",\"ups\",\"url\")\nforum2 \u003d forum2.withColumn(\"t2\", regexp_replace(lower($\"t2\"), \"\"\"[\\p{Punct}]\"\"\", \" \"))\n} else {\nval make_flink \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else Array(\"https://www.reddit.com/r/\", text).mkString(\"\")\n})\nval link \u003d udf((text: String, id:String) \u003d\u003e {\n    if(text\u003d\u003dnull || id\u003d\u003dnull) null\n    else Array(\"https://www.reddit.com/comments/\", ((text.split(\"\").drop(4)).mkString(\"\")), \"/_/\", id).mkString(\"\")\n})\nval link2 \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else Array(\"https://www.reddit.com/comments/\", ((text.split(\"\").drop(4)).mkString(\"\"))).mkString(\"\")\n})\n    forum2 \u003d forum.withColumn(\"t\", $\"body\").\nwithColumn(\"url\", link($\"link_id\", $\"id\")).\nwithColumn(\"tid\", $\"parent_id\").\nwithColumn(\"tlink\", link2($\"link_id\")).\nwithColumn(\"cr\", $\"created_utc\".cast(\"timestamp\")).\nwithColumn(\"rdt\",  from_unixtime($\"retrieved_on\").cast(\"timestamp\")).\nwithColumnRenamed(\"author\", \"uname\").\nwithColumn(\"bname\", when($\"ymds\"\u003d\u003d\u003d\"20150600\", \"Reddit\").otherwise(\"Reddit\")).\nwithColumn(\"dom\", when($\"ymds\"\u003d\u003d\u003d\"20150600\", \"reddit.com\").otherwise(\"reddit.com\")).\nwithColumn(\"flink\", make_flink($\"subreddit\")).withColumn(\"fname\", $\"subreddit\").\nwithColumn(\"tstarter\", when($\"ymds\"\u003d\u003d\u003d20150600, \"F\").otherwise(\"F\")).\nwithColumn(\"downs\", $\"score\" - $\"ups\").\nfilter(\"ymds is not null\").\nselect(\"type\",\"ds\",\"rdt\",\"ymds\",\"id\",\"bname\",\"cr\",\"dom\",\"downs\",\"flink\",\"fname\",\"parent_id\",\"score\",\"t\",\"tid\",\"tlink\",\"tstarter\",\"uname\",\"ups\",\"url\")\nforum2 \u003d forum2.withColumn(\"t2\", regexp_replace(lower($\"t\"), \"\"\"[\\p{Punct}]\"\"\", \" \"))\n}\n\n",
      "dateUpdated": "Oct 23, 2016 4:30:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476916506497_653448889",
      "id": "20161019-223506_1513022743",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "warning: there were 1 deprecation warning(s); re-run with -deprecation for details\nforum: org.apache.spark.sql.DataFrame \u003d [_corrupt_record: string, adserver_click_url: string, adserver_imp_pixel: string, approved_by: string, archived: boolean, author: string, author_flair_css_class: string, author_flair_text: string, banned_by: string, clicked: boolean, created: bigint, created_utc: string, disable_comments: boolean, distinguished: string, domain: string, downs: bigint, edited: string, from: string, from_id: string, from_kind: string, gilded: bigint, hidden: boolean, hide_score: boolean, href_url: string, id: string, imp_pixel: string, is_self: boolean, likes: boolean, link_flair_css_class: string, link_flair_text: string, media: string, media_embed: struct\u003ccontent:string,height:bigint,scrolling:boolean,width:bigint\u003e, mobile_ad_url: string, mod_reports: array\u003cstring\u003e...forum2: org.apache.spark.sql.DataFrame \u003d [_corrupt_record: string, adserver_click_url: string, adserver_imp_pixel: string, approved_by: string, archived: boolean, author: string, author_flair_css_class: string, author_flair_text: string, banned_by: string, clicked: boolean, created: bigint, created_utc: string, disable_comments: boolean, distinguished: string, domain: string, downs: bigint, edited: string, from: string, from_id: string, from_kind: string, gilded: bigint, hidden: boolean, hide_score: boolean, href_url: string, id: string, imp_pixel: string, is_self: boolean, likes: boolean, link_flair_css_class: string, link_flair_text: string, media: string, media_embed: struct\u003ccontent:string,height:bigint,scrolling:boolean,width:bigint\u003e, mobile_ad_url: string, mod_reports: array\u003cstring...n: Array[String] \u003d Array(_corrupt_record, adserver_click_url, adserver_imp_pixel, approved_by, archived, author, author_flair_css_class, author_flair_text, banned_by, clicked, created, created_utc, disable_comments, distinguished, domain, downs, edited, from, from_id, from_kind, gilded, hidden, hide_score, href_url, id, imp_pixel, is_self, likes, link_flair_css_class, link_flair_text, media, media_embed, mobile_ad_url, mod_reports, name, num_comments, num_reports, over_18, permalink, post_hint, preview, promoted, quarantine, report_reasons, retrieved_on, saved, score, secure_media, secure_media_embed, selftext, selftext_html, stickied, subreddit, subreddit_id, third_party_tracking, third_party_tracking_2, thumbnail, title, ups, url, user_reports, ymds, ds, type)\n"
      },
      "dateCreated": "Oct 19, 2016 10:35:06 PM",
      "dateStarted": "Oct 23, 2016 4:30:54 PM",
      "dateFinished": "Oct 23, 2016 4:44:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.feature.NGram\nimport org.apache.spark.ml.feature.Tokenizer\nval tokenizer \u003d new Tokenizer().setInputCol(\"t2\").setOutputCol(\"words\")\nvar tokenized \u003d tokenizer.transform(forum2).withColumn(\"w2\", $\"words\")\nfor(i \u003c- 1 to nsize){\n    var num \u003d Array(\"ngram\", i).mkString\n    var ngi \u003d new NGram().setInputCol(\"words\").setOutputCol(num).setN(i)\n    tokenized \u003d ngi.transform(tokenized)\n    tokenized \u003d tokenized.withColumn(\"test\", tokenized(num).cast(\"String\")).withColumn(\"w2\", concat($\"w2\".cast(\"String\"), $\"test\")).drop(\"test\").drop(num)\n}",
      "dateUpdated": "Oct 23, 2016 4:30:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "tableHide": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476916618733_919880663",
      "id": "20161019-223658_646092307",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.NGram\nimport org.apache.spark.ml.feature.Tokenizer\ntokenizer: org.apache.spark.ml.feature.Tokenizer \u003d tok_5e84d09df1fb\ntokenized: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, t2: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string, words: array\u003cstring\u003e, w2: array\u003cstring\u003e]\n"
      },
      "dateCreated": "Oct 19, 2016 10:36:58 PM",
      "dateStarted": "Oct 23, 2016 4:31:33 PM",
      "dateFinished": "Oct 23, 2016 4:44:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val matcher \u003d udf((text: String) \u003d\u003e {\n    if(text\u003d\u003dnull) null\n    else {\n    val words \u003d text.split(\",\").toSet\n    val pc \u003d p_syn.value \u0026 words\n    val cc \u003d s_syn.value \u0026 words\n    val uc \u003d u_syn.value \u0026 words\n    val pic \u003d pi_syn.value \u0026 words\n    val coc \u003d comp_syn.value \u0026 words\n    val toc \u003d top_syn.value \u0026 words\n    val doc \u003d des_syn.value \u0026 words\n//    val joc \u003d cat_syn.value \u0026 words\n//    val soc \u003d spec_syn.value \u0026 words\n    val poc \u003d pos_syn.value \u0026 words\n    val noc \u003d neg_syn.value \u0026 words\n    Map(\"prod\" -\u003e (pc.toArray collect pa.value).distinct.map(_.toDouble),\n\"prod_syn\" -\u003e (pc.toArray collect ias.value).distinct.map(_.toDouble),\n \"con\" -\u003e (cc.toArray collect sa.value).distinct.map(_.toDouble),\n\"con_syn\" -\u003e (cc.toArray collect ias.value).distinct.map(_.toDouble),\n \"ue\" -\u003e (uc.toArray collect ua.value).distinct.map(_.toDouble),\n\"ue_syn\" -\u003e (uc.toArray collect ias.value).distinct.map(_.toDouble),\n\"pii\" -\u003e (pic.toArray collect pia.value).distinct.map(_.toDouble),\n\"company\" -\u003e (coc.toArray collect ca.value).distinct.map(_.toDouble),\n\"company_syn\" -\u003e (coc.toArray collect ias.value).distinct.map(_.toDouble),\n\"topic\" -\u003e (toc.toArray collect ta.value).distinct.map(_.toDouble),\n\"keys\" -\u003e (toc.toArray collect ias.value).distinct.map(_.toDouble),\n\"disease\" -\u003e (doc.toArray collect das.value).distinct.map(_.toDouble),\n\"disease_syn\" -\u003e (doc.toArray collect ias.value).distinct.map(_.toDouble), \n//\"category\" -\u003e (joc.toArray collect cas).distinct.map(_.toDouble),\n//\"category_syn\" -\u003e (joc.toArray collect ias).distinct.map(_.toDouble), \n//\"species\" -\u003e (soc.toArray collect sas).distinct.map(_.toDouble),\n//\"species_syn\" -\u003e (soc.toArray collect ias).distinct.map(_.toDouble),\n\"pos\" -\u003e (poc.toArray collect pas.value).map(_.toDouble), \n\"neg\" -\u003e (noc.toArray collect nas.value).map(_.toDouble))\n}})\nval to_double \u003d udf((text: WrappedArray[String]) \u003d\u003e {\n    if(text.length\u003d\u003d0) null\n    else text.map(_.toDouble)\n})",
      "dateUpdated": "Oct 23, 2016 4:30:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "tableHide": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476916627902_-1018099547",
      "id": "20161019-223707_14291314",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "matcher: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,MapType(StringType,ArrayType(DoubleType,false),true),List(StringType))\nto_double: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,ArrayType(DoubleType,false),List(ArrayType(StringType,true)))\n"
      },
      "dateCreated": "Oct 19, 2016 10:37:07 PM",
      "dateStarted": "Oct 23, 2016 4:44:25 PM",
      "dateFinished": "Oct 23, 2016 4:44:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val end0 \u003d tokenized.withColumn(\"temp\", matcher($\"w2\")).\nwithColumn(\"product\", $\"temp\"(\"prod\")).\nwithColumn(\"product_synonyms\", $\"temp\"(\"prod_syn\")).\nwithColumn(\"symptom\", $\"temp\"(\"con\")).\nwithColumn(\"symptom_synonyms\", $\"temp\"(\"con_syn\")).\nwithColumn(\"ue\", $\"temp\"(\"ue\")).\nwithColumn(\"ue_synonyms\", $\"temp\"(\"ue_syn\")).\nwithColumn(\"pii\", $\"temp\"(\"pii\")).\n//withColumn(\"pii_synonyms\", $\"temp\"(\"pii_syn\"))\nwithColumn(\"organization\", $\"temp\"(\"company\")).\nwithColumn(\"organization_synonyms\", $\"temp\"(\"company_syn\")).\nwithColumn(\"business_category\", $\"temp\"(\"topic\")).\nwithColumn(\"business_category_synonyms\", $\"temp\"(\"keys\")).\nwithColumn(\"disease\", $\"temp\"(\"disease\")).\nwithColumn(\"disease_synonyms\", $\"temp\"(\"disease_syn\")).\n//withColumn(\"category\", $\"temp\"(\"category\")).\n//withColumn(\"category_synonyms\", $\"temp\"(\"category_syn\")).\n//withColumn(\"species\", $\"temp\"(\"species\")).\n//withColumn(\"species_synonyms\", $\"temp\"(\"species_syn\")).\nwithColumn(\"pos\",  $\"temp\"(\"pos\")).\nwithColumn(\"neg\", $\"temp\"(\"neg\")).drop(\"w2\").drop(\"temp\").drop(\"words\")",
      "dateUpdated": "Oct 23, 2016 4:30:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476916654570_-1125029093",
      "id": "20161019-223734_790072647",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "end0: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, t2: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string, product: array\u003cdouble\u003e, product_synonyms: array\u003cdouble\u003e, symptom: array\u003cdouble\u003e, symptom_synonyms: array\u003cdouble\u003e, ue: array\u003cdouble\u003e, ue_synonyms: array\u003cdouble\u003e, pii: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, organization_synonyms: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, business_category_synonyms: array\u003cdouble\u003e, disease: array\u003cdouble\u003e, disease_synonyms: array\u003cdouble\u003e, pos: array\u003cdouble\u003e, neg: array\u003cdouble\u003e]\n"
      },
      "dateCreated": "Oct 19, 2016 10:37:34 PM",
      "dateStarted": "Oct 23, 2016 4:44:28 PM",
      "dateFinished": "Oct 23, 2016 4:44:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.PipelineModel\n\n\nval yolo \u003d PipelineModel.load(\"/opt/syed/AeClassifier.model\")\nval class_s \u003d udf((prediction: Double, ue: WrappedArray[Double]) \u003d\u003e {\n    if(ue \u003d\u003d null) 1.0\n  else if (ue.length \u003d\u003d 0) 1.0\n  else if (ue.length !\u003d0 \u0026\u0026 prediction \u003d\u003d 0.0D) 2.0\n  else if (ue.length !\u003d0 \u0026\u0026 prediction \u003d\u003d 1.0D) 7.0\n  else 1.0\n})\nval ind \u003d udf((ue: org.apache.spark.mllib.linalg.Vector) \u003d\u003e {\n    ue.toArray.max\n})\nval senti \u003d udf((sen: Int) \u003d\u003e {\n    if(sen \u003d\u003d0) 0\n    else if (sen\u003e0) 1\n    else -1\n})\nval end20 \u003d yolo.transform(end0).drop(\"rawPrediction\").drop(\"words\").drop(\"features\").drop(\"filtered\").withColumn(\"ind\", ind($\"probability\").cast(\"Double\")).drop(\"probability\").withColumn(\"tg\", class_s($\"prediction\", $\"ue\")).drop(\"t2\").withColumn(\"sentiment\", senti((size($\"pos\") - size($\"neg\")))).drop(\"pos\").drop(\"neg\").drop(\"prediction\")\n",
      "dateUpdated": "Oct 23, 2016 4:30:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476916541026_1535339341",
      "id": "20161019-223541_1392425551",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.PipelineModel\nyolo: org.apache.spark.ml.PipelineModel \u003d pipeline_e6ec2d152454\nclass_s: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction2\u003e,DoubleType,List(DoubleType, ArrayType(DoubleType,false)))\nind: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,DoubleType,List(org.apache.spark.mllib.linalg.VectorUDT@f71b0bce))\nsenti: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,IntegerType,List(IntegerType))\nend20: org.apache.spark.sql.DataFrame \u003d [type: string, ds: string, rdt: timestamp, ymds: int, id: string, bname: string, cr: timestamp, dom: string, downs: bigint, flink: string, fname: string, num_comments: bigint, score: bigint, t: string, tid: string, title: string, tlink: string, tstarter: string, uname: string, ups: bigint, url: string, product: array\u003cdouble\u003e, product_synonyms: array\u003cdouble\u003e, symptom: array\u003cdouble\u003e, symptom_synonyms: array\u003cdouble\u003e, ue: array\u003cdouble\u003e, ue_synonyms: array\u003cdouble\u003e, pii: array\u003cdouble\u003e, organization: array\u003cdouble\u003e, organization_synonyms: array\u003cdouble\u003e, business_category: array\u003cdouble\u003e, business_category_synonyms: array\u003cdouble\u003e, disease: array\u003cdouble\u003e, disease_synonyms: array\u003cdouble\u003e, ind: double, tg: double, sentiment: int]\n"
      },
      "dateCreated": "Oct 19, 2016 10:35:41 PM",
      "dateStarted": "Oct 23, 2016 4:44:29 PM",
      "dateFinished": "Oct 23, 2016 4:44:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "end20.count()",
      "dateUpdated": "Oct 23, 2016 4:30:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477157326271_-1365084345",
      "id": "20161022-172846_1030245043",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res18: Long \u003d 148798014\n"
      },
      "dateCreated": "Oct 22, 2016 5:28:46 PM",
      "dateStarted": "Oct 23, 2016 4:44:30 PM",
      "dateFinished": "Oct 23, 2016 4:46:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "end20.write.format(\"org.apache.spark.sql.cassandra\").mode(\"append\").options(Map( \"table\" -\u003e \"reddit\", \"keyspace\" -\u003e \"processed_forum\")).save()",
      "dateUpdated": "Oct 23, 2016 4:30:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1476919186945_316824774",
      "id": "20161019-231946_1157764431",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 55 in stage 50.0 failed 1 times, most recent failure: Lost task 55.0 in stage 50.0 (TID 12953, localhost): java.lang.NullPointerException\n\tat com.datastax.driver.core.Cluster$Manager.close(Cluster.java:1575)\n\tat com.datastax.driver.core.Cluster$Manager.access$200(Cluster.java:1283)\n\tat com.datastax.driver.core.Cluster.closeAsync(Cluster.java:554)\n\tat com.datastax.driver.core.Cluster.close(Cluster.java:566)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$3.apply(CassandraConnector.scala:148)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$3.apply(CassandraConnector.scala:148)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:31)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:56)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.writer.TableWriter.write(TableWriter.scala:134)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:37)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:67)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:85)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:181)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:186)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:188)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:190)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:192)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:194)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:196)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:198)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:200)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:202)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:204)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:206)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:208)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:210)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:212)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:214)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:216)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:218)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:220)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:222)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:224)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:226)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:228)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:230)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:232)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:234)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:236)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:238)\n\tat \u003cinit\u003e(\u003cconsole\u003e:240)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:244)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:810)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:753)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:746)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat com.datastax.driver.core.Cluster$Manager.close(Cluster.java:1575)\n\tat com.datastax.driver.core.Cluster$Manager.access$200(Cluster.java:1283)\n\tat com.datastax.driver.core.Cluster.closeAsync(Cluster.java:554)\n\tat com.datastax.driver.core.Cluster.close(Cluster.java:566)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:161)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$3.apply(CassandraConnector.scala:148)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$3.apply(CassandraConnector.scala:148)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:31)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:56)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n\tat com.datastax.spark.connector.writer.TableWriter.write(TableWriter.scala:134)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\t... 3 more\n\n"
      },
      "dateCreated": "Oct 19, 2016 11:19:46 PM",
      "dateStarted": "Oct 23, 2016 4:44:33 PM",
      "dateFinished": "Oct 23, 2016 4:46:51 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "end20.write.mode(\"append\").json(\"/opt/all_sub.json\")",
      "dateUpdated": "Oct 23, 2016 4:47:02 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477029562759_1667983833",
      "id": "20161021-055922_1040059033",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "error: \n     while compiling: \u003cconsole\u003e\n        during phase: jvm\n     library version: version 2.10.5\n    compiler version: version 2.10.5\n  reconstructed args: -classpath /opt/zeppelin/interpreter/spark/zeppelin-spark-0.6.0.jar:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.2.0.jar:/root/.ivy2/jars/com.stratio.datasource_spark-mongodb_2.10-0.11.1.jar:/root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.10-1.6.0.jar:/root/.ivy2/jars/org.apache.spark_spark-streaming-twitter_2.10-1.3.1.jar:/root/.ivy2/jars/RedisLabs_spark-redis-0.3.2.jar:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar:/root/.ivy2/jars/org.mongodb_casbah-commons_2.10-2.8.0.jar:/root/.ivy2/jars/org.mongodb_casbah-query_2.10-2.8.0.jar:/root/.ivy2/jars/org.mongodb_casbah-core_2.10-2.8.0.jar:/root/.ivy2/jars/com.github.nscala-time_nscala-time_2.10-1.0.0.jar:/root/.ivy2/jars/org.mongodb_mongo-java-driver-2.13.0.jar:/root/.ivy2/jars/joda-time_joda-time-2.3.jar:/root/.ivy2/jars/org.joda_joda-convert-1.2.jar:/root/.ivy2/jars/org.apache.cassandra_cassandra-clientutil-3.0.2.jar:/root/.ivy2/jars/com.datastax.cassandra_cassandra-driver-core-3.0.0.jar:/root/.ivy2/jars/org.apache.commons_commons-lang3-3.3.2.jar:/root/.ivy2/jars/com.google.guava_guava-16.0.1.jar:/root/.ivy2/jars/com.twitter_jsr166e-1.1.0.jar:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.10.5.jar:/root/.ivy2/jars/io.netty_netty-handler-4.0.33.Final.jar:/root/.ivy2/jars/io.dropwizard.metrics_metrics-core-3.1.2.jar:/root/.ivy2/jars/io.netty_netty-buffer-4.0.33.Final.jar:/root/.ivy2/jars/io.netty_netty-transport-4.0.33.Final.jar:/root/.ivy2/jars/io.netty_netty-codec-4.0.33.Final.jar:/root/.ivy2/jars/io.netty_netty-common-4.0.33.Final.jar:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar:/root/.ivy2/jars/org.twitter4j_twitter4j-stream-3.0.3.jar:/root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/root/.ivy2/jars/org.twitter4j_twitter4j-core-3.0.3.jar:/root/.ivy2/jars/redis.clients_jedis-2.7.2.jar:/root/.ivy2/jars/org.apache.commons_commons-pool2-2.3.jar:/opt/spark/guava-16.0.1.jar:/opt/spark/conf:/opt/spark/lib/spark-assembly-1.6.2-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.2.0.jar:/root/.ivy2/jars/com.stratio.datasource_spark-mongodb_2.10-0.11.1.jar:/root/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.10-1.6.0.jar:/root/.ivy2/jars/org.apache.spark_spark-streaming-twitter_2.10-1.3.1.jar:/root/.ivy2/jars/RedisLabs_spark-redis-0.3.2.jar:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar:/root/.ivy2/jars/org.mongodb_casbah-commons_2.10-2.8.0.jar:/root/.ivy2/jars/org.mongodb_casbah-query_2.10-2.8.0.jar:/root/.ivy2/jars/org.mongodb_casbah-core_2.10-2.8.0.jar:/root/.ivy2/jars/com.github.nscala-time_nscala-time_2.10-1.0.0.jar:/root/.ivy2/jars/org.mongodb_mongo-java-driver-2.13.0.jar:/root/.ivy2/jars/joda-time_joda-time-2.3.jar:/root/.ivy2/jars/org.joda_joda-convert-1.2.jar:/root/.ivy2/jars/org.apache.cassandra_cassandra-clientutil-3.0.2.jar:/root/.ivy2/jars/com.datastax.cassandra_cassandra-driver-core-3.0.0.jar:/root/.ivy2/jars/org.apache.commons_commons-lang3-3.3.2.jar:/root/.ivy2/jars/com.google.guava_guava-16.0.1.jar:/root/.ivy2/jars/com.twitter_jsr166e-1.1.0.jar:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.10.5.jar:/root/.ivy2/jars/io.netty_netty-handler-4.0.33.Final.jar:/root/.ivy2/jars/io.dropwizard.metrics_metrics-core-3.1.2.jar:/root/.ivy2/jars/io.netty_netty-buffer-4.0.33.Final.jar:/root/.ivy2/jars/io.netty_netty-transport-4.0.33.Final.jar:/root/.ivy2/jars/io.netty_netty-codec-4.0.33.Final.jar:/root/.ivy2/jars/io.netty_netty-common-4.0.33.Final.jar:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar:/root/.ivy2/jars/org.twitter4j_twitter4j-stream-3.0.3.jar:/root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/root/.ivy2/jars/org.twitter4j_twitter4j-core-3.0.3.jar:/root/.ivy2/jars/redis.clients_jedis-2.7.2.jar:/root/.ivy2/jars/org.apache.commons_commons-pool2-2.3.jar:/opt/zeppelin/interpreter/spark/zeppelin-spark-0.6.0.jar\n\n  last tree to typer: term value \n              symbol: variable value in object $eval (flags: \u003cmutable\u003e \u003ctriedcooking\u003e private[this])\n   symbol definition: private[this] var value: Throwable\n                 tpe: \u003cnotype\u003e\n       symbol owners: variable value -\u003e object $eval -\u003e package line377594537$188\n      context owners: object $eval -\u003e package line377594537$188\n\n\u003d\u003d Enclosing template or block \u003d\u003d\n\nTemplate( // val \u003clocal $eval\u003e: \u003cnotype\u003e in object $eval, tree.tpe\u003dtype\n  \"java.lang.Object\" // parents\n  ValDef(\n    private\n    \"_\"\n    \u003ctpt\u003e\n    \u003cempty\u003e\n  )\n  // 5 statements\n  ValDef( // private[this] var value: Throwable in object $eval\n    private \u003cmutable\u003e \u003clocal\u003e \u003cdefaultinit\u003e \u003ctriedcooking\u003e\n    \"value \"\n    \u003ctpt\u003e // tree.tpe\u003dThrowable\n    \u003cempty\u003e\n  )\n  DefDef( // def value(): Throwable in object $eval\n    \u003cmethod\u003e \u003caccessor\u003e \u003cdefaultinit\u003e \u003ctriedcooking\u003e\n    \"value\"\n    []\n    List(Nil)\n    \u003ctpt\u003e // tree.tpe\u003dThrowable\n    $eval.this.\"value \" // private[this] var value: Throwable in object $eval, tree.tpe\u003dThrowable\n  )\n  DefDef( // def value_\u003d(x$1: Throwable): Unit in object $eval\n    \u003cmethod\u003e \u003caccessor\u003e \u003cdefaultinit\u003e \u003ctriedcooking\u003e\n    \"value_$eq\"\n    []\n    // 1 parameter list\n    ValDef( // x$1: Throwable\n      \u003cparam\u003e \u003csynthetic\u003e \u003ctriedcooking\u003e\n      \"x$1\"\n      \u003ctpt\u003e // tree.tpe\u003dThrowable\n      \u003cempty\u003e\n    )\n    \u003ctpt\u003e // tree.tpe\u003dUnit\n    Assign( // tree.tpe\u003dUnit\n      $eval.this.\"value \" // private[this] var value: Throwable in object $eval, tree.tpe\u003dThrowable\n      \"x$1\" // x$1: Throwable, tree.tpe\u003dThrowable\n    )\n  )\n  DefDef( // def set(x: Object): Unit in object $eval\n    \u003cmethod\u003e\n    \"set\"\n    []\n    // 1 parameter list\n    ValDef( // x: Object\n      \u003cparam\u003e \u003ctriedcooking\u003e\n      \"x\"\n      \u003ctpt\u003e // tree.tpe\u003dObject\n      \u003cempty\u003e\n    )\n    \u003ctpt\u003e // tree.tpe\u003dUnit\n    Apply( // def value_\u003d(x$1: Throwable): Unit in object $eval, tree.tpe\u003dUnit\n      $eval.this.\"value_$eq\" // def value_\u003d(x$1: Throwable): Unit in object $eval, tree.tpe\u003d(x$1: Throwable)Unit\n      Apply( // final def $asInstanceOf[T0 \u003e: ? \u003c: ?](): T0 in class Object, tree.tpe\u003dThrowable\n        TypeApply( // final def $asInstanceOf[T0 \u003e: ? \u003c: ?](): T0 in class Object, tree.tpe\u003d()Throwable\n          \"x\".\"$asInstanceOf\" // final def $asInstanceOf[T0 \u003e: ? \u003c: ?](): T0 in class Object, tree.tpe\u003d[T0 \u003e: ? \u003c: ?]()T0\n          \u003ctpt\u003e // tree.tpe\u003dThrowable\n        )\n        Nil\n      )\n    )\n  )\n  DefDef( // def \u003cinit\u003e(): type in object $eval\n    \u003cmethod\u003e\n    \"\u003cinit\u003e\"\n    []\n    List(Nil)\n    \u003ctpt\u003e // tree.tpe\u003dtype\n    Block( // tree.tpe\u003dUnit\n      Apply( // def \u003cinit\u003e(): Object in class Object, tree.tpe\u003dObject\n        $eval.super.\"\u003cinit\u003e\" // def \u003cinit\u003e(): Object in class Object, tree.tpe\u003d()Object\n        Nil\n      )\n      ()\n    )\n  )\n)\n\n\u003d\u003d Expanded type of tree \u003d\u003d\n\n\u003cnotype\u003e\n\nuncaught exception during compilation: java.io.IOException\njava.io.FileNotFoundException: /tmp/spark-563fb061-4f25-4d36-938f-6a8500fb3eeb/$iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.class (Too many open files)\n\tat java.io.FileInputStream.open0(Native Method)\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\n\tat java.io.FileInputStream.\u003cinit\u003e(FileInputStream.java:138)\n\tat scala.reflect.io.File.inputStream(File.scala:97)\n\tat scala.reflect.io.PlainFile.input(PlainFile.scala:52)\n\tat scala.reflect.io.PlainFile.input(PlainFile.scala:34)\n\tat scala.reflect.io.AbstractFile.toByteArray(AbstractFile.scala:161)\n\tat scala.tools.nsc.interpreter.AbstractFileClassLoader.classBytes(AbstractFileClassLoader.scala:78)\n\tat scala.tools.nsc.interpreter.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:81)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:194)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:196)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:198)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:200)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:202)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:204)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:206)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:208)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:210)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:212)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:214)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:216)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:218)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:220)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:222)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:224)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:226)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:228)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:230)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:232)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:234)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:236)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:238)\n\tat \u003cinit\u003e(\u003cconsole\u003e:240)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:244)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:810)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:753)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:746)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Oct 21, 2016 5:59:22 AM",
      "dateStarted": "Oct 23, 2016 4:46:32 PM",
      "dateFinished": "Oct 23, 2016 4:46:51 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "end20.show()",
      "dateUpdated": "Oct 23, 2016 4:30:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477204345407_430941921",
      "id": "20161023-063225_857440822",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+------+---------+----+----------+-----+------+--------------------+----------+-----+--------------------+------------------+------------+-----+--------------------+--------+--------------------+--------------------+--------+----------------+----+--------------------+-------+----------------+--------------------+--------------------+--------------------+--------------------+------------------+------------+---------------------+-----------------+--------------------------+---------+----------------+------------------+---+---------+\n|  type|       ds| rdt|      ymds|   id| bname|                  cr|       dom|downs|               flink|             fname|num_comments|score|                   t|     tid|               title|               tlink|tstarter|           uname| ups|                 url|product|product_synonyms|             symptom|    symptom_synonyms|                  ue|         ue_synonyms|               pii|organization|organization_synonyms|business_category|business_category_synonyms|  disease|disease_synonyms|               ind| tg|sentiment|\n+------+---------+----+----------+-----+------+--------------------+----------+-----+--------------------+------------------+------------+-----+--------------------+--------+--------------------+--------------------+--------+----------------+----+--------------------+-------+----------------+--------------------+--------------------+--------------------+--------------------+------------------+------------+---------------------+-----------------+--------------------------+---------+----------------+------------------+---+---------+\n|reddit|pushshift|null|2011080152|j66ac|Reddit|2011-08-01 23:59:...|reddit.com|   -6|https://www.reddi...|             trees|           1|    3|                    |t3_j66ac|Your darn right w...|/r/trees/comments...|       T|          Gruzzy|   9|/r/trees/comments...|     []|              []|                  []|                  []|                  []|                  []|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|1.0|        0|\n|reddit|pushshift|null|2011080150|j66ab|Reddit|2011-08-01 23:59:...|reddit.com|   -1|https://www.reddi...|            gaming|           3|    0|                    |t3_j66ab|Did anyone else l...|/r/gaming/comment...|       T|       [deleted]|   1|/r/gaming/comment...|     []|              []|                  []|                  []|                  []|                  []|         [35180.0]|          []|                   []|               []|                        []|[25312.0]|       [25318.0]|0.5547410546297091|1.0|        1|\n|reddit|pushshift|null|2011080150|j66aa|Reddit|2011-08-01 23:59:...|reddit.com|    0|https://www.reddi...|           running|          11|    2|    Any suggestions?|t3_j66aa|Good pair of shoe...|/r/running/commen...|       T|     jakegarland|   2|/r/running/commen...|     []|              []|                  []|                  []|           [16550.0]|           [16553.0]|                []|          []|                   []|               []|                        []|       []|              []|0.6073676384684997|2.0|        1|\n|reddit|pushshift|null|2011080147|j66a9|Reddit|2011-08-01 23:59:...|reddit.com|   -9|https://www.reddi...|firstworldproblems|           0|    7|                    |t3_j66a9|I don\u0027t have a mi...|/r/firstworldprob...|       T|       [deleted]|  16|/r/firstworldprob...|     []|              []|                  []|                  []|  [16550.0, 17700.0]|  [16553.0, 17701.0]|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|2.0|        0|\n|reddit|pushshift|null|2011080144|j66a8|Reddit|2011-08-01 23:59:...|reddit.com|   -1|https://www.reddi...|         AskReddit|           5|    0|(Driving a car, o...|t3_j66a8|Do you remember y...|/r/AskReddit/comm...|       T|dizzyfingerz3525|   1|/r/AskReddit/comm...|     []|              []|            [8006.0]|            [8016.0]|[17769.0, 16550.0...|[17775.0, 16553.0...|                []|          []|                   []|               []|                        []|       []|              []|0.5688496361298722|2.0|        0|\n|reddit|pushshift|null|2011080144|j66a7|Reddit|2011-08-01 23:59:...|reddit.com|    0|https://www.reddi...|     todayilearned|           1|    1|                    |t3_j66a7|TIL that in 2008 ...|/r/todayilearned/...|       T|       [deleted]|   1|/r/todayilearned/...|     []|              []|                  []|                  []|  [18228.0, 18420.0]|  [18243.0, 18443.0]|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|2.0|        0|\n|reddit|pushshift|null|2011080143|j66a6|Reddit|2011-08-01 23:59:...|reddit.com|    0|https://www.reddi...|         newjersey|           1|    3|I heard about the...|t3_j66a6|Sting Rays at the...|/r/newjersey/comm...|       T|        noooonan|   3|/r/newjersey/comm...|     []|              []|            [7095.0]|            [7109.0]|           [16550.0]|           [16553.0]|[34913.0, 34772.0]|          []|                   []|               []|                        []|       []|              []|0.7544583698784753|2.0|       -1|\n|reddit|pushshift|null|2011080142|j66a5|Reddit|2011-08-01 23:59:...|reddit.com|   -3|https://www.reddi...|        reddit.com|           4|    0|What do you think...|t3_j66a5|8 STEPS TO A KILL...|/r/reddit.com/com...|       T|    technomensch|   3|/r/reddit.com/com...|     []|              []|                  []|                  []|  [16946.0, 16550.0]|[16947.0, 16553.0...|                []|          []|                   []|               []|                        []|       []|              []|0.7805402086248134|7.0|       -1|\n|reddit|pushshift|null|2011080142|j66a4|Reddit|2011-08-01 23:59:...|reddit.com|-2633|https://www.reddi...|             funny|          79|  480|                    |t3_j66a4|We are twins. We ...|/r/funny/comments...|       T|        nomdeweb|3113|/r/funny/comments...|     []|              []|                  []|                  []|                  []|                  []|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|1.0|        0|\n|reddit|pushshift|null|2011080140|j66a3|Reddit|2011-08-01 23:59:...|reddit.com|   -6|https://www.reddi...|         Minecraft|           4|    2|                    |t3_j66a3|pistons cant push...|/r/Minecraft/comm...|       T|           xevoc|   8|/r/Minecraft/comm...|     []|              []|                  []|                  []|                  []|                  []|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|1.0|        0|\n|reddit|pushshift|null|2011080139|j66a2|Reddit|2011-08-01 23:59:...|reddit.com|    0|https://www.reddi...|      SFBayHousing|           0|    1|I\u0027m having a hell...|t3_j66a2|[Seeking] Help wi...|/r/SFBayHousing/c...|       T|         ososoul|   1|/r/SFBayHousing/c...|     []|              []|                  []|                  []|[18187.0, 17043.0...|[18193.0, 17050.0...|                []|          []|                   []|               []|                        []|       []|              []|0.8951902411762495|7.0|        1|\n|reddit|pushshift|null|2011080138|j66a1|Reddit|2011-08-01 23:59:...|reddit.com|   -1|https://www.reddi...|              nsfw|           4|    0|                    |t3_j66a1|Who is the lady i...|/r/nsfw/comments/...|       T|       [deleted]|   1|/r/nsfw/comments/...|     []|              []|                  []|                  []|                  []|                  []|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|1.0|        0|\n|reddit|pushshift|null|2011080137|j66a0|Reddit|2011-08-01 23:59:...|reddit.com|   -2|https://www.reddi...|               aww|           0|    3|                    |t3_j66a0|what a good surro...|/r/aww/comments/j...|       T|       [deleted]|   5|/r/aww/comments/j...|     []|              []|                  []|                  []|                  []|                  []|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|1.0|        0|\n|reddit|pushshift|null|2011080137|j669z|Reddit|2011-08-01 23:59:...|reddit.com|   -1|https://www.reddi...|firstworldproblems|           0|    1|                    |t3_j669z|           Leftovers|/r/firstworldprob...|       T|     peteyboy100|   2|/r/firstworldprob...|     []|              []|                  []|                  []|           [16550.0]|           [16553.0]|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|2.0|        0|\n|reddit|pushshift|null|2011080137|j669y|Reddit|2011-08-01 23:59:...|reddit.com|   -2|https://www.reddi...|           atheism|          19|   12|The family dog di...|t3_j669y|My mother continu...|/r/atheism/commen...|       T|    TheBlackHive|  14|/r/atheism/commen...|     []|              []|[10136.0, 7937.0,...|[18340.0, 7985.0,...|[17708.0, 18332.0...|[17720.0, 18340.0...|         [34802.0]|          []|                   []|               []|                        []|       []|              []|0.5864222360311944|7.0|       -1|\n|reddit|pushshift|null|2011080137|j669x|Reddit|2011-08-01 23:59:...|reddit.com|   -6|https://www.reddi...|               WTF|           0|    0|                    |t3_j669x|Naked New Yorkers...|/r/WTF/comments/j...|       T|    misterthingy|   6|/r/WTF/comments/j...|     []|              []|           [10945.0]|           [10946.0]|                  []|                  []|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|1.0|        0|\n|reddit|pushshift|null|2011080134|j669w|Reddit|2011-08-01 23:59:...|reddit.com|    0|https://www.reddi...|        reddit.com|           0|    1|                    |t3_j669w|How to take care ...|/r/reddit.com/com...|       T|         DLew978|   1|/r/reddit.com/com...|     []|              []|                  []|                  []|                  []|                  []|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|1.0|        0|\n|reddit|pushshift|null|2011080131|j669v|Reddit|2011-08-01 23:59:...|reddit.com|  -24|https://www.reddi...|   TwoXChromosomes|          41|   39|     Just wondering.|t3_j669v|Is it just me or ...|/r/TwoXChromosome...|       T|  Palinsupporter|  63|/r/TwoXChromosome...|     []|              []|                  []|                  []|[17402.0, 16550.0...|[17411.0, 16553.0...|                []|          []|                   []|               []|                        []|       []|              []|0.6051103463037246|2.0|        0|\n|reddit|pushshift|null|2011080131|j669u|Reddit|2011-08-01 23:59:...|reddit.com|   -2|https://www.reddi...|        circlejerk|           1|    1|                    |t3_j669u|Reddit, would you...|/r/circlejerk/com...|       T|           LCON1|   3|/r/circlejerk/com...|     []|              []|                  []|                  []|           [16550.0]|           [16553.0]|                []|          []|                   []|               []|                        []|       []|              []|0.5547410546297091|2.0|        0|\n|reddit|pushshift|null|2011080129|j669t|Reddit|2011-08-01 23:59:...|reddit.com|    0|https://www.reddi...|         AskReddit|           8|    2|I have always bee...|t3_j669t|Best way of getti...|/r/AskReddit/comm...|       T|    Mustkunstn1k|   2|/r/AskReddit/comm...|     []|              []|           [15343.0]|           [15345.0]|[16809.0, 17402.0...|[16812.0, 17407.0...|         [34911.0]|          []|                   []|               []|                        []|       []|              []|0.9396675321270388|7.0|       -1|\n+------+---------+----+----------+-----+------+--------------------+----------+-----+--------------------+------------------+------------+-----+--------------------+--------+--------------------+--------------------+--------+----------------+----+--------------------+-------+----------------+--------------------+--------------------+--------------------+--------------------+------------------+------------+---------------------+-----------------+--------------------------+---------+----------------+------------------+---+---------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Oct 23, 2016 6:32:25 AM",
      "dateStarted": "Oct 23, 2016 4:46:51 PM",
      "dateFinished": "Oct 23, 2016 4:46:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Oct 23, 2016 4:30:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477205151623_1913900979",
      "id": "20161023-064551_522727546",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 23, 2016 6:45:51 AM",
      "dateStarted": "Oct 23, 2016 4:46:52 PM",
      "dateFinished": "Oct 23, 2016 4:46:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark_submit_reddit",
  "id": "2BZNF8DV5",
  "angularObjects": {
    "2BUZX9EWW:shared_process": [],
    "2BRP3ZUWJ:shared_process": [],
    "2BTJ3P41C:shared_process": [],
    "2BSJYK2YE:shared_process": [],
    "2BU87RU3U:shared_process": [],
    "2BTB82RPQ:shared_process": [],
    "2BSSEDUXN:shared_process": [],
    "2BRZ896X6:shared_process": [],
    "2BT3JK3T4:shared_process": [],
    "2BTXGDVEJ:shared_process": [],
    "2BUSJ5B5T:shared_process": [],
    "2BT211CDH:shared_process": [],
    "2BTX1MQS6:shared_process": [],
    "2BUCYP1D2:shared_process": [],
    "2BUTB2HA6:shared_process": [],
    "2BRYFEHJ7:shared_process": [],
    "2BSMJA8VG:shared_process": []
  },
  "config": {},
  "info": {}
}