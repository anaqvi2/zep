{
  "paragraphs": [
    {
      "text": "%pyspark\n\n\"\"\"\nImports and versioning.\n\nUpdate processor_version as necessary\n\"\"\"\n\nimport os\nimport argparse\nimport os\nimport logging\nimport logging.config\nimport traceback\nimport nltk\nimport json\nimport re\nfrom collections import OrderedDict\nfrom nltk.downloader import download \n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.storagelevel import StorageLevel\nfrom pyspark.sql.functions import lit, lower\n\nprocessor_name \u003d \u0027place_list\u0027\nprocessor_version \u003d \u0027v0.0.1\u0027",
      "dateUpdated": "Oct 20, 2016 1:33:02 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474390415222_-1427388267",
      "id": "20160915-135531_953039949",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 20, 2016 4:53:35 PM",
      "dateStarted": "Oct 20, 2016 1:33:02 PM",
      "dateFinished": "Oct 20, 2016 1:33:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/**\n * Create version columns if necessary in Scylla/Cassandra. \n * Written in Scala because pyspark drivers seem to be missing?\n * Make sure to update processor_version as necessary\n **/\n\nimport java.util.Calendar\nimport java.text.SimpleDateFormat\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.SparkContext\n\nval processor_name \u003d \"place_list\"\nvar processor_version \u003d \"v0.0.1\"\n\nval format \u003d new SimpleDateFormat(\"yw\")\nval calendar \u003d Calendar.getInstance()\ncalendar.add(Calendar.DAY_OF_MONTH, -7)\nval yrweek \u003d format.format(calendar.getTime())\n\nprocessor_version \u003d processor_version.replace(\".\", \"_\")\n\nval v \u003d  s\"${processor_name}_v\"\nval pvd \u003d s\"${processor_name}__${processor_version}__${yrweek}\"\nval pvd \u003d \"place_list__v0_0_1__201639\"\n\ndef writeVersionColumn(sc:SparkContext, column: String, coltype: String, table: String, keyspace: String) \u003d\n{\n\n val cassTable \u003d sc.cassandraTable(keyspace, table)\n val cassSchema \u003d cassTable.selectedColumnNames.toSet\n \n if(!cassSchema.contains(column)) {\n      val cassTable \u003d sc.cassandraTable(keyspace, table)\n      cassTable.connector.withSessionDo {\n            session \u003d\u003e {\n              session.execute(s\"ALTER TABLE $keyspace.$table ADD $column $coltype\")\n    } } }\n }\n\nwriteVersionColumn(sc, v, \"text\", \"forum\", \"processed_forum\")\nwriteVersionColumn(sc, pvd, \"frozen\u003cset\u003cfrozen\u003cplace\u003e\u003e\u003e\", \"forum\", \"processed_forum\")\n//writeVersionColumn(sc, v, \"text\", \"agg_table\", \"testing\")\n",
      "dateUpdated": "Oct 20, 2016 12:36:22 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474565111573_1046488057",
      "id": "20160922-172511_112230612",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.util.Calendar\nimport java.text.SimpleDateFormat\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql._\nimport org.apache.spark.SparkContext\nprocessor_name: String \u003d place_list\nprocessor_version: String \u003d v0.0.1\nformat: java.text.SimpleDateFormat \u003d java.text.SimpleDateFormat@f1e\ncalendar: java.util.Calendar \u003d java.util.GregorianCalendar[time\u003d1475627533432,areFieldsSet\u003dtrue,areAllFieldsSet\u003dtrue,lenient\u003dtrue,zone\u003dsun.util.calendar.ZoneInfo[id\u003d\"Etc/UTC\",offset\u003d0,dstSavings\u003d0,useDaylight\u003dfalse,transitions\u003d0,lastRule\u003dnull],firstDayOfWeek\u003d1,minimalDaysInFirstWeek\u003d1,ERA\u003d1,YEAR\u003d2016,MONTH\u003d9,WEEK_OF_YEAR\u003d41,WEEK_OF_MONTH\u003d2,DAY_OF_MONTH\u003d5,DAY_OF_YEAR\u003d279,DAY_OF_WEEK\u003d4,DAY_OF_WEEK_IN_MONTH\u003d1,AM_PM\u003d0,HOUR\u003d0,HOUR_OF_DAY\u003d0,MINUTE\u003d32,SECOND\u003d13,MILLISECOND\u003d432,ZONE_OFFSET\u003d0,DST_OFFSET\u003d0]\nyrweek: String \u003d 201640\nprocessor_version: String \u003d v0_0_1\nv: String \u003d place_list_v\npvd: String \u003d place_list__v0_0_1__201640\npvd: String \u003d place_list__v0_0_1__201639\nwriteVersionColumn: (sc: org.apache.spark.SparkContext, column: String, coltype: String, table: String, keyspace: String)Any\nres53: Any \u003d ()\nres54: Any \u003d ()\n"
      },
      "dateCreated": "Sep 22, 2016 5:25:11 PM",
      "dateStarted": "Oct 5, 2016 12:32:11 AM",
      "dateFinished": "Oct 5, 2016 12:32:25 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.functions import lit, lower\n\"\"\"\nLoad geonames from ScyllaDB (TODO: Load from csv? Load from redis?)\n\"\"\"\ngeocode_rdd \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"geonames\", keyspace\u003d\"geonames\").load()\ngeocode_rdd_lower \u003d geocode_rdd.withColumn(\"name\", lower(geocode_rdd[\"name\"])).persist(StorageLevel.MEMORY_AND_DISK_SER)\nprint geocode_rdd_lower.count()\n\ngeocode_pdd \u003d  geocode_rdd_lower.toPandas()\n#geocode_pdd \u003d geocode_pdd[geocode_pdd[\"source\"] \u003d\u003d \"name\"]\n\n# geocode_pdd[\u0027name\u0027] \u003d geocode_pdd[\u0027name\u0027].str.lower()\n# sqlContext.registerDataFrameAsTable(geocode_rdd, \"geonames\")",
      "dateUpdated": "Oct 20, 2016 1:33:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474555193142_1102932969",
      "id": "20160922-143953_1778987938",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "713991\n"
      },
      "dateCreated": "Sep 22, 2016 2:39:53 PM",
      "dateStarted": "Oct 20, 2016 1:33:07 PM",
      "dateFinished": "Oct 20, 2016 1:33:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ngeocode_rdd_lower.filter(geocode_rdd_lower[\"name\"] \u003d\u003d \u0027bangor\u0027).show()",
      "dateUpdated": "Oct 5, 2016 3:43:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475638437949_-264103188",
      "id": "20161005-033357_440133324",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---------+------+-----------+-----------+-----------+-----------+-----------+---+------------+-------------+------------+-----------+--------+---------+-----------------+----------+------+----------------+\n|geonameid|  name|admin1_abbr|admin1_code|admin2_code|admin3_code|admin4_code|cc2|country_code|feature_class|feature_code|isolanguage|latitude|longitude|modification_date|population|source|        timezone|\n+---------+------+-----------+-----------+-----------+-----------+-----------+---+------------+-------------+------------+-----------+--------+---------+-----------------+----------+------+----------------+\n|  4984863|bangor|         MI|         MI|      159.0|           |           |   |          US|            P|         PPL|      ascii|42.31254|-86.11308|       2011-05-14|    1885.0| ascii| America/Detroit|\n|  5178996|bangor|         PA|         PA|       95.0|           |           |   |          US|            P|         PPL|      ascii|40.86565|-75.20657|       2011-05-14|    5273.0| ascii|America/New_York|\n|  2656396|bangor|           |        NIR|         T2|           |           |   |          GB|            P|       PPLA2|         sv|54.65338| -5.66895|       2010-10-16|   60385.0|  name|   Europe/London|\n|  5244626|bangor|         WI|         WI|       63.0|           |           |   |          US|            P|         PPL|      ascii|43.89302|-90.99041|       2011-05-14|    1459.0| ascii| America/Chicago|\n|  4957280|bangor|         ME|         ME|       19.0|           |           |   |          US|            P|       PPLA2|      ascii|44.80118|-68.77781|       2011-05-14|   33039.0| ascii|America/New_York|\n|  2656397|bangor|           |        WLS|         Y2|    00NC006|           |   |          GB|            P|         PPL|         en|53.22752| -4.12936|       2016-03-08|   15449.0|  name|   Europe/London|\n+---------+------+-----------+-----------+-----------+-----------+-----------+---+------------+-------------+------------+-----------+--------+---------+-----------------+----------+------+----------------+\n\n"
      },
      "dateCreated": "Oct 5, 2016 3:33:57 AM",
      "dateStarted": "Oct 5, 2016 3:43:50 AM",
      "dateFinished": "Oct 5, 2016 3:43:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nprint geocode_pdd.head(5)",
      "dateUpdated": "Sep 29, 2016 11:21:29 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474980249763_-778407248",
      "id": "20160927-124409_261582650",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "   geonameid                  name admin1_abbr admin1_code admin2_code  \\\n0    6436727            malzeville                    44.0          54   \n1    6540242  comune di costermano                    20.0          VR   \n2    6540242            costermano                    20.0          VR   \n3     263219                 delfi                  ESYE24        32.0   \n4     263219                delfoi                  ESYE24        32.0   \n\n  admin3_code admin4_code cc2 country_code feature_class feature_code  \\\n0         543       54339               FR             A         ADM4   \n1     23030.0                           IT             A         ADM3   \n2     23030.0                           IT             A         ADM3   \n3        9165                           GR             P          PPL   \n4        9165                           GR             P          PPL   \n\n  isolanguage   latitude  longitude modification_date  population     source  \\\n0       ascii  48.710281    6.18639        2016-02-21      8118.0      ascii   \n1          it  45.586891   10.73718        2014-01-02      3586.0  alternate   \n2          it  45.586891   10.73718        2014-01-02      3586.0       name   \n3          sr  38.479420   22.49357        2015-11-08      2373.0  alternate   \n4          fi  38.479420   22.49357        2015-11-08      2373.0  alternate   \n\n        timezone  \n0   Europe/Paris  \n1    Europe/Rome  \n2    Europe/Rome  \n3  Europe/Athens  \n4  Europe/Athens  \n"
      },
      "dateCreated": "Sep 27, 2016 12:44:09 PM",
      "dateStarted": "Sep 29, 2016 11:21:29 AM",
      "dateFinished": "Sep 29, 2016 11:21:29 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nGlobal variables\n\"\"\"\n\nSCRIPT_DIR \u003d os.path.dirname(os.path.realpath(__file__))\nCODE_REGEX \u003d r\u0027(?:\\s|^|!|,|\\()([A-Z]{2,3})(?:\\s|$|!|,|\\))\u0027\nDELETE_REGEX \u003d r\u0027\\\\n|\\\\t|\\\u003e|\\\u003c|\\(|\\)|\\\u003e|\\\u003c\u0027\n\n# Words that should not be considered placenames\nIGNORE_WORDS \u003d [\u0027a\u0027,\u0027ai\u0027,\u0027i\u0027,\u0027of\u0027,\u0027the\u0027\u0027many\u0027,\u0027may\u0027,\u0027march\u0027,\u0027center\u0027,\u0027as\u0027,\u0027see\u0027,\u0027valley\u0027,\u0027university\u0027,\u0027about\u0027,\u0027new\u0027,\u0027sars\u0027,\u0027aids\u0027,\u0027hpv\u0027,\u0027adhd\u0027,\u0027newcastle\u0027,\u0027elisa\u0027,\u0027to\u0027,\u0027influenza\u0027,\u0027who\u0027,\u0027pro\u0027,\u0027os\u0027,\u0027and\u0027,\u0027pdt\u0027,\u0027in\u0027,\u0027flu\u0027,\u0027ha\u0027,\u0027ron\u0027,\u0027control\u0027,\u0027mod\u0027,\u0027northern\u0027,\u0027southern\u0027,\u0027eastern\u0027,\u0027western\u0027,\u0027all\u0027,\u0027am\u0027,\u0027pm\u0027,\u0027eua\u0027,\u0027avian\u0027,\u0027ah\u0027,\u0027human\u0027,\u0027date\u0027,\u0027health\u0027,\u0027fry\u0027,\u0027many\u0027,\u0027is\u0027,\u0027the\u0027,\u0027by\u0027,\u0027on\u0027,\u0027ap\u0027,\u0027er\u0027,\u0027church\u0027,\n]\n",
      "dateUpdated": "Oct 20, 2016 1:33:37 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474505429104_2000259038",
      "id": "20160922-005029_1129076521",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 12:50:29 AM",
      "dateStarted": "Oct 20, 2016 1:33:38 PM",
      "dateFinished": "Oct 20, 2016 1:33:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nStanford Tagger Functions\nTODO: Need jar loaded in pyspark, training data jar available in path\nNot currently being used\n\"\"\"\n\ndef get_stanford_tagger():\n    \"\"\"\n    Use the superior Stanford tagger\n    \"\"\"\n    training_db \u003d os.path.join(\n        SCRIPT_DIR,\n        \u0027resources/classifiers/english.conll.4class.distsim.crf.ser.gz\u0027)\n    jar_file \u003d os.path.join(SCRIPT_DIR, \u0027resources/stanford-ner.jar\u0027)\n    tagger \u003d nltk.StanfordNERTagger(training_db, path_to_jar\u003djar_file)\n    return tagger\n\ndef get_stanford_gpes(text):\n    \"\"\"\n    Return a list of possible names based on Stanford NLTK chunking\n    \"\"\"\n    tokens \u003d [x.strip(\u0027.\u0027) for x in nltk.word_tokenize(text)]\n    tagger \u003d get_stanford_tagger()\n    stagged_text \u003d tagger.tag(tokens)\n    ne_tree \u003d stanford2tree(stagged_text)\n    ne_in_sent \u003d []\n    for subtree in ne_tree:\n        if type(\n                subtree) \u003d\u003d nltk.Tree:  # If subtree is a noun chunk, i.e. NE !\u003d \"O\"\n            ne_label \u003d subtree.label()\n            ne_string \u003d \" \".join([token for token, pos in subtree.leaves()])\n            if ne_label in [\u0027ORGANIZATION\u0027, \u0027LOCATION\u0027]:\n                ne_in_sent.append(ne_string)\n    return ne_in_sent\n\n\ndef stanfordNE2BIO(tagged_sent):\n    bio_tagged_sent \u003d []\n    prev_tag \u003d \"O\"\n    for token, tag in tagged_sent:\n        if tag \u003d\u003d \"O\":  # O\n            prev_tag \u003d tag\n            continue\n        if tag !\u003d \"O\" and prev_tag \u003d\u003d \"O\":  # Begin NE\n            bio_tagged_sent.append((token, \"B-\" + tag))\n            prev_tag \u003d tag\n        elif prev_tag !\u003d \"O\" and prev_tag \u003d\u003d tag:  # Inside NE\n            bio_tagged_sent.append((token, \"I-\" + tag))\n            prev_tag \u003d tag\n        elif prev_tag !\u003d \"O\" and prev_tag !\u003d tag:  # Adjacent NE\n            bio_tagged_sent.append((token, \"B-\" + tag))\n            prev_tag \u003d tag\n\n    return bio_tagged_sent\n\n\ndef stanford2tree(ne_tagged_sent):\n    bio_tagged_sent \u003d stanfordNE2BIO(ne_tagged_sent)\n    bio_tagged_filtered \u003d [x for x in bio_tagged_sent if x[0]]\n    if bio_tagged_filtered:\n        sent_tokens, sent_ne_tags \u003d zip(*bio_tagged_filtered)\n        pos_tokens \u003d nltk.pos_tag(sent_tokens)\n        sent_pos_tags \u003d [pos for token, pos in pos_tokens]\n        sent_conlltags \u003d [(token, pos, ne) for token, pos, ne in\n                          zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n        ne_tree \u003d nltk.conlltags2tree(sent_conlltags)\n        return ne_tree\n    else:\n        return []\n        ",
      "dateUpdated": "Oct 20, 2016 1:33:42 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474390415223_-1427773016",
      "id": "20160915-132118_1500802402",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 20, 2016 4:53:35 PM",
      "dateStarted": "Oct 20, 2016 1:33:42 PM",
      "dateFinished": "Oct 20, 2016 1:33:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nAlternate NLTK tag methods\nCurrently being used but not as good as Stanford tagger\n\"\"\"\n\nnospace_langs \u003d (\u0027zh\u0027,)\n\ndef get_nltk_gpes(text):\n    \"\"\"\n    Use standard NLTK tagger (sucks)\n    \"\"\"\n    try:\n        sents \u003d nltk.sent_tokenize(re.sub(\u0027[\\:\\\\n]\u0027,\u0027.\u0027,text))\n    except LookupError:\n        print(\u0027Download nltk data\u0027)\n        download([\u0027punkt\u0027,\u0027maxent_treebank_pos_tagger\u0027,\u0027averaged_perceptron_tagger\u0027,\u0027maxent_ne_chunker\u0027,\u0027words\u0027])\n        sents \u003d nltk.sent_tokenize(re.sub(\u0027[\\:\\\\n]\u0027,\u0027.\u0027,text))\n    gpes \u003d []\n    for sent in sents:\n        tokenizer \u003d nltk.RegexpTokenizer(\u0027\\w(?:[-\\w]*[\\\u0027\\w]*\\w)?\u0027)\n        tokens \u003d tokenizer.tokenize(sent.replace(\u0027,\u0027, \u0027.\u0027))\n        tagged_text \u003d nltk.pos_tag(tokens)\n        for subtree in nltk.ne_chunk(tagged_text).subtrees():\n            # State/country codes often end up labelled as \u0027ORGANIZATION\u0027\n            if subtree.label() in [\u0027GPE\u0027, \u0027ORGANIZATION\u0027, \u0027PERSON\u0027]:\n                gpe \u003d u\u0027 \u0027.join([t[0] for t in subtree])\n                if len(gpe) \u003e 2 or (len(gpe) \u003e 1 and gpe.isupper()):\n                    gpes.append(gpe)\n            elif subtree.label() \u003d\u003d \u0027S\u0027:\n                for t in subtree:\n                    if type(t) \u003d\u003d tuple and t[0] and t[1].startswith(\u0027NNP\u0027):\n                        gpe \u003d u\u0027 \u0027.join([t[0]])\n                        if len(gpe) \u003e 2 or (len(gpe) \u003e 1 and gpe.isupper()):\n                            gpes.append(gpe)\n    return gpes\n\ndef dumb_gpes(text, max\u003d10):\n    \"\"\"\n    Split text by spaces, stripping non-alphanumeric characters,\n    and return the resulting list\n    \"\"\"\n    words \u003d re.sub(r\u0027,|\\\u0027|\"|:\u0027, \u0027 \u0027, text)\n    words \u003d filter(unicode.isalnum, words.split())\n    words \u003d [word for word in words if len(word) \u003e 2]\n    return words[0:max]\n\n\ndef splitchars(text, maxwords\u003d20, maxlen\u003d5, minlen\u003d2):\n    \"\"\"\n    Intended for Asian languages lacking spaces between words,\n    this function splits text at intervals producing a list of\n    words each containing minlen to maxlen characters.\n    \"\"\"\n    text \u003d filter(unicode.isalnum, text)\n    words \u003d set()\n    texts \u003d text.split()\n    for t in texts:\n        for x in range(len(t)):\n            if len(words) \u003e\u003d maxwords:\n                break\n            for y in range(maxlen, 0, -1):\n                if len(words) \u003e\u003d maxwords:\n                    break\n                if x + y \u003c\u003d len(t) and ((x + y) - x) \u003e\u003d minlen:\n                    words.add((t[x:x + y]))\n    return list(words)",
      "dateUpdated": "Oct 20, 2016 1:33:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474505545607_-1088196480",
      "id": "20160922-005225_708283355",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 12:52:25 AM",
      "dateStarted": "Oct 20, 2016 1:33:46 PM",
      "dateFinished": "Oct 20, 2016 1:33:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nres \u003d extract(\"I want to live in Bangor, Maine\")\nprint(res)",
      "dateUpdated": "Oct 20, 2016 1:33:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475638010223_1924134017",
      "id": "20161005-032650_594516216",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[{\u0027place_lng\u0027: -69.24977111816406, \u0027geo_geonameid\u0027: 4971068, \u0027place_lat\u0027: 45.50032043457031, \u0027place_name\u0027: u\u0027Maine\u0027}]\n"
      },
      "dateCreated": "Oct 5, 2016 3:26:50 AM",
      "dateStarted": "Oct 20, 2016 1:33:49 PM",
      "dateFinished": "Oct 20, 2016 1:33:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\ngeocode match Scoring methods\n\"\"\"\n\ndef relation_score(places, admins, countries):\n    \"\"\"\n    Adjust scores if the admin/country name or code of a place/admin match\n    equals the name/code of an admin/country match.\n    \"\"\"\n    for place in places.keys():\n        for item in places[place]:\n            admin_code \u003d item[\u0027admin1_abbr\u0027]\n            country \u003d item[\u0027country\u0027]\n            if admin_code:\n                if admin_code in admins.keys():\n                    item[\u0027score\u0027] *\u003d 3\n                    for admitem in admins[admin_code]:\n                        if admitem[\u0027country\u0027] \u003d\u003d country:\n                            admitem[\u0027score\u0027] *\u003d 3\n                else:\n                    for admin in admins.keys():\n                        for admitem in admins[admin]:\n                            if (admitem[\u0027country\u0027] \u003d\u003d country and\n                                        admitem[\u0027admin1_abbr\u0027] \u003d\u003d admin_code and\n                                        admitem[\u0027feature_code\u0027] \u003d\u003d \u0027ADM1\u0027):\n                                item[\u0027score\u0027] *\u003d 4\n                                admitem[\u0027score\u0027] *\u003d 3\n            if country:\n                for cc in countries.keys():\n                    for ccitem in countries[cc]:\n                        if ccitem[\u0027country\u0027] \u003d\u003d country:\n                            ccitem[\u0027score\u0027] *\u003d 2\n                            if ccitem[\u0027matching_word\u0027] !\u003d item[\u0027matching_word\u0027] \\\n                                    and item[\u0027matching_word\u0027] not in (admins.keys() + countries.keys()):\n                                item[\u0027score\u0027] *\u003d 2\n    for adm in admins.keys():\n        for item in admins[adm]:\n            country \u003d item[\u0027country\u0027]\n            if country:\n                for cc in countries.keys():\n                    for ccitem in countries[cc]:\n                        if ccitem[\u0027country\u0027] \u003d\u003d country:\n                            if ccitem[\u0027matching_word\u0027] !\u003d item[\u0027matching_word\u0027] \\\n                                    and item[\u0027matching_word\u0027] in admins.keys():\n                                item[\u0027score\u0027] *\u003d 2\n\n\n\ndef score_matches(matches, name, codes):\n    \"\"\"\n    Make a semi-educated guess about which match is better\n    \"\"\"\n    for nmatch in matches:\n        fc \u003d nmatch[\u0027feature_code\u0027]\n        alternates \u003d []\n        score \u003d 1\n        exact_match \u003d False\n        source \u003d nmatch[\u0027source\u0027]\n        if source in (\u0027name\u0027,\u0027ascii\u0027):\n            # Best type of nmatch\n            score +\u003d 100000\n            exact_match \u003d True\n        elif source \u003d\u003d \u0027alternate\u0027:\n            # 2nd best (alternate name - for all foreign languages)\n            score +\u003d 100000\n            exact_match \u003d True\n        if name not in codes and (nmatch[\u0027name\u0027].lower().startswith(name.lower()) or nmatch[\u0027name\u0027].lower().endswith(name.lower())):\n            score +\u003d 50000\n            if fc.startswith(\u0027PCL\u0027):\n                score +\u003d 1000000\n        if name in codes:\n            if (name.upper() \u003d\u003d nmatch[\u0027country\u0027] or name.upper() in alternates) and fc.startswith(\u0027PCL\u0027):\n                score +\u003d 50000\n            elif (name.upper() \u003d\u003d nmatch[\u0027admin1\u0027] or name.upper() in alternates) and fc \u003d\u003d \u0027ADM1\u0027:\n                score +\u003d 50000\n            else:\n                score \u003d -1\n        # Give higher scores to political capitals.\n        # \u0027Moscow\u0027 is most likely the one in Russia, not Maine\n        if score \u003e 50000:\n            if fc:\n                if fc.startswith(u\u0027PCL\u0027):\n                    score +\u003d 100000\n                elif fc \u003d\u003d \u0027ADM1\u0027:\n                    score +\u003d 50000\n                elif fc.startswith(\u0027PPLA\u0027) and exact_match:\n                    score +\u003d 40000\n                elif fc \u003d\u003d \u0027PPLC\u0027:\n                    score +\u003d 400000\n        if score \u003e 1:\n            if codes:\n                admin \u003d nmatch[\u0027admin1\u0027]\n                if admin and admin in codes:\n                    score *\u003d 4\n                admin \u003d nmatch[\u0027country\u0027]\n                if admin and admin in codes:\n                    score *\u003d 2\n            if alternates:\n                # Bonus points for actually having alternate names\n                score +\u003d 5000\n            if nmatch[\u0027feature_code\u0027].startswith(\u0027PPL\u0027):\n                score +\u003d ((nmatch[\u0027population\u0027] + 1) / 10)\n            else:\n                score +\u003d ((nmatch[\u0027population\u0027] + 1) / 1000)\n        nmatch[\u0027score\u0027] \u003d score\n\n",
      "dateUpdated": "Oct 20, 2016 1:33:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474505587834_1137637204",
      "id": "20160922-005307_662518208",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 12:53:07 AM",
      "dateStarted": "Oct 20, 2016 1:33:52 PM",
      "dateFinished": "Oct 20, 2016 1:33:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndef name_match(name, results, admin1\u003dNone, country\u003dNone):\n    \"\"\"\n    Create a list of objects for each match found for a name\n    \"\"\"\n    name_matches \u003d []\n    for geoname in results:\n        if not admin1 or admin1 \u003d\u003d geoname[\u0027admin1_code\u0027]:\n            if not country or country \u003d\u003d geoname[\"country_code\"]:\n                name_matches.append({\n                    \u0027coord\u0027: [[geoname[\u0027longitude\u0027], geoname[\u0027latitude\u0027]]],\n                    \u0027geonameid\u0027: geoname[\u0027geonameid\u0027],\n                    \u0027name\u0027: geoname[\u0027name\u0027],\n                    \u0027matching_word\u0027: name,\n                    \u0027admin1\u0027: geoname[\u0027admin1_code\u0027],\n                    \u0027admin1_abbr\u0027: geoname[\u0027admin1_abbr\u0027],\n                    \u0027admin2\u0027: geoname[\u0027admin2_code\u0027],\n                    \u0027admin3\u0027: geoname[\u0027admin3_code\u0027],\n                    \u0027admin4\u0027: geoname[\u0027admin4_code\u0027],\n                    \u0027feature_code\u0027: geoname[\u0027feature_code\u0027],\n                    \u0027country\u0027: geoname[\u0027country_code\u0027],\n                    \u0027population\u0027: geoname.get(\u0027population\u0027) or 0,\n                    \u0027source\u0027: geoname.get(\u0027source\u0027),\n                    \u0027score\u0027: -1\n                })\n    return name_matches",
      "dateUpdated": "Oct 20, 2016 1:33:56 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474390415223_-1427773016",
      "id": "20160919-191612_1719005909",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 20, 2016 4:53:35 PM",
      "dateStarted": "Oct 20, 2016 1:33:56 PM",
      "dateFinished": "Oct 20, 2016 1:33:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndef panda_query(names, codes):\n    \"\"\"\n    Performs a query against Geonames Panda DataFrame.\n    :param names: list of placenames to search for\n    :param codes: list of potential admin/country codes to search for\n    :return: matching geonames records\n    \"\"\"\n    place_matches \u003d {}\n    admin_matches \u003d {}\n    country_matches \u003d {}\n\n    for name in names:\n        #result_rdd \u003d geocode_rdd_lower.filter(geocode_rdd_lower[\"name\"] \u003d\u003d name)\n        result_pdd \u003d geocode_pdd[\n             geocode_pdd[\"name\"] \u003d\u003d name.lower()\n          ]\n        #result_pdd.head(10)\n        results \u003d result_pdd.T.to_dict().values()\n        #results \u003d result_rdd.toPandas().T.to_dict().values()\n        name_matches \u003d name_match(name, results)\n        \n        score_matches(name_matches, name, codes)\n        for nmatch in name_matches:\n            if nmatch[\u0027score\u0027] \u003e 1:\n                featurecode \u003d nmatch[\u0027feature_code\u0027]\n                if featurecode and featurecode.startswith(\u0027ADM\u0027):\n                    admin_matches.setdefault(name, []).append(nmatch)\n                elif featurecode and featurecode.startswith(\u0027PCL\u0027):\n                    country_matches.setdefault(name, []).append(nmatch)\n                else:\n                    place_matches.setdefault(name, []).append(nmatch)\n    return place_matches, admin_matches, country_matches",
      "dateUpdated": "Oct 20, 2016 1:34:00 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474481523614_-377810751",
      "id": "20160921-181203_1373825730",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 21, 2016 6:12:03 PM",
      "dateStarted": "Oct 20, 2016 1:34:00 PM",
      "dateFinished": "Oct 20, 2016 1:34:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndef get_geonames(names, codes):\n    \"\"\"\n    Find matching geonames records given a list of placenames and (possibly blank) state/country codes\n    \"\"\"\n    all_matches \u003d {}\n    place_matches, admin_matches, country_matches \u003d panda_query(names, codes)\n    relation_score(place_matches, admin_matches, country_matches)\n\n    for group in (place_matches, admin_matches, country_matches):\n        unique_ids \u003d []\n        for k, v in group.iteritems():\n            ranked \u003d sorted(v, key\u003dlambda k: k[\u0027score\u0027], reverse\u003dTrue)\n            if ranked[0][\u0027score\u0027] \u003e 1 and \\\n                            ranked[0][\u0027geonameid\u0027] not in unique_ids and \\\n                    (k not in all_matches or all_matches[k][\u0027score\u0027] \u003c\n                        ranked[0][\u0027score\u0027]):\n                all_matches[k] \u003d ranked[0]\n                unique_ids.append(ranked[0][\u0027geonameid\u0027])\n\n    place_list \u003d []\n    for place in sorted(all_matches.values(), key\u003dlambda k: k[\u0027score\u0027], reverse\u003dTrue):\n        if place[\u0027score\u0027] \u003e 1:\n            place_list.append({\n                            \u0027place_name\u0027: place[\u0027name\u0027],\n                            \u0027place_lat\u0027: place[\u0027coord\u0027][0][1],\n                            \u0027place_lng\u0027: place[\u0027coord\u0027][0][0],\n                            \u0027geo_geonameid\u0027: int(place[\u0027geonameid\u0027])\n                        })   \n    return place_list\n    \ndef geocode(name):\n    return get_geonames([name],[])",
      "dateUpdated": "Oct 20, 2016 1:34:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474506183760_1096727938",
      "id": "20160922-010303_2088559782",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 1:03:03 AM",
      "dateStarted": "Oct 20, 2016 1:34:03 PM",
      "dateFinished": "Oct 20, 2016 1:34:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndef extract(text, lang\u003d\u0027en\u0027, dumb_tokens\u003dFalse, stanford\u003dFalse):\n    \"\"\"\n    Extract possible placenames from text, then query Geonames dataframe for matches.\n    \"\"\"\n    name_counts \u003d OrderedDict([])\n    code_counts \u003d {}\n\n    if not text:\n        # Nothing to do\n        return\n\n    if type(text) \u003d\u003d str:\n        text \u003d unicode(text)\n\n    text \u003d re.sub(DELETE_REGEX, u\u0027, \u0027, text)\n    text \u003d re.sub(r\u0027\\\u0027\u0027, u\u0027\\\u0027\\\u0027\u0027, text)\n\n    # Look for state/country codes (2-3 capital letters)\n    possible_codes \u003d list(\n        set(re.findall(CODE_REGEX, text)))\n    for code in possible_codes:\n        if code.lower() not in IGNORE_WORDS:\n            code_counts[code.lower()] \u003d code_counts.get(\n                code, 0) + 1\n\n    possible_names \u003d []\n    if lang in nospace_langs:\n        # No spaces in language, so make spaces\n        possible_names \u003d splitchars(text)[0:255]\n    elif dumb_tokens:\n        # Split words on spaces\n        possible_names \u003d dumb_gpes(text)\n    else:\n        # Use NLTK tokenizer and Stanford tagger to find placenames.\n        if stanford:\n            try:\n                possible_names.extend(get_stanford_gpes(text))\n            except OSError:\n                possible_names.extend(get_nltk_gpes(text))\n        else:\n            possible_names.extend(get_nltk_gpes(text))\n            name_prefixes \u003d [\u0027san\u0027, \u0027las\u0027, \u0027los\u0027]\n            for prefix in name_prefixes:\n                san_matches \u003d re.findall(\u0027{} \\w+\u0027.format(prefix), text, flags\u003dre.IGNORECASE)\n                for san_match in san_matches:\n                    if san_match not in possible_names:\n                        possible_names.append(san_match)\n                    for san_word in san_match.split(\u0027 \u0027):\n                        if san_word in possible_names:\n                            possible_names.remove(san_word)\n\n    for i, possible_geoname in enumerate(possible_names):\n        if possible_geoname.lower() in IGNORE_WORDS: continue\n\n        # If it looks like a state/country code, treat it that way\n        if (2 \u003c\u003d len(\n                possible_geoname) \u003c\u003d 3) and possible_geoname.isupper() and \\\n                        lang not in nospace_langs:\n            code_counts[possible_geoname.lower()] \u003d code_counts.get(\n                possible_geoname, 0) + 1\n        else:\n            name_counts[possible_geoname.lower()] \u003d name_counts.get(\n                possible_geoname.lower(), 0) + 1\n\n    if not name_counts and not code_counts:\n        # Nothing to do\n        return []\n\n    codes \u003d code_counts.keys()\n\n    names \u003d name_counts.keys()\n    names.extend(code_counts.keys())\n\n    return get_geonames(names, codes)",
      "dateUpdated": "Oct 20, 2016 1:34:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474505709811_-669342846",
      "id": "20160922-005509_310516318",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 12:55:09 AM",
      "dateStarted": "Oct 20, 2016 1:34:07 PM",
      "dateFinished": "Oct 20, 2016 1:34:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nres \u003d extract(\"I want to live in Bangor, Maine\")\nprint(res)\n\n#res \u003d geocode(\"paris\")\n#print(res)",
      "dateUpdated": "Oct 20, 2016 1:34:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474506670732_-929257958",
      "id": "20160922-011110_1485682505",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[{\u0027place_lng\u0027: -69.24977111816406, \u0027geo_geonameid\u0027: 4971068, \u0027place_lat\u0027: 45.50032043457031, \u0027place_name\u0027: u\u0027maine\u0027}, {\u0027place_lng\u0027: -68.7778091430664, \u0027geo_geonameid\u0027: 4957280, \u0027place_lat\u0027: 44.80118179321289, \u0027place_name\u0027: u\u0027bangor\u0027}]\n"
      },
      "dateCreated": "Sep 22, 2016 1:11:10 AM",
      "dateStarted": "Oct 20, 2016 1:34:11 PM",
      "dateFinished": "Oct 20, 2016 1:34:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\"\"\"\nLoad processed_news.news table into SparkRDD\n\"\"\"\nall_news \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"news\", keyspace\u003d\"test_news\").load().select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_list\", \"place_lat\", \"place_lng\", \"place_name\", \"place_list_v\", \"t\").persist(StorageLevel.MEMORY_AND_DISK_SER)",
      "dateUpdated": "Oct 3, 2016 1:33:04 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474390415223_-1427773016",
      "id": "20160919-191927_1080118395",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 20, 2016 4:53:35 PM",
      "dateStarted": "Oct 3, 2016 1:33:04 PM",
      "dateFinished": "Oct 3, 2016 1:33:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n#all_news.count()",
      "dateUpdated": "Sep 30, 2016 12:38:38 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474919379365_-757590594",
      "id": "20160926-194939_1278515211",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 26, 2016 7:49:39 PM",
      "dateStarted": "Sep 29, 2016 2:39:03 PM",
      "dateFinished": "Sep 29, 2016 2:40:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nSelect news records which have a place_name\n\"\"\"\n\nplacename_news \u003d all_news.filter(\n    (all_news[\"place_name\"].isNotNull()) \u0026\n    (\n        (all_news[\u0027place_list_v\u0027].isNull()) |\n        (all_news[\u0027place_list_v\u0027] !\u003d processor_version)\n    )\n).persist(StorageLevel.MEMORY_AND_DISK_SER)\n#placename_news.select(\u0027place_list\u0027, \u0027place_name\u0027, \u0027t\u0027, \u0027ds\u0027, \u0027place_list_v\u0027).show(10)\n",
      "dateUpdated": "Oct 3, 2016 1:33:16 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474467180384_-2090966117",
      "id": "20160921-141300_1549303169",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 21, 2016 2:13:00 PM",
      "dateStarted": "Oct 3, 2016 1:33:16 PM",
      "dateFinished": "Oct 3, 2016 1:33:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\"\"\"\nTemp commands to write to place_lat, place_lng columns for geocoded rows\n\"\"\"\n#tmp_news \u003d all_news.filter((all_news[\"place_list\"].isNotNull())).persist(StorageLevel.MEMORY_AND_DISK_SER)\n#tmpcoords \u003d tmp_news.withColumn(\"place_lat\", tmp_news[\"place_list\"][0].place_lat).withColumn(\"place_lng\", tmp_news[\"place_list\"][0].place_lng).persist(StorageLevel##.MEMORY_AND_DISK_SER)\n#tmpcoords.select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_lat\", \"place_lng\").write.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"news\", keyspace\u003d\"test_news\").save(mode\u003d\"append\")\n",
      "dateUpdated": "Sep 29, 2016 3:07:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475075759401_-1423334299",
      "id": "20160928-151559_1059253550",
      "dateCreated": "Sep 28, 2016 3:15:59 PM",
      "dateStarted": "Sep 29, 2016 3:04:57 PM",
      "dateFinished": "Sep 29, 2016 3:05:01 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nUDF Functions for a) extract and geocode, b) geocode only\n\"\"\"\n\nfrom pyspark.sql.types import StringType, StructType, FloatType, DoubleType, IntegerType, StructField, ArrayType\nfrom pyspark.sql.functions import udf\n\nPlaceType \u003d  ArrayType(StructType([\n    StructField(\u0027place_name\u0027, StringType(), True),\n    StructField(\u0027place_lat\u0027, FloatType(), True),    \n    StructField(\u0027place_lng\u0027, FloatType(), True),  \n    StructField(\u0027place_id\u0027, IntegerType(), True),\n    StructField(\u0027geo_geonameid\u0027, IntegerType(), True)\n    ]),True)\n\nextract_udf \u003d udf(lambda txt: extract(txt), PlaceType)\ngeocode_udf \u003d udf(lambda txt: geocode(txt), PlaceType)\n    \n",
      "dateUpdated": "Oct 20, 2016 1:34:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474469939359_-34759678",
      "id": "20160921-145859_2099202807",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 21, 2016 2:58:59 PM",
      "dateStarted": "Oct 20, 2016 1:34:22 PM",
      "dateFinished": "Oct 20, 2016 1:35:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nCalculate version column for this week\n\"\"\"\nimport datetime\n\ndef versiondate_column(column, version, dt):\n    return \u0027{}__{}__{}\u0027.format(column,\n                               version.replace(\u0027.\u0027, \u0027_\u0027),\n                               datetime.datetime.strftime(dt, \u0027%Y%W\u0027))\n                               \n\n",
      "dateUpdated": "Oct 20, 2016 1:36:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474554975480_890830700",
      "id": "20160922-143615_1684514285",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 2:36:15 PM",
      "dateStarted": "Oct 20, 2016 1:36:52 PM",
      "dateFinished": "Oct 20, 2016 1:36:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\"\"\"\nGeocode rows with existing place_name column\n\"\"\"\n\nagg_table_df \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"agg_table\", keyspace\u003d\"epione\").load().select(\"id\", \"source\", \"cr\", \"description\").persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n\nplacename_geo \u003d agg_table_df.withColumn(\"place_list\", extract_udf(agg_table_df[\"description\"]))#.persist(StorageLevel.MEMORY_AND_DISK_SER)\nprint placename_geo.show(1)\n",
      "dateUpdated": "Oct 20, 2016 12:40:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474467276947_1546186185",
      "id": "20160921-141436_313357053",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling o643.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 4611, localhost): java.io.IOException: Exception during execution of SELECT \"id\", \"source\", \"cr\", \"description\" FROM \"epione\".\"agg_table\" WHERE token(\"id\", \"source\", \"cr\") \u003e ? AND token(\"id\", \"source\", \"cr\") \u003c\u003d ?   ALLOW FILTERING: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:320)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)\n\tat sun.reflect.GeneratedMethodAccessor293.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy22.execute(Unknown Source)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:309)\n\t... 35 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:212)\n\tat org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)\n\tat org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374)\n\tat org.apache.spark.sql.DataFrame.take(DataFrame.scala:1456)\n\tat org.apache.spark.sql.DataFrame.showString(DataFrame.scala:170)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Exception during execution of SELECT \"id\", \"source\", \"cr\", \"description\" FROM \"epione\".\"agg_table\" WHERE token(\"id\", \"source\", \"cr\") \u003e ? AND token(\"id\", \"source\", \"cr\") \u003c\u003d ?   ALLOW FILTERING: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:320)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$18.apply(CassandraTableScanRDD.scala:334)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)\n\tat org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:88)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:25)\n\tat com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n\tat com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n\tat com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)\n\tat sun.reflect.GeneratedMethodAccessor293.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)\n\tat com.sun.proxy.$Proxy22.execute(Unknown Source)\n\tat com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:309)\n\t... 35 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:115)\n\tat com.datastax.driver.core.Responses$Error.asException(Responses.java:124)\n\tat com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:477)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)\n\tat com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)\n\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:831)\n\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:346)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:254)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\nCaused by: com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:62)\n\tat com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:266)\n\tat com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:246)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)\n\t... 11 more\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o643.showString.\\n\u0027, JavaObject id\u003do644), \u003ctraceback object at 0x7f8344631ab8\u003e)"
      },
      "dateCreated": "Sep 21, 2016 2:14:36 PM",
      "dateStarted": "Oct 20, 2016 12:40:31 AM",
      "dateFinished": "Oct 20, 2016 12:41:39 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n#placename_news.select(\"id\", \"ds\",  \"place_list\", \"place_name\", \"place_lng\", \"place_lat\").show(5)",
      "dateUpdated": "Sep 27, 2016 1:17:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474556474965_1079474200",
      "id": "20160922-150114_1556117963",
      "dateCreated": "Sep 22, 2016 3:01:14 PM",
      "dateStarted": "Sep 27, 2016 1:47:09 AM",
      "dateFinished": "Sep 27, 2016 2:54:16 AM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\"\"\"\nAssign geocode results to  \"place_lat\", \"place_lng\" columns\n\"\"\"\nplacename_geo_coord \u003d placename_geo.filter(placename_geo[\"place_list\"].isNotNull()).withColumn(\"place_lat\", placename_geo[\"place_list\"][0].place_lat).withColumn(\"place_lng\", placename_geo[\"place_list\"][0].place_lng).persist(StorageLevel.MEMORY_AND_DISK_SER)",
      "dateUpdated": "Oct 3, 2016 2:52:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474510474630_385038754",
      "id": "20160922-021434_507900216",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 2:14:34 AM",
      "dateStarted": "Oct 3, 2016 2:52:45 PM",
      "dateFinished": "Oct 3, 2016 2:52:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nCreate versioning columns \n\"\"\"\ntoday \u003d datetime.datetime.now()                               \nvdc \u003d versiondate_column(processor_name, processor_version, today)\nv \u003d \u0027{}_v\u0027.format(processor_name)\nvdc \u003d \"place_list__v0_0_1__201639\"\n",
      "dateUpdated": "Oct 3, 2016 2:52:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474555356930_-687289983",
      "id": "20160922-144236_1717998006",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 2:42:36 PM",
      "dateStarted": "Oct 3, 2016 2:52:35 PM",
      "dateFinished": "Oct 3, 2016 2:52:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\"\"\"\nand populate them\n\"\"\"\n\nplacename_geo_coord_version \u003d placename_geo_coord.withColumn(v, lit(processor_version)).withColumn(vdc, placename_geo_coord[\u0027place_list\u0027]).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
      "dateUpdated": "Oct 3, 2016 2:53:02 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474467343887_340291187",
      "id": "20160921-141543_103518315",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 21, 2016 2:15:43 PM",
      "dateStarted": "Oct 3, 2016 2:53:02 PM",
      "dateFinished": "Oct 3, 2016 2:53:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nSave the records\n\"\"\"\n\nplacename_geo_coord_version.select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_list\", \"place_lat\", \"place_lng\", v, vdc).write.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"news\", keyspace\u003d\"test_news\").save(mode\u003d\"append\")\n\ntry:\n    placename_geo_coord_version.unpersist()\n    placename_geo_coord.unpersist()\n    placename_geo.unpersist()\n    placename_news.unpersist()\nexcept Exception as e:\n    print(\"NO NEED TO UNPERSIST HERE?\")\n    print(e)",
      "dateUpdated": "Oct 3, 2016 2:53:16 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474471053835_-1128978972",
      "id": "20160921-151733_906993747",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling o204.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 11.0 failed 1 times, most recent failure: Lost task 2.0 in stage 11.0 (TID 4275, localhost): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:2271)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1876)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:153)\n\tat org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1251)\n\tat org.apache.spark.storage.BlockManager.dataSerialize(BlockManager.scala:1257)\n\tat org.apache.spark.storage.MemoryStore.putArray(MemoryStore.scala:136)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:810)\n\tat org.apache.spark.storage.BlockManager.putArray(BlockManager.scala:686)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:175)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:37)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:67)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:85)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:2271)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1876)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:153)\n\tat org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1251)\n\tat org.apache.spark.storage.BlockManager.dataSerialize(BlockManager.scala:1257)\n\tat org.apache.spark.storage.MemoryStore.putArray(MemoryStore.scala:136)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:810)\n\tat org.apache.spark.storage.BlockManager.putArray(BlockManager.scala:686)\n\tat org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:175)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:268)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o204.save.\\n\u0027, JavaObject id\u003do205), \u003ctraceback object at 0x7f0fa29dcea8\u003e)"
      },
      "dateCreated": "Sep 21, 2016 3:17:33 PM",
      "dateStarted": "Oct 3, 2016 2:53:16 PM",
      "dateFinished": "Oct 3, 2016 3:33:01 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nNow select all rows with null/empty place_name values and non-null t values (if any)\n\"\"\"\nfrom pyspark.sql.functions import col, when, size\n\n#def blank_as_null(x):\n#    return when(col(x) !\u003d \"\", col(x)).otherwise(None)\n\n#noname_news \u003d all_news.withColumn(\"place_name\", blank_as_null(\"place_name\"))\nnoname_news \u003d all_news.filter(\n    (all_news[\"place_name\"].isNull()) \u0026\n    (all_news[\"t\"].isNotNull()) \u0026\n    (\n        (all_news[\u0027place_list_v\u0027].isNull()) |\n        (all_news[\u0027place_list_v\u0027] !\u003d processor_version)\n    )\n).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
      "dateUpdated": "Sep 30, 2016 10:47:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474548941930_-891800886",
      "id": "20160922-125541_568720035",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 12:55:41 PM",
      "dateStarted": "Sep 30, 2016 10:47:36 AM",
      "dateFinished": "Sep 30, 2016 10:47:36 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nExtract and geocode rows without place_name values based on \u0027t\u0027 column\n\"\"\"\n\nnoname_news_geo \u003d noname_news.withColumn(\"place_list\", extract_udf(noname_news[\"t\"])).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
      "dateUpdated": "Sep 30, 2016 10:47:45 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474549114559_-555702202",
      "id": "20160922-125834_1157030855",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 12:58:34 PM",
      "dateStarted": "Sep 30, 2016 10:47:45 AM",
      "dateFinished": "Sep 30, 2016 10:47:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n#noname_news.select(\u0027place_list\u0027,\u0027t\u0027).show(10)",
      "dateUpdated": "Sep 27, 2016 1:22:26 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474549530321_1153016033",
      "id": "20160922-130530_1376046344",
      "result": "org.apache.zeppelin.interpreter.InterpreterException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused",
      "dateCreated": "Sep 22, 2016 1:05:30 PM",
      "dateStarted": "Sep 27, 2016 1:44:30 AM",
      "dateFinished": "Sep 27, 2016 1:44:32 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nAssign extract/geocode results to  \"place_lat\", \"place_lng\" columns, based on 1st element of place_list\n\"\"\"\n\nfrom pyspark.sql.functions import lit\n\nnoname_news_geo_coord \u003d noname_news_geo.filter(noname_news_geo[\"place_list\"].isNotNull()).withColumn(\"place_lat\", noname_news_geo[\"place_list\"][0].place_lat).withColumn(\"place_lng\", noname_news_geo[\"place_list\"][0].place_lng).withColumn(v, lit(processor_version)).withColumn(vdc, noname_news_geo[\u0027place_list\u0027]).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
      "dateUpdated": "Sep 30, 2016 10:49:00 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474549326285_-229528725",
      "id": "20160922-130206_1754881568",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 1:02:06 PM",
      "dateStarted": "Sep 30, 2016 10:49:00 AM",
      "dateFinished": "Sep 30, 2016 10:49:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n#noname_news.select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_list\", \"place_lat\", \"place_lng\", v, vdc)#.persist(StorageLevel.MEMORY_AND_DISK_SER)\n#noname_latlng.count()",
      "dateUpdated": "Sep 30, 2016 10:49:23 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474551665772_-1807393078",
      "id": "20160922-134105_1821340891",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 1:41:05 PM",
      "dateStarted": "Sep 27, 2016 1:05:26 PM",
      "dateFinished": "Sep 27, 2016 1:05:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nSave results to Scylla\n\"\"\"\n\nnoname_news_geo_coord.select(\"id\", \"ds\", \"ymds\", \"rdt\", \"place_list\", \"place_lat\", \"place_lng\", v, vdc).write.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"news\", keyspace\u003d\"test_news\").save(mode\u003d\"append\")",
      "dateUpdated": "Sep 30, 2016 10:49:25 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474549411485_1631055659",
      "id": "20160922-130331_639004188",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 1:03:31 PM",
      "dateStarted": "Sep 30, 2016 10:49:25 AM",
      "dateFinished": "Sep 30, 2016 4:51:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndef extract_dumb(text, df):\n    name_counts \u003d OrderedDict([])\n    possible_names \u003d re.findall(\u0027[A-Z][\\w\\-]*(?:\\s+[A-Z][\\w\\-]*)*\u0027, text)\n    possible_names \u003d  [word for word in possible_names if word[0].isupper() and word.lower() not in IGNORE_WORDS]\n    #code_counts \u003d []\n    for i, possible_geoname in enumerate(possible_names):\n\n        name_counts[possible_geoname.lower()] \u003d name_counts.get(\n                possible_geoname.lower(), 0) + 1\n\n    if not name_counts:\n        # Nothing to do\n        return []\n\n    names \u003d name_counts.keys()\n    return names\n    #return get_geoname_stupid(names[0:3], df)\n    \ndef panda_query_stupid(names, codes, df):\n    \"\"\"\n    Performs a query against Geonames Panda DataFrame.\n    :param names: list of placenames to search for\n    :param codes: list of potential admin/country codes to search for\n    :return: matching geonames records\n    \"\"\"\n\n    # featurecodes \u003d [\u0027PCL\u0027,\u0027PCLD\u0027,\u0027PCLF\u0027,\u0027PCLI\u0027,\u0027PCLS\u0027,\u0027TERR\u0027,\n    #                     \u0027ADM1\u0027,\u0027ADM2\u0027,\u0027ADM3\u0027,\u0027ADM4\u0027,\n    #                     \u0027PPL\u0027,\u0027PPLA\u0027,\u0027PPLA2\u0027,\u0027PPLA3\u0027,\u0027PPLA4\u0027,\u0027PPLC\u0027\n    #                     ]\n    # featureclasses \u003d [\u0027P\u0027, \u0027A\u0027]\n    # population \u003d 1000\n\n    place_matches \u003d {}\n    admin_matches \u003d {}\n    country_matches \u003d {}\n\n    for name in names:\n        result_pdd \u003d df[df[\"name\"] \u003d\u003d name.lower()]\n        if len(result_pdd.index) \u003e 0:\n            results \u003d result_pdd.T.to_dict().values()\n            name_matches \u003d name_match(name, results)\n            \n            score_matches(name_matches, name, codes)\n            for nmatch in name_matches:\n                if nmatch[\u0027score\u0027] \u003e 1:\n                    featurecode \u003d nmatch[\u0027feature_code\u0027]\n                    if featurecode and featurecode.startswith(\u0027ADM\u0027):\n                        admin_matches.setdefault(name, []).append(nmatch)\n                    elif featurecode and featurecode.startswith(\u0027PCL\u0027):\n                        country_matches.setdefault(name, []).append(nmatch)\n                    else:\n                        place_matches.setdefault(name, []).append(nmatch)\n    return place_matches, admin_matches, country_matches    \n    \ndef get_geoname_stupid(names, df):\n    \"\"\"\n    Find matching geonames records given a list of placenames and (possibly blank) state/country codes\n    \"\"\"\n    all_matches \u003d {}\n    place_matches, admin_matches, country_matches \u003d panda_query_stupid(names, [], df)\n    #relation_score(place_matches, admin_matches, country_matches)\n\n    for group in (place_matches, admin_matches, country_matches):\n        unique_ids \u003d []\n        for k, v in group.iteritems():\n            ranked \u003d sorted(v, key\u003dlambda k: k[\u0027score\u0027], reverse\u003dTrue)\n            if ranked[0][\u0027score\u0027] \u003e 1 and \\\n                            ranked[0][\u0027geonameid\u0027] not in unique_ids and \\\n                    (k not in all_matches or all_matches[k][\u0027score\u0027] \u003c\n                        ranked[0][\u0027score\u0027]):\n                all_matches[k] \u003d ranked[0]\n                unique_ids.append(ranked[0][\u0027geonameid\u0027])\n\n    place_list \u003d []\n    for place in sorted(all_matches.values(), key\u003dlambda k: k[\u0027score\u0027],\n                        reverse\u003dTrue):\n        if place[\u0027score\u0027] \u003e 1:                    \n            place_list.append({\n                \u0027place_name\u0027: place[\u0027name\u0027],\n                \u0027place_lat\u0027: place[\u0027coord\u0027][0][1],\n                \u0027place_lng\u0027: place[\u0027coord\u0027][0][0],\n                \u0027geo_geonameid\u0027: int(place[\u0027geonameid\u0027])\n            })\n    return place_list\n    \nextract_stupid_udf \u003d udf(lambda txt: extract_dumb(txt, geocode_pdd), ArrayType(StringType()))\ngeocode_stupid_udf \u003d udf(lambda txt: get_geoname_stupid(txt, geocode_pdd), PlaceList)\n",
      "dateUpdated": "Sep 28, 2016 3:20:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474998943011_1147615703",
      "id": "20160927-175543_252196534",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 27, 2016 5:55:43 PM",
      "dateStarted": "Sep 28, 2016 3:08:39 PM",
      "dateFinished": "Sep 28, 2016 3:09:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nprint extract_dumb(\"I want to live in Colorado or Fee Fi Fo Fum.\", geocode_pdd)",
      "dateUpdated": "Sep 28, 2016 3:09:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475000883083_-1665678121",
      "id": "20160927-182803_881805308",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "({\u0027colorado\u0027: [{\u0027matching_word\u0027: \u0027colorado\u0027, \u0027feature_code\u0027: u\u0027PPL\u0027, \u0027score\u0027: 50458.3, \u0027population\u0027: 4572.0, \u0027name\u0027: u\u0027colorado\u0027, \u0027geonameid\u0027: 4010260, \u0027country\u0027: u\u0027MX\u0027, \u0027coord\u0027: [[-100.24520111083984, 20.56184959411621]], \u0027admin1\u0027: u\u002722\u0027, \u0027source\u0027: u\u0027alternate\u0027, \u0027admin3\u0027: u\u0027\u0027, \u0027admin2\u0027: u\u002711.0\u0027, \u0027admin4\u0027: u\u0027\u0027, \u0027admin1_abbr\u0027: u\u0027\u0027}, {\u0027matching_word\u0027: \u0027colorado\u0027, \u0027feature_code\u0027: u\u0027PPL\u0027, \u0027score\u0027: 52046.0, \u0027population\u0027: 20449.0, \u0027name\u0027: u\u0027colorado\u0027, \u0027geonameid\u0027: 3465881, \u0027country\u0027: u\u0027BR\u0027, \u0027coord\u0027: [[-51.973060607910156, -22.837499618530273]], \u0027admin1\u0027: u\u002718\u0027, \u0027source\u0027: u\u0027ascii\u0027, \u0027admin3\u0027: u\u0027\u0027, \u0027admin2\u0027: u\u00274105904.0\u0027, \u0027admin4\u0027: u\u0027\u0027, \u0027admin1_abbr\u0027: u\u0027PR\u0027}]}, {\u0027colorado\u0027: [{\u0027matching_word\u0027: \u0027colorado\u0027, \u0027feature_code\u0027: u\u0027ADM1\u0027, \u0027score\u0027: 204679.631, \u0027population\u0027: 4678630.0, \u0027name\u0027: u\u0027colorado\u0027, \u0027geonameid\u0027: 5417618, \u0027country\u0027: u\u0027US\u0027, \u0027coord\u0027: [[-105.5008316040039, 39.00027084350586]], \u0027admin1\u0027: u\u0027CO\u0027, \u0027source\u0027: u\u0027name\u0027, \u0027admin3\u0027: u\u0027\u0027, \u0027admin2\u0027: u\u0027\u0027, \u0027admin4\u0027: u\u0027\u0027, \u0027admin1_abbr\u0027: u\u0027CO\u0027}, {\u0027matching_word\u0027: \u0027colorado\u0027, \u0027feature_code\u0027: u\u0027ADM2\u0027, \u0027score\u0027: 50004.551, \u0027population\u0027: 3550.0, \u0027name\u0027: u\u0027colorado\u0027, \u0027geonameid\u0027: 6323403, \u0027country\u0027: u\u0027BR\u0027, \u0027coord\u0027: [[-52.99087905883789, -28.483200073242188]], \u0027admin1\u0027: u\u002723.0\u0027, \u0027source\u0027: u\u0027ascii\u0027, \u0027admin3\u0027: u\u0027\u0027, \u0027admin2\u0027: u\u00274305603.0\u0027, \u0027admin4\u0027: u\u0027\u0027, \u0027admin1_abbr\u0027: u\u0027RS\u0027}, {\u0027matching_word\u0027: \u0027colorado\u0027, \u0027feature_code\u0027: u\u0027ADM2\u0027, \u0027score\u0027: 50023.348, \u0027population\u0027: 22347.0, \u0027name\u0027: u\u0027colorado\u0027, \u0027geonameid\u0027: 6322738, \u0027country\u0027: u\u0027BR\u0027, \u0027coord\u0027: [[-51.98014831542969, -22.83827018737793]], \u0027admin1\u0027: u\u002718.0\u0027, \u0027source\u0027: u\u0027ascii\u0027, \u0027admin3\u0027: u\u0027\u0027, \u0027admin2\u0027: u\u00274105904.0\u0027, \u0027admin4\u0027: u\u0027\u0027, \u0027admin1_abbr\u0027: u\u0027PR\u0027}]}, {})\n"
      },
      "dateCreated": "Sep 27, 2016 6:28:03 PM",
      "dateStarted": "Sep 28, 2016 3:09:08 PM",
      "dateFinished": "Sep 28, 2016 3:09:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\"\"\"\nLoad testing.agg_table table into SparkRDD\n\"\"\"\nall_agg \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"agg_table\", keyspace\u003d\"testing\").load()\nagg_geo \u003d all_agg.filter(all_agg[\u0027description\u0027].isNotNull()).persist(StorageLevel.MEMORY_AND_DISK_SER)",
      "dateUpdated": "Sep 28, 2016 1:35:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474554377058_979251784",
      "id": "20160922-142617_394676347",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 22, 2016 2:26:17 PM",
      "dateStarted": "Sep 28, 2016 1:35:35 PM",
      "dateFinished": "Sep 28, 2016 1:35:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nprint agg_geo.count()",
      "dateUpdated": "Sep 28, 2016 5:52:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474995056436_1898530662",
      "id": "20160927-165056_680129354",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "349033\n"
      },
      "dateCreated": "Sep 27, 2016 4:50:56 PM",
      "dateStarted": "Sep 28, 2016 5:52:53 PM",
      "dateFinished": "Sep 28, 2016 5:52:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nagg_geo_europe \u003d agg_geo.filter(agg_geo[\"description\"].startswith(\"Europe\")).persist(StorageLevel.MEMORY_AND_DISK_SER)\nagg_geo_europe.show(5)\nprint agg_geo_europe.count()",
      "dateUpdated": "Sep 28, 2016 5:53:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474995242358_-1956796692",
      "id": "20160927-165402_1176598805",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----------+--------------------+--------------------+------------+---------+----------+------------+----------+---------+-------+--------------------+--------------------+\n|        id|                  cr|         description|organization|place_lat|place_list|place_list_v| place_lng|sentiment| source|               title|                 url|\n+----------+--------------------+--------------------+------------+---------+----------+------------+----------+---------+-------+--------------------+--------------------+\n|0.16147459|2016-05-05 22:56:...|European Financia...|     [207.0]|     null|        []|        null|      null|        0|twitter|European Financia...|https://www.twitt...|\n|0.24317169|2016-04-05 10:52:...|Europe News: \u0027\u0027Br...|     [210.0]| 50.58341|        []|        null|-111.88509|        1|twitter|Europe News: \u0027\u0027Br...|https://www.twitt...|\n| 0.2508334|2016-04-12 19:49:...|Europe and Asia d...|     [207.0]|   9.5506|        []|        null|  122.5164|        1|twitter|Europe and Asia d...|https://www.twitt...|\n|0.34535706|2016-04-04 17:52:...|Europe News: Josh...|     [210.0]|     null|        []|        null|      null|       -1|twitter|Europe News: Josh...|https://www.twitt...|\n|  0.239281|2016-04-19 16:30:...|Europe and Asia d...|     [207.0]|   9.5506|        []|        null|  122.5164|        1|twitter|Europe and Asia d...|https://www.twitt...|\n+----------+--------------------+--------------------+------------+---------+----------+------------+----------+---------+-------+--------------------+--------------------+\nonly showing top 5 rows\n\n137\n"
      },
      "dateCreated": "Sep 27, 2016 4:54:02 PM",
      "dateStarted": "Sep 28, 2016 5:53:07 PM",
      "dateFinished": "Sep 28, 2016 5:53:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nagg_geo2 \u003d agg_geo.withColumn(\"place_names\", extract_udf(agg_geo[\"description\"])).persist(StorageLevel.MEMORY_AND_DISK_SER)",
      "dateUpdated": "Sep 28, 2016 5:53:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474995164728_1909621626",
      "id": "20160927-165244_1209557008",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 27, 2016 4:52:44 PM",
      "dateStarted": "Sep 28, 2016 5:53:19 PM",
      "dateFinished": "Sep 28, 2016 5:53:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nprint agg_geo2.count()",
      "dateUpdated": "Sep 28, 2016 5:53:24 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474995235839_-2069158695",
      "id": "20160927-165355_404286843",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling o3130.count.\n: org.apache.spark.SparkException: Job 251 cancelled part of cancelled job group zeppelin-20160927-165355_404286843\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:166)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1515)\n\tat org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1514)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)\n\tat org.apache.spark.sql.DataFrame.count(DataFrame.scala:1514)\n\tat sun.reflect.GeneratedMethodAccessor144.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o3130.count.\\n\u0027, JavaObject id\u003do3134), \u003ctraceback object at 0x7f7210888c68\u003e)"
      },
      "dateCreated": "Sep 27, 2016 4:53:55 PM",
      "dateStarted": "Sep 28, 2016 5:53:24 PM",
      "dateFinished": "Sep 28, 2016 5:53:58 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nprint agg_geo2.select(\u0027place_list\u0027).show(10)",
      "dateUpdated": "Sep 28, 2016 1:44:43 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475000617462_183842135",
      "id": "20160927-182337_1415141025",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+\n|          place_list|\n+--------------------+\n|[Federal, K, Delo...|\n|[RT, Accenture, B...|\n|[Rod, David, Wats...|\n|[RT, Dropbox, ELz...|\n|           [IPCHdMY]|\n|[Check, Sm17ZyUuF...|\n|     [Key, PcVJpZjQ]|\n|[IBM, Internation...|\n|[Jul, GMT, The ST...|\n|[IBM Just Put A, ...|\n+--------------------+\nonly showing top 10 rows\n\nNone\n"
      },
      "dateCreated": "Sep 27, 2016 6:23:37 PM",
      "dateStarted": "Sep 28, 2016 1:44:43 PM",
      "dateFinished": "Sep 28, 2016 1:44:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nagg_geo2 \u003d agg_geo.filter(agg_geo[\"place_list\"].isNotNull()).withColumn(\"place_lat\", agg_geo[\"place_list\"][0].place_lat).withColumn(\"place_lng\", agg_geo[\"place_list\"][0].place_lng).persist(StorageLevel.MEMORY_AND_DISK_SER)",
      "dateUpdated": "Sep 27, 2016 6:51:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474995196798_-1640810576",
      "id": "20160927-165316_637447126",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 27, 2016 4:53:16 PM",
      "dateStarted": "Sep 27, 2016 6:51:56 PM",
      "dateFinished": "Sep 27, 2016 6:51:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nagg_geo2.select(\u0027place_list\u0027, \u0027place_lat\u0027, \u0027place_lng\u0027).show(5)",
      "dateUpdated": "Sep 27, 2016 6:58:01 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1475001304522_-1537376136",
      "id": "20160927-183504_1500780376",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+---------+----------+\n|          place_list|place_lat| place_lng|\n+--------------------+---------+----------+\n|                  []|     null|      null|\n|[[brooks,50.58341...| 50.58341|-111.88509|\n|[[asia,9.5506,122...|   9.5506|  122.5164|\n|                  []|     null|      null|\n|[[asia,9.5506,122...|   9.5506|  122.5164|\n+--------------------+---------+----------+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Sep 27, 2016 6:35:04 PM",
      "dateStarted": "Sep 27, 2016 6:58:01 PM",
      "dateFinished": "Sep 27, 2016 6:58:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\n\n\nagg_geo2.select(\"id\", \"place_lat\", \"place_lng\").write.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"agg_table\", keyspace\u003d\"testing\").save(mode\u003d\"append\")\n",
      "dateUpdated": "Sep 27, 2016 6:57:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474991854290_-1018050938",
      "id": "20160927-155734_703188919",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 27, 2016 3:57:34 PM",
      "dateStarted": "Sep 27, 2016 6:57:22 PM",
      "dateFinished": "Sep 27, 2016 6:57:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1474991996444_-620773890",
      "id": "20160927-155956_1081954069",
      "dateCreated": "Sep 27, 2016 3:59:56 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "geocode_news_pyspark",
  "id": "2BXR2X9D9",
  "angularObjects": {
    "2BUZX9EWW:shared_process": [],
    "2BRP3ZUWJ:shared_process": [],
    "2BTJ3P41C:shared_process": [],
    "2BSJYK2YE:shared_process": [],
    "2BU87RU3U:shared_process": [],
    "2BTB82RPQ:shared_process": [],
    "2BSSEDUXN:shared_process": [],
    "2BRZ896X6:shared_process": [],
    "2BT3JK3T4:shared_process": [],
    "2BTXGDVEJ:shared_process": [],
    "2BUSJ5B5T:shared_process": [],
    "2BT211CDH:shared_process": [],
    "2BTX1MQS6:shared_process": [],
    "2BUCYP1D2:shared_process": [],
    "2BUTB2HA6:shared_process": [],
    "2BRYFEHJ7:shared_process": [],
    "2BSMJA8VG:shared_process": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}